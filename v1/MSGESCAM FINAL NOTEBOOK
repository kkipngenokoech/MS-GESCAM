{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11040930,"sourceType":"datasetVersion","datasetId":6877343}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\nimport kagglehub\nthebrokenvessel_gescam_partial_path = kagglehub.dataset_download('thebrokenvessel/gescam-partial')\n\nprint('Data source import complete.')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YX64n2c1eLK4","outputId":"01187d9f-5429-47ca-a922-3ef7c9ecbeb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/thebrokenvessel/gescam-partial?dataset_version_number=1...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7.75G/7.75G [01:25<00:00, 97.0MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Data source import complete.\n"]}],"execution_count":1},{"cell_type":"markdown","source":"## IMPORTS","metadata":{"id":"ahkCTbQdeLK7"}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport xml.etree.ElementTree as ET\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\nimport cv2\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\nimport random\nimport torchvision.models as models\nimport math\nfrom sklearn.metrics import roc_curve, auc\nimport bisect\nimport torch\n\nimport torch\nprint(\"Is CUDA available?\", torch.cuda.is_available())\nprint(\"CUDA device count:\", torch.cuda.device_count())\nprint(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kQJVbCRreLK8","outputId":"2dddcace-2ec2-4d39-9d7b-3f27f7dbf982"},"outputs":[{"output_type":"stream","name":"stdout","text":["Is CUDA available? True\n","CUDA device count: 1\n","CUDA device name: Tesla T4\n"]}],"execution_count":2},{"cell_type":"code","source":"print(os.listdir('.'))\nprint(thebrokenvessel_gescam_partial_path)\n\nimport os\n\n# # List top-level files and directories\n# for root, dirs, files in os.walk(thebrokenvessel_gescam_partial_path):\n#     print(\"Directory:\", root)\n#     for name in files:\n#         print(\"  File:\", name)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8p739Lb2gNaP","outputId":"a3c802f5-fa2d-4045-973f-c01c705be4ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["['.config', 'sample_data']\n","/root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video03_final\n","  File: annotations.xml\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video03_final/images\n","  File: frame_000110.PNG\n","  File: frame_000116.PNG\n","  File: frame_000395.PNG\n","  File: frame_000531.PNG\n","  File: frame_000158.PNG\n","  File: frame_000444.PNG\n","  File: frame_000168.PNG\n","  File: frame_000034.PNG\n","  File: frame_000361.PNG\n","  File: frame_000438.PNG\n","  File: frame_000023.PNG\n","  File: frame_000027.PNG\n","  File: frame_000277.PNG\n","  File: frame_000461.PNG\n","  File: frame_000388.PNG\n","  File: frame_000103.PNG\n","  File: frame_000186.PNG\n","  File: frame_000262.PNG\n","  File: frame_000468.PNG\n","  File: frame_000067.PNG\n","  File: frame_000005.PNG\n","  File: frame_000209.PNG\n","  File: frame_000242.PNG\n","  File: frame_000247.PNG\n","  File: frame_000493.PNG\n","  File: frame_000582.PNG\n","  File: frame_000569.PNG\n","  File: frame_000217.PNG\n","  File: frame_000021.PNG\n","  File: frame_000511.PNG\n","  File: frame_000414.PNG\n","  File: frame_000128.PNG\n","  File: frame_000058.PNG\n","  File: frame_000528.PNG\n","  File: frame_000534.PNG\n","  File: frame_000332.PNG\n","  File: frame_000198.PNG\n","  File: frame_000417.PNG\n","  File: frame_000093.PNG\n","  File: frame_000295.PNG\n","  File: frame_000235.PNG\n","  File: frame_000230.PNG\n","  File: frame_000157.PNG\n","  File: frame_000309.PNG\n","  File: frame_000540.PNG\n","  File: frame_000187.PNG\n","  File: frame_000221.PNG\n","  File: frame_000368.PNG\n","  File: frame_000055.PNG\n","  File: frame_000583.PNG\n","  File: frame_000087.PNG\n","  File: frame_000194.PNG\n","  File: frame_000107.PNG\n","  File: frame_000097.PNG\n","  File: frame_000318.PNG\n","  File: frame_000108.PNG\n","  File: frame_000485.PNG\n","  File: frame_000386.PNG\n","  File: frame_000416.PNG\n","  File: frame_000029.PNG\n","  File: frame_000182.PNG\n","  File: frame_000392.PNG\n","  File: frame_000476.PNG\n","  File: frame_000111.PNG\n","  File: frame_000147.PNG\n","  File: frame_000355.PNG\n","  File: frame_000196.PNG\n","  File: frame_000179.PNG\n","  File: frame_000296.PNG\n","  File: frame_000048.PNG\n","  File: frame_000222.PNG\n","  File: frame_000520.PNG\n","  File: frame_000251.PNG\n","  File: frame_000022.PNG\n","  File: frame_000246.PNG\n","  File: frame_000557.PNG\n","  File: frame_000418.PNG\n","  File: frame_000056.PNG\n","  File: frame_000458.PNG\n","  File: frame_000538.PNG\n","  File: frame_000390.PNG\n","  File: frame_000439.PNG\n","  File: frame_000449.PNG\n","  File: frame_000210.PNG\n","  File: frame_000016.PNG\n","  File: frame_000229.PNG\n","  File: frame_000279.PNG\n","  File: frame_000285.PNG\n","  File: frame_000199.PNG\n","  File: frame_000321.PNG\n","  File: frame_000330.PNG\n","  File: frame_000270.PNG\n","  File: frame_000423.PNG\n","  File: frame_000076.PNG\n","  File: frame_000237.PNG\n","  File: frame_000101.PNG\n","  File: frame_000516.PNG\n","  File: frame_000162.PNG\n","  File: frame_000521.PNG\n","  File: frame_000213.PNG\n","  File: frame_000334.PNG\n","  File: frame_000567.PNG\n","  File: frame_000203.PNG\n","  File: frame_000480.PNG\n","  File: frame_000435.PNG\n","  File: frame_000486.PNG\n","  File: frame_000104.PNG\n","  File: frame_000041.PNG\n","  File: frame_000033.PNG\n","  File: frame_000078.PNG\n","  File: frame_000507.PNG\n","  File: frame_000156.PNG\n","  File: frame_000164.PNG\n","  File: frame_000263.PNG\n","  File: frame_000555.PNG\n","  File: frame_000130.PNG\n","  File: frame_000422.PNG\n","  File: frame_000477.PNG\n","  File: frame_000483.PNG\n","  File: frame_000257.PNG\n","  File: frame_000383.PNG\n","  File: frame_000549.PNG\n","  File: frame_000552.PNG\n","  File: frame_000566.PNG\n","  File: frame_000169.PNG\n","  File: frame_000053.PNG\n","  File: frame_000159.PNG\n","  File: frame_000075.PNG\n","  File: frame_000399.PNG\n","  File: frame_000335.PNG\n","  File: frame_000030.PNG\n","  File: frame_000051.PNG\n","  File: frame_000371.PNG\n","  File: frame_000174.PNG\n","  File: frame_000434.PNG\n","  File: frame_000430.PNG\n","  File: frame_000155.PNG\n","  File: frame_000254.PNG\n","  File: frame_000170.PNG\n","  File: frame_000287.PNG\n","  File: frame_000133.PNG\n","  File: frame_000553.PNG\n","  File: frame_000471.PNG\n","  File: frame_000347.PNG\n","  File: frame_000381.PNG\n","  File: frame_000496.PNG\n","  File: frame_000492.PNG\n","  File: frame_000123.PNG\n","  File: frame_000303.PNG\n","  File: frame_000426.PNG\n","  File: frame_000046.PNG\n","  File: frame_000327.PNG\n","  File: frame_000580.PNG\n","  File: frame_000178.PNG\n","  File: frame_000394.PNG\n","  File: frame_000572.PNG\n","  File: frame_000518.PNG\n","  File: frame_000490.PNG\n","  File: frame_000578.PNG\n","  File: frame_000152.PNG\n","  File: frame_000124.PNG\n","  File: frame_000428.PNG\n","  File: frame_000200.PNG\n","  File: frame_000008.PNG\n","  File: frame_000598.PNG\n","  File: frame_000227.PNG\n","  File: frame_000581.PNG\n","  File: frame_000529.PNG\n","  File: frame_000585.PNG\n","  File: frame_000424.PNG\n","  File: frame_000079.PNG\n","  File: frame_000313.PNG\n","  File: frame_000234.PNG\n","  File: frame_000012.PNG\n","  File: frame_000166.PNG\n","  File: frame_000249.PNG\n","  File: frame_000267.PNG\n","  File: frame_000276.PNG\n","  File: frame_000219.PNG\n","  File: frame_000366.PNG\n","  File: frame_000352.PNG\n","  File: frame_000340.PNG\n","  File: frame_000324.PNG\n","  File: frame_000292.PNG\n","  File: frame_000563.PNG\n","  File: frame_000083.PNG\n","  File: frame_000054.PNG\n","  File: frame_000223.PNG\n","  File: frame_000391.PNG\n","  File: frame_000447.PNG\n","  File: frame_000057.PNG\n","  File: frame_000017.PNG\n","  File: frame_000375.PNG\n","  File: frame_000297.PNG\n","  File: frame_000398.PNG\n","  File: frame_000474.PNG\n","  File: frame_000264.PNG\n","  File: frame_000472.PNG\n","  File: frame_000497.PNG\n","  File: frame_000346.PNG\n","  File: frame_000000.PNG\n","  File: frame_000120.PNG\n","  File: frame_000291.PNG\n","  File: frame_000127.PNG\n","  File: frame_000167.PNG\n","  File: frame_000188.PNG\n","  File: frame_000129.PNG\n","  File: frame_000459.PNG\n","  File: frame_000301.PNG\n","  File: frame_000011.PNG\n","  File: frame_000215.PNG\n","  File: frame_000119.PNG\n","  File: frame_000510.PNG\n","  File: frame_000420.PNG\n","  File: frame_000406.PNG\n","  File: frame_000498.PNG\n","  File: frame_000184.PNG\n","  File: frame_000319.PNG\n","  File: frame_000010.PNG\n","  File: frame_000478.PNG\n","  File: frame_000360.PNG\n","  File: frame_000020.PNG\n","  File: frame_000134.PNG\n","  File: frame_000084.PNG\n","  File: frame_000126.PNG\n","  File: frame_000465.PNG\n","  File: frame_000070.PNG\n","  File: frame_000597.PNG\n","  File: frame_000042.PNG\n","  File: frame_000544.PNG\n","  File: frame_000589.PNG\n","  File: frame_000317.PNG\n","  File: frame_000069.PNG\n","  File: frame_000099.PNG\n","  File: frame_000085.PNG\n","  File: frame_000369.PNG\n","  File: frame_000460.PNG\n","  File: frame_000241.PNG\n","  File: frame_000266.PNG\n","  File: frame_000539.PNG\n","  File: frame_000535.PNG\n","  File: frame_000015.PNG\n","  File: frame_000451.PNG\n","  File: frame_000508.PNG\n","  File: frame_000312.PNG\n","  File: frame_000526.PNG\n","  File: frame_000331.PNG\n","  File: frame_000501.PNG\n","  File: frame_000244.PNG\n","  File: frame_000019.PNG\n","  File: frame_000205.PNG\n","  File: frame_000527.PNG\n","  File: frame_000537.PNG\n","  File: frame_000143.PNG\n","  File: frame_000356.PNG\n","  File: frame_000522.PNG\n","  File: frame_000001.PNG\n","  File: frame_000139.PNG\n","  File: frame_000283.PNG\n","  File: frame_000378.PNG\n","  File: frame_000336.PNG\n","  File: frame_000523.PNG\n","  File: frame_000429.PNG\n","  File: frame_000144.PNG\n","  File: frame_000064.PNG\n","  File: frame_000172.PNG\n","  File: frame_000066.PNG\n","  File: frame_000481.PNG\n","  File: frame_000519.PNG\n","  File: frame_000571.PNG\n","  File: frame_000036.PNG\n","  File: frame_000546.PNG\n","  File: frame_000311.PNG\n","  File: frame_000259.PNG\n","  File: frame_000142.PNG\n","  File: frame_000509.PNG\n","  File: frame_000562.PNG\n","  File: frame_000308.PNG\n","  File: frame_000141.PNG\n","  File: frame_000408.PNG\n","  File: frame_000269.PNG\n","  File: frame_000433.PNG\n","  File: frame_000446.PNG\n","  File: frame_000204.PNG\n","  File: frame_000181.PNG\n","  File: frame_000341.PNG\n","  File: frame_000310.PNG\n","  File: frame_000385.PNG\n","  File: frame_000413.PNG\n","  File: frame_000594.PNG\n","  File: frame_000587.PNG\n","  File: frame_000082.PNG\n","  File: frame_000512.PNG\n","  File: frame_000271.PNG\n","  File: frame_000464.PNG\n","  File: frame_000536.PNG\n","  File: frame_000547.PNG\n","  File: frame_000382.PNG\n","  File: frame_000163.PNG\n","  File: frame_000367.PNG\n","  File: frame_000201.PNG\n","  File: frame_000278.PNG\n","  File: frame_000148.PNG\n","  File: frame_000250.PNG\n","  File: frame_000091.PNG\n","  File: frame_000574.PNG\n","  File: frame_000153.PNG\n","  File: frame_000052.PNG\n","  File: frame_000135.PNG\n","  File: frame_000265.PNG\n","  File: frame_000243.PNG\n","  File: frame_000577.PNG\n","  File: frame_000559.PNG\n","  File: frame_000239.PNG\n","  File: frame_000043.PNG\n","  File: frame_000185.PNG\n","  File: frame_000208.PNG\n","  File: frame_000339.PNG\n","  File: frame_000348.PNG\n","  File: frame_000045.PNG\n","  File: frame_000115.PNG\n","  File: frame_000028.PNG\n","  File: frame_000098.PNG\n","  File: frame_000211.PNG\n","  File: frame_000440.PNG\n","  File: frame_000226.PNG\n","  File: frame_000225.PNG\n","  File: frame_000349.PNG\n","  File: frame_000068.PNG\n","  File: frame_000183.PNG\n","  File: frame_000032.PNG\n","  File: frame_000092.PNG\n","  File: frame_000323.PNG\n","  File: frame_000146.PNG\n","  File: frame_000405.PNG\n","  File: frame_000338.PNG\n","  File: frame_000556.PNG\n","  File: frame_000154.PNG\n","  File: frame_000548.PNG\n","  File: frame_000025.PNG\n","  File: frame_000112.PNG\n","  File: frame_000300.PNG\n","  File: frame_000384.PNG\n","  File: frame_000145.PNG\n","  File: frame_000245.PNG\n","  File: frame_000441.PNG\n","  File: frame_000047.PNG\n","  File: frame_000506.PNG\n","  File: frame_000473.PNG\n","  File: frame_000491.PNG\n","  File: frame_000379.PNG\n","  File: frame_000149.PNG\n","  File: frame_000325.PNG\n","  File: frame_000191.PNG\n","  File: frame_000351.PNG\n","  File: frame_000024.PNG\n","  File: frame_000503.PNG\n","  File: frame_000561.PNG\n","  File: frame_000374.PNG\n","  File: frame_000591.PNG\n","  File: frame_000402.PNG\n","  File: frame_000125.PNG\n","  File: frame_000533.PNG\n","  File: frame_000450.PNG\n","  File: frame_000412.PNG\n","  File: frame_000380.PNG\n","  File: frame_000419.PNG\n","  File: frame_000320.PNG\n","  File: frame_000118.PNG\n","  File: frame_000065.PNG\n","  File: frame_000206.PNG\n","  File: frame_000342.PNG\n","  File: frame_000002.PNG\n","  File: frame_000060.PNG\n","  File: frame_000502.PNG\n","  File: frame_000218.PNG\n","  File: frame_000364.PNG\n","  File: frame_000568.PNG\n","  File: frame_000049.PNG\n","  File: frame_000248.PNG\n","  File: frame_000176.PNG\n","  File: frame_000404.PNG\n","  File: frame_000513.PNG\n","  File: frame_000427.PNG\n","  File: frame_000306.PNG\n","  File: frame_000073.PNG\n","  File: frame_000038.PNG\n","  File: frame_000541.PNG\n","  File: frame_000294.PNG\n","  File: frame_000586.PNG\n","  File: frame_000299.PNG\n","  File: frame_000442.PNG\n","  File: frame_000315.PNG\n","  File: frame_000596.PNG\n","  File: frame_000233.PNG\n","  File: frame_000256.PNG\n","  File: frame_000397.PNG\n","  File: frame_000445.PNG\n","  File: frame_000117.PNG\n","  File: frame_000260.PNG\n","  File: frame_000504.PNG\n","  File: frame_000337.PNG\n","  File: frame_000455.PNG\n","  File: frame_000329.PNG\n","  File: frame_000165.PNG\n","  File: frame_000100.PNG\n","  File: frame_000542.PNG\n","  File: frame_000592.PNG\n","  File: frame_000151.PNG\n","  File: frame_000573.PNG\n","  File: frame_000353.PNG\n","  File: frame_000333.PNG\n","  File: frame_000122.PNG\n","  File: frame_000359.PNG\n","  File: frame_000063.PNG\n","  File: frame_000350.PNG\n","  File: frame_000114.PNG\n","  File: frame_000466.PNG\n","  File: frame_000304.PNG\n","  File: frame_000595.PNG\n","  File: frame_000080.PNG\n","  File: frame_000524.PNG\n","  File: frame_000113.PNG\n","  File: frame_000579.PNG\n","  File: frame_000354.PNG\n","  File: frame_000035.PNG\n","  File: frame_000437.PNG\n","  File: frame_000190.PNG\n","  File: frame_000275.PNG\n","  File: frame_000040.PNG\n","  File: frame_000044.PNG\n","  File: frame_000050.PNG\n","  File: frame_000096.PNG\n","  File: frame_000505.PNG\n","  File: frame_000140.PNG\n","  File: frame_000175.PNG\n","  File: frame_000443.PNG\n","  File: frame_000316.PNG\n","  File: frame_000062.PNG\n","  File: frame_000389.PNG\n","  File: frame_000525.PNG\n","  File: frame_000415.PNG\n","  File: frame_000456.PNG\n","  File: frame_000220.PNG\n","  File: frame_000255.PNG\n","  File: frame_000370.PNG\n","  File: frame_000532.PNG\n","  File: frame_000059.PNG\n","  File: frame_000482.PNG\n","  File: frame_000564.PNG\n","  File: frame_000432.PNG\n","  File: frame_000006.PNG\n","  File: frame_000095.PNG\n","  File: frame_000290.PNG\n","  File: frame_000584.PNG\n","  File: frame_000286.PNG\n","  File: frame_000061.PNG\n","  File: frame_000138.PNG\n","  File: frame_000121.PNG\n","  File: frame_000457.PNG\n","  File: frame_000253.PNG\n","  File: frame_000463.PNG\n","  File: frame_000086.PNG\n","  File: frame_000007.PNG\n","  File: frame_000009.PNG\n","  File: frame_000224.PNG\n","  File: frame_000088.PNG\n","  File: frame_000431.PNG\n","  File: frame_000014.PNG\n","  File: frame_000268.PNG\n","  File: frame_000171.PNG\n","  File: frame_000393.PNG\n","  File: frame_000193.PNG\n","  File: frame_000284.PNG\n","  File: frame_000387.PNG\n","  File: frame_000252.PNG\n","  File: frame_000326.PNG\n","  File: frame_000343.PNG\n","  File: frame_000590.PNG\n","  File: frame_000401.PNG\n","  File: frame_000288.PNG\n","  File: frame_000328.PNG\n","  File: frame_000274.PNG\n","  File: frame_000106.PNG\n","  File: frame_000077.PNG\n","  File: frame_000517.PNG\n","  File: frame_000576.PNG\n","  File: frame_000575.PNG\n","  File: frame_000136.PNG\n","  File: frame_000131.PNG\n","  File: frame_000231.PNG\n","  File: frame_000484.PNG\n","  File: frame_000377.PNG\n","  File: frame_000565.PNG\n","  File: frame_000109.PNG\n","  File: frame_000462.PNG\n","  File: frame_000018.PNG\n","  File: frame_000293.PNG\n","  File: frame_000302.PNG\n","  File: frame_000425.PNG\n","  File: frame_000240.PNG\n","  File: frame_000307.PNG\n","  File: frame_000160.PNG\n","  File: frame_000031.PNG\n","  File: frame_000281.PNG\n","  File: frame_000289.PNG\n","  File: frame_000071.PNG\n","  File: frame_000600.PNG\n","  File: frame_000305.PNG\n","  File: frame_000004.PNG\n","  File: frame_000470.PNG\n","  File: frame_000554.PNG\n","  File: frame_000570.PNG\n","  File: frame_000372.PNG\n","  File: frame_000314.PNG\n","  File: frame_000094.PNG\n","  File: frame_000137.PNG\n","  File: frame_000357.PNG\n","  File: frame_000238.PNG\n","  File: frame_000403.PNG\n","  File: frame_000543.PNG\n","  File: frame_000345.PNG\n","  File: frame_000489.PNG\n","  File: frame_000373.PNG\n","  File: frame_000448.PNG\n","  File: frame_000090.PNG\n","  File: frame_000207.PNG\n","  File: frame_000545.PNG\n","  File: frame_000192.PNG\n","  File: frame_000407.PNG\n","  File: frame_000363.PNG\n","  File: frame_000421.PNG\n","  File: frame_000282.PNG\n","  File: frame_000453.PNG\n","  File: frame_000232.PNG\n","  File: frame_000272.PNG\n","  File: frame_000396.PNG\n","  File: frame_000344.PNG\n","  File: frame_000410.PNG\n","  File: frame_000500.PNG\n","  File: frame_000026.PNG\n","  File: frame_000436.PNG\n","  File: frame_000322.PNG\n","  File: frame_000189.PNG\n","  File: frame_000514.PNG\n","  File: frame_000560.PNG\n","  File: frame_000081.PNG\n","  File: frame_000089.PNG\n","  File: frame_000037.PNG\n","  File: frame_000593.PNG\n","  File: frame_000195.PNG\n","  File: frame_000358.PNG\n","  File: frame_000074.PNG\n","  File: frame_000003.PNG\n","  File: frame_000105.PNG\n","  File: frame_000409.PNG\n","  File: frame_000515.PNG\n","  File: frame_000132.PNG\n","  File: frame_000072.PNG\n","  File: frame_000551.PNG\n","  File: frame_000411.PNG\n","  File: frame_000362.PNG\n","  File: frame_000495.PNG\n","  File: frame_000177.PNG\n","  File: frame_000599.PNG\n","  File: frame_000454.PNG\n","  File: frame_000197.PNG\n","  File: frame_000530.PNG\n","  File: frame_000173.PNG\n","  File: frame_000550.PNG\n","  File: frame_000469.PNG\n","  File: frame_000479.PNG\n","  File: frame_000487.PNG\n","  File: frame_000039.PNG\n","  File: frame_000400.PNG\n","  File: frame_000258.PNG\n","  File: frame_000236.PNG\n","  File: frame_000202.PNG\n","  File: frame_000588.PNG\n","  File: frame_000214.PNG\n","  File: frame_000365.PNG\n","  File: frame_000102.PNG\n","  File: frame_000273.PNG\n","  File: frame_000216.PNG\n","  File: frame_000150.PNG\n","  File: frame_000467.PNG\n","  File: frame_000013.PNG\n","  File: frame_000298.PNG\n","  File: frame_000212.PNG\n","  File: frame_000376.PNG\n","  File: frame_000499.PNG\n","  File: frame_000261.PNG\n","  File: frame_000161.PNG\n","  File: frame_000228.PNG\n","  File: frame_000475.PNG\n","  File: frame_000452.PNG\n","  File: frame_000180.PNG\n","  File: frame_000488.PNG\n","  File: frame_000558.PNG\n","  File: frame_000280.PNG\n","  File: frame_000494.PNG\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video04_final\n","  File: annotations.xml\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video04_final/images\n","  File: frame_000110.PNG\n","  File: frame_000116.PNG\n","  File: frame_000395.PNG\n","  File: frame_000531.PNG\n","  File: frame_000158.PNG\n","  File: frame_000444.PNG\n","  File: frame_000168.PNG\n","  File: frame_000034.PNG\n","  File: frame_000361.PNG\n","  File: frame_000438.PNG\n","  File: frame_000023.PNG\n","  File: frame_000027.PNG\n","  File: frame_000277.PNG\n","  File: frame_000461.PNG\n","  File: frame_000388.PNG\n","  File: frame_000103.PNG\n","  File: frame_000186.PNG\n","  File: frame_000262.PNG\n","  File: frame_000468.PNG\n","  File: frame_000067.PNG\n","  File: frame_000005.PNG\n","  File: frame_000209.PNG\n","  File: frame_000242.PNG\n","  File: frame_000247.PNG\n","  File: frame_000493.PNG\n","  File: frame_000582.PNG\n","  File: frame_000569.PNG\n","  File: frame_000217.PNG\n","  File: frame_000021.PNG\n","  File: frame_000511.PNG\n","  File: frame_000414.PNG\n","  File: frame_000128.PNG\n","  File: frame_000058.PNG\n","  File: frame_000528.PNG\n","  File: frame_000534.PNG\n","  File: frame_000332.PNG\n","  File: frame_000198.PNG\n","  File: frame_000417.PNG\n","  File: frame_000093.PNG\n","  File: frame_000295.PNG\n","  File: frame_000235.PNG\n","  File: frame_000230.PNG\n","  File: frame_000157.PNG\n","  File: frame_000309.PNG\n","  File: frame_000540.PNG\n","  File: frame_000187.PNG\n","  File: frame_000221.PNG\n","  File: frame_000368.PNG\n","  File: frame_000055.PNG\n","  File: frame_000583.PNG\n","  File: frame_000087.PNG\n","  File: frame_000194.PNG\n","  File: frame_000107.PNG\n","  File: frame_000097.PNG\n","  File: frame_000318.PNG\n","  File: frame_000108.PNG\n","  File: frame_000485.PNG\n","  File: frame_000386.PNG\n","  File: frame_000416.PNG\n","  File: frame_000029.PNG\n","  File: frame_000182.PNG\n","  File: frame_000392.PNG\n","  File: frame_000476.PNG\n","  File: frame_000111.PNG\n","  File: frame_000147.PNG\n","  File: frame_000355.PNG\n","  File: frame_000196.PNG\n","  File: frame_000179.PNG\n","  File: frame_000296.PNG\n","  File: frame_000048.PNG\n","  File: frame_000222.PNG\n","  File: frame_000520.PNG\n","  File: frame_000251.PNG\n","  File: frame_000022.PNG\n","  File: frame_000246.PNG\n","  File: frame_000557.PNG\n","  File: frame_000418.PNG\n","  File: frame_000056.PNG\n","  File: frame_000458.PNG\n","  File: frame_000538.PNG\n","  File: frame_000390.PNG\n","  File: frame_000439.PNG\n","  File: frame_000449.PNG\n","  File: frame_000210.PNG\n","  File: frame_000016.PNG\n","  File: frame_000229.PNG\n","  File: frame_000279.PNG\n","  File: frame_000285.PNG\n","  File: frame_000199.PNG\n","  File: frame_000321.PNG\n","  File: frame_000330.PNG\n","  File: frame_000270.PNG\n","  File: frame_000423.PNG\n","  File: frame_000076.PNG\n","  File: frame_000237.PNG\n","  File: frame_000101.PNG\n","  File: frame_000516.PNG\n","  File: frame_000162.PNG\n","  File: frame_000521.PNG\n","  File: frame_000213.PNG\n","  File: frame_000334.PNG\n","  File: frame_000567.PNG\n","  File: frame_000203.PNG\n","  File: frame_000480.PNG\n","  File: frame_000435.PNG\n","  File: frame_000486.PNG\n","  File: frame_000104.PNG\n","  File: frame_000041.PNG\n","  File: frame_000033.PNG\n","  File: frame_000078.PNG\n","  File: frame_000507.PNG\n","  File: frame_000156.PNG\n","  File: frame_000164.PNG\n","  File: frame_000263.PNG\n","  File: frame_000555.PNG\n","  File: frame_000130.PNG\n","  File: frame_000422.PNG\n","  File: frame_000477.PNG\n","  File: frame_000483.PNG\n","  File: frame_000257.PNG\n","  File: frame_000383.PNG\n","  File: frame_000549.PNG\n","  File: frame_000552.PNG\n","  File: frame_000566.PNG\n","  File: frame_000169.PNG\n","  File: frame_000053.PNG\n","  File: frame_000159.PNG\n","  File: frame_000075.PNG\n","  File: frame_000399.PNG\n","  File: frame_000335.PNG\n","  File: frame_000030.PNG\n","  File: frame_000051.PNG\n","  File: frame_000371.PNG\n","  File: frame_000174.PNG\n","  File: frame_000434.PNG\n","  File: frame_000430.PNG\n","  File: frame_000155.PNG\n","  File: frame_000254.PNG\n","  File: frame_000170.PNG\n","  File: frame_000287.PNG\n","  File: frame_000133.PNG\n","  File: frame_000553.PNG\n","  File: frame_000471.PNG\n","  File: frame_000347.PNG\n","  File: frame_000381.PNG\n","  File: frame_000496.PNG\n","  File: frame_000492.PNG\n","  File: frame_000123.PNG\n","  File: frame_000303.PNG\n","  File: frame_000426.PNG\n","  File: frame_000046.PNG\n","  File: frame_000327.PNG\n","  File: frame_000580.PNG\n","  File: frame_000178.PNG\n","  File: frame_000394.PNG\n","  File: frame_000572.PNG\n","  File: frame_000518.PNG\n","  File: frame_000490.PNG\n","  File: frame_000578.PNG\n","  File: frame_000152.PNG\n","  File: frame_000124.PNG\n","  File: frame_000428.PNG\n","  File: frame_000200.PNG\n","  File: frame_000008.PNG\n","  File: frame_000598.PNG\n","  File: frame_000227.PNG\n","  File: frame_000581.PNG\n","  File: frame_000529.PNG\n","  File: frame_000585.PNG\n","  File: frame_000424.PNG\n","  File: frame_000079.PNG\n","  File: frame_000313.PNG\n","  File: frame_000234.PNG\n","  File: frame_000012.PNG\n","  File: frame_000166.PNG\n","  File: frame_000249.PNG\n","  File: frame_000267.PNG\n","  File: frame_000276.PNG\n","  File: frame_000219.PNG\n","  File: frame_000366.PNG\n","  File: frame_000352.PNG\n","  File: frame_000340.PNG\n","  File: frame_000324.PNG\n","  File: frame_000292.PNG\n","  File: frame_000563.PNG\n","  File: frame_000083.PNG\n","  File: frame_000054.PNG\n","  File: frame_000223.PNG\n","  File: frame_000391.PNG\n","  File: frame_000447.PNG\n","  File: frame_000057.PNG\n","  File: frame_000017.PNG\n","  File: frame_000375.PNG\n","  File: frame_000297.PNG\n","  File: frame_000398.PNG\n","  File: frame_000474.PNG\n","  File: frame_000264.PNG\n","  File: frame_000472.PNG\n","  File: frame_000497.PNG\n","  File: frame_000346.PNG\n","  File: frame_000000.PNG\n","  File: frame_000120.PNG\n","  File: frame_000291.PNG\n","  File: frame_000127.PNG\n","  File: frame_000167.PNG\n","  File: frame_000188.PNG\n","  File: frame_000129.PNG\n","  File: frame_000459.PNG\n","  File: frame_000301.PNG\n","  File: frame_000011.PNG\n","  File: frame_000215.PNG\n","  File: frame_000119.PNG\n","  File: frame_000510.PNG\n","  File: frame_000420.PNG\n","  File: frame_000406.PNG\n","  File: frame_000498.PNG\n","  File: frame_000184.PNG\n","  File: frame_000319.PNG\n","  File: frame_000010.PNG\n","  File: frame_000478.PNG\n","  File: frame_000360.PNG\n","  File: frame_000020.PNG\n","  File: frame_000134.PNG\n","  File: frame_000084.PNG\n","  File: frame_000126.PNG\n","  File: frame_000465.PNG\n","  File: frame_000070.PNG\n","  File: frame_000597.PNG\n","  File: frame_000042.PNG\n","  File: frame_000544.PNG\n","  File: frame_000589.PNG\n","  File: frame_000317.PNG\n","  File: frame_000069.PNG\n","  File: frame_000099.PNG\n","  File: frame_000085.PNG\n","  File: frame_000369.PNG\n","  File: frame_000460.PNG\n","  File: frame_000241.PNG\n","  File: frame_000266.PNG\n","  File: frame_000539.PNG\n","  File: frame_000535.PNG\n","  File: frame_000015.PNG\n","  File: frame_000451.PNG\n","  File: frame_000508.PNG\n","  File: frame_000312.PNG\n","  File: frame_000526.PNG\n","  File: frame_000331.PNG\n","  File: frame_000501.PNG\n","  File: frame_000244.PNG\n","  File: frame_000019.PNG\n","  File: frame_000205.PNG\n","  File: frame_000527.PNG\n","  File: frame_000537.PNG\n","  File: frame_000143.PNG\n","  File: frame_000356.PNG\n","  File: frame_000522.PNG\n","  File: frame_000001.PNG\n","  File: frame_000139.PNG\n","  File: frame_000283.PNG\n","  File: frame_000378.PNG\n","  File: frame_000336.PNG\n","  File: frame_000523.PNG\n","  File: frame_000429.PNG\n","  File: frame_000144.PNG\n","  File: frame_000064.PNG\n","  File: frame_000172.PNG\n","  File: frame_000066.PNG\n","  File: frame_000481.PNG\n","  File: frame_000519.PNG\n","  File: frame_000571.PNG\n","  File: frame_000036.PNG\n","  File: frame_000546.PNG\n","  File: frame_000311.PNG\n","  File: frame_000259.PNG\n","  File: frame_000142.PNG\n","  File: frame_000509.PNG\n","  File: frame_000562.PNG\n","  File: frame_000308.PNG\n","  File: frame_000141.PNG\n","  File: frame_000408.PNG\n","  File: frame_000269.PNG\n","  File: frame_000433.PNG\n","  File: frame_000446.PNG\n","  File: frame_000204.PNG\n","  File: frame_000181.PNG\n","  File: frame_000341.PNG\n","  File: frame_000310.PNG\n","  File: frame_000385.PNG\n","  File: frame_000413.PNG\n","  File: frame_000594.PNG\n","  File: frame_000587.PNG\n","  File: frame_000082.PNG\n","  File: frame_000512.PNG\n","  File: frame_000271.PNG\n","  File: frame_000464.PNG\n","  File: frame_000536.PNG\n","  File: frame_000547.PNG\n","  File: frame_000382.PNG\n","  File: frame_000163.PNG\n","  File: frame_000367.PNG\n","  File: frame_000201.PNG\n","  File: frame_000278.PNG\n","  File: frame_000148.PNG\n","  File: frame_000250.PNG\n","  File: frame_000091.PNG\n","  File: frame_000574.PNG\n","  File: frame_000153.PNG\n","  File: frame_000052.PNG\n","  File: frame_000135.PNG\n","  File: frame_000265.PNG\n","  File: frame_000243.PNG\n","  File: frame_000577.PNG\n","  File: frame_000559.PNG\n","  File: frame_000239.PNG\n","  File: frame_000043.PNG\n","  File: frame_000185.PNG\n","  File: frame_000208.PNG\n","  File: frame_000339.PNG\n","  File: frame_000348.PNG\n","  File: frame_000045.PNG\n","  File: frame_000115.PNG\n","  File: frame_000028.PNG\n","  File: frame_000098.PNG\n","  File: frame_000211.PNG\n","  File: frame_000440.PNG\n","  File: frame_000226.PNG\n","  File: frame_000225.PNG\n","  File: frame_000349.PNG\n","  File: frame_000068.PNG\n","  File: frame_000183.PNG\n","  File: frame_000032.PNG\n","  File: frame_000092.PNG\n","  File: frame_000323.PNG\n","  File: frame_000146.PNG\n","  File: frame_000405.PNG\n","  File: frame_000338.PNG\n","  File: frame_000556.PNG\n","  File: frame_000154.PNG\n","  File: frame_000548.PNG\n","  File: frame_000025.PNG\n","  File: frame_000112.PNG\n","  File: frame_000300.PNG\n","  File: frame_000384.PNG\n","  File: frame_000145.PNG\n","  File: frame_000245.PNG\n","  File: frame_000441.PNG\n","  File: frame_000047.PNG\n","  File: frame_000506.PNG\n","  File: frame_000473.PNG\n","  File: frame_000491.PNG\n","  File: frame_000379.PNG\n","  File: frame_000149.PNG\n","  File: frame_000325.PNG\n","  File: frame_000191.PNG\n","  File: frame_000351.PNG\n","  File: frame_000024.PNG\n","  File: frame_000503.PNG\n","  File: frame_000561.PNG\n","  File: frame_000374.PNG\n","  File: frame_000591.PNG\n","  File: frame_000402.PNG\n","  File: frame_000125.PNG\n","  File: frame_000533.PNG\n","  File: frame_000450.PNG\n","  File: frame_000412.PNG\n","  File: frame_000380.PNG\n","  File: frame_000419.PNG\n","  File: frame_000320.PNG\n","  File: frame_000118.PNG\n","  File: frame_000065.PNG\n","  File: frame_000206.PNG\n","  File: frame_000342.PNG\n","  File: frame_000002.PNG\n","  File: frame_000060.PNG\n","  File: frame_000502.PNG\n","  File: frame_000218.PNG\n","  File: frame_000364.PNG\n","  File: frame_000568.PNG\n","  File: frame_000049.PNG\n","  File: frame_000248.PNG\n","  File: frame_000176.PNG\n","  File: frame_000404.PNG\n","  File: frame_000513.PNG\n","  File: frame_000427.PNG\n","  File: frame_000306.PNG\n","  File: frame_000073.PNG\n","  File: frame_000038.PNG\n","  File: frame_000541.PNG\n","  File: frame_000294.PNG\n","  File: frame_000586.PNG\n","  File: frame_000299.PNG\n","  File: frame_000442.PNG\n","  File: frame_000315.PNG\n","  File: frame_000596.PNG\n","  File: frame_000233.PNG\n","  File: frame_000256.PNG\n","  File: frame_000397.PNG\n","  File: frame_000445.PNG\n","  File: frame_000117.PNG\n","  File: frame_000260.PNG\n","  File: frame_000504.PNG\n","  File: frame_000337.PNG\n","  File: frame_000455.PNG\n","  File: frame_000329.PNG\n","  File: frame_000165.PNG\n","  File: frame_000100.PNG\n","  File: frame_000542.PNG\n","  File: frame_000592.PNG\n","  File: frame_000151.PNG\n","  File: frame_000573.PNG\n","  File: frame_000353.PNG\n","  File: frame_000333.PNG\n","  File: frame_000122.PNG\n","  File: frame_000359.PNG\n","  File: frame_000063.PNG\n","  File: frame_000350.PNG\n","  File: frame_000114.PNG\n","  File: frame_000466.PNG\n","  File: frame_000304.PNG\n","  File: frame_000595.PNG\n","  File: frame_000080.PNG\n","  File: frame_000524.PNG\n","  File: frame_000113.PNG\n","  File: frame_000579.PNG\n","  File: frame_000354.PNG\n","  File: frame_000035.PNG\n","  File: frame_000437.PNG\n","  File: frame_000190.PNG\n","  File: frame_000275.PNG\n","  File: frame_000040.PNG\n","  File: frame_000044.PNG\n","  File: frame_000050.PNG\n","  File: frame_000096.PNG\n","  File: frame_000505.PNG\n","  File: frame_000140.PNG\n","  File: frame_000175.PNG\n","  File: frame_000443.PNG\n","  File: frame_000316.PNG\n","  File: frame_000062.PNG\n","  File: frame_000389.PNG\n","  File: frame_000525.PNG\n","  File: frame_000415.PNG\n","  File: frame_000456.PNG\n","  File: frame_000220.PNG\n","  File: frame_000255.PNG\n","  File: frame_000370.PNG\n","  File: frame_000532.PNG\n","  File: frame_000059.PNG\n","  File: frame_000482.PNG\n","  File: frame_000564.PNG\n","  File: frame_000432.PNG\n","  File: frame_000006.PNG\n","  File: frame_000095.PNG\n","  File: frame_000290.PNG\n","  File: frame_000584.PNG\n","  File: frame_000286.PNG\n","  File: frame_000061.PNG\n","  File: frame_000138.PNG\n","  File: frame_000121.PNG\n","  File: frame_000457.PNG\n","  File: frame_000253.PNG\n","  File: frame_000463.PNG\n","  File: frame_000086.PNG\n","  File: frame_000007.PNG\n","  File: frame_000009.PNG\n","  File: frame_000224.PNG\n","  File: frame_000088.PNG\n","  File: frame_000431.PNG\n","  File: frame_000014.PNG\n","  File: frame_000268.PNG\n","  File: frame_000171.PNG\n","  File: frame_000393.PNG\n","  File: frame_000193.PNG\n","  File: frame_000284.PNG\n","  File: frame_000387.PNG\n","  File: frame_000252.PNG\n","  File: frame_000326.PNG\n","  File: frame_000343.PNG\n","  File: frame_000590.PNG\n","  File: frame_000401.PNG\n","  File: frame_000288.PNG\n","  File: frame_000328.PNG\n","  File: frame_000274.PNG\n","  File: frame_000106.PNG\n","  File: frame_000077.PNG\n","  File: frame_000517.PNG\n","  File: frame_000576.PNG\n","  File: frame_000575.PNG\n","  File: frame_000136.PNG\n","  File: frame_000131.PNG\n","  File: frame_000231.PNG\n","  File: frame_000484.PNG\n","  File: frame_000377.PNG\n","  File: frame_000565.PNG\n","  File: frame_000109.PNG\n","  File: frame_000462.PNG\n","  File: frame_000018.PNG\n","  File: frame_000293.PNG\n","  File: frame_000302.PNG\n","  File: frame_000425.PNG\n","  File: frame_000240.PNG\n","  File: frame_000307.PNG\n","  File: frame_000160.PNG\n","  File: frame_000031.PNG\n","  File: frame_000281.PNG\n","  File: frame_000289.PNG\n","  File: frame_000071.PNG\n","  File: frame_000305.PNG\n","  File: frame_000004.PNG\n","  File: frame_000470.PNG\n","  File: frame_000554.PNG\n","  File: frame_000570.PNG\n","  File: frame_000372.PNG\n","  File: frame_000314.PNG\n","  File: frame_000094.PNG\n","  File: frame_000137.PNG\n","  File: frame_000357.PNG\n","  File: frame_000238.PNG\n","  File: frame_000403.PNG\n","  File: frame_000543.PNG\n","  File: frame_000345.PNG\n","  File: frame_000489.PNG\n","  File: frame_000373.PNG\n","  File: frame_000448.PNG\n","  File: frame_000090.PNG\n","  File: frame_000207.PNG\n","  File: frame_000545.PNG\n","  File: frame_000192.PNG\n","  File: frame_000407.PNG\n","  File: frame_000363.PNG\n","  File: frame_000421.PNG\n","  File: frame_000282.PNG\n","  File: frame_000453.PNG\n","  File: frame_000232.PNG\n","  File: frame_000272.PNG\n","  File: frame_000396.PNG\n","  File: frame_000344.PNG\n","  File: frame_000410.PNG\n","  File: frame_000500.PNG\n","  File: frame_000026.PNG\n","  File: frame_000436.PNG\n","  File: frame_000322.PNG\n","  File: frame_000189.PNG\n","  File: frame_000514.PNG\n","  File: frame_000560.PNG\n","  File: frame_000081.PNG\n","  File: frame_000089.PNG\n","  File: frame_000037.PNG\n","  File: frame_000593.PNG\n","  File: frame_000195.PNG\n","  File: frame_000358.PNG\n","  File: frame_000074.PNG\n","  File: frame_000003.PNG\n","  File: frame_000105.PNG\n","  File: frame_000409.PNG\n","  File: frame_000515.PNG\n","  File: frame_000132.PNG\n","  File: frame_000072.PNG\n","  File: frame_000551.PNG\n","  File: frame_000411.PNG\n","  File: frame_000362.PNG\n","  File: frame_000495.PNG\n","  File: frame_000177.PNG\n","  File: frame_000599.PNG\n","  File: frame_000454.PNG\n","  File: frame_000197.PNG\n","  File: frame_000530.PNG\n","  File: frame_000173.PNG\n","  File: frame_000550.PNG\n","  File: frame_000469.PNG\n","  File: frame_000479.PNG\n","  File: frame_000487.PNG\n","  File: frame_000039.PNG\n","  File: frame_000400.PNG\n","  File: frame_000258.PNG\n","  File: frame_000236.PNG\n","  File: frame_000202.PNG\n","  File: frame_000588.PNG\n","  File: frame_000214.PNG\n","  File: frame_000365.PNG\n","  File: frame_000102.PNG\n","  File: frame_000273.PNG\n","  File: frame_000216.PNG\n","  File: frame_000150.PNG\n","  File: frame_000467.PNG\n","  File: frame_000013.PNG\n","  File: frame_000298.PNG\n","  File: frame_000212.PNG\n","  File: frame_000376.PNG\n","  File: frame_000499.PNG\n","  File: frame_000261.PNG\n","  File: frame_000161.PNG\n","  File: frame_000228.PNG\n","  File: frame_000475.PNG\n","  File: frame_000452.PNG\n","  File: frame_000180.PNG\n","  File: frame_000488.PNG\n","  File: frame_000558.PNG\n","  File: frame_000280.PNG\n","  File: frame_000494.PNG\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video05_final\n","  File: annotations.xml\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video05_final/images\n","  File: frame_000110.PNG\n","  File: frame_000116.PNG\n","  File: frame_000395.PNG\n","  File: frame_000531.PNG\n","  File: frame_000158.PNG\n","  File: frame_000444.PNG\n","  File: frame_000168.PNG\n","  File: frame_000034.PNG\n","  File: frame_000361.PNG\n","  File: frame_000438.PNG\n","  File: frame_000023.PNG\n","  File: frame_000027.PNG\n","  File: frame_000277.PNG\n","  File: frame_000461.PNG\n","  File: frame_000388.PNG\n","  File: frame_000103.PNG\n","  File: frame_000186.PNG\n","  File: frame_000262.PNG\n","  File: frame_000468.PNG\n","  File: frame_000067.PNG\n","  File: frame_000005.PNG\n","  File: frame_000209.PNG\n","  File: frame_000242.PNG\n","  File: frame_000247.PNG\n","  File: frame_000493.PNG\n","  File: frame_000582.PNG\n","  File: frame_000569.PNG\n","  File: frame_000217.PNG\n","  File: frame_000021.PNG\n","  File: frame_000511.PNG\n","  File: frame_000414.PNG\n","  File: frame_000128.PNG\n","  File: frame_000058.PNG\n","  File: frame_000528.PNG\n","  File: frame_000534.PNG\n","  File: frame_000332.PNG\n","  File: frame_000198.PNG\n","  File: frame_000417.PNG\n","  File: frame_000093.PNG\n","  File: frame_000295.PNG\n","  File: frame_000235.PNG\n","  File: frame_000230.PNG\n","  File: frame_000157.PNG\n","  File: frame_000309.PNG\n","  File: frame_000540.PNG\n","  File: frame_000187.PNG\n","  File: frame_000221.PNG\n","  File: frame_000368.PNG\n","  File: frame_000055.PNG\n","  File: frame_000583.PNG\n","  File: frame_000087.PNG\n","  File: frame_000194.PNG\n","  File: frame_000107.PNG\n","  File: frame_000097.PNG\n","  File: frame_000318.PNG\n","  File: frame_000108.PNG\n","  File: frame_000485.PNG\n","  File: frame_000386.PNG\n","  File: frame_000416.PNG\n","  File: frame_000029.PNG\n","  File: frame_000182.PNG\n","  File: frame_000392.PNG\n","  File: frame_000476.PNG\n","  File: frame_000111.PNG\n","  File: frame_000147.PNG\n","  File: frame_000355.PNG\n","  File: frame_000196.PNG\n","  File: frame_000179.PNG\n","  File: frame_000296.PNG\n","  File: frame_000048.PNG\n","  File: frame_000222.PNG\n","  File: frame_000520.PNG\n","  File: frame_000251.PNG\n","  File: frame_000022.PNG\n","  File: frame_000246.PNG\n","  File: frame_000557.PNG\n","  File: frame_000418.PNG\n","  File: frame_000056.PNG\n","  File: frame_000458.PNG\n","  File: frame_000538.PNG\n","  File: frame_000390.PNG\n","  File: frame_000439.PNG\n","  File: frame_000449.PNG\n","  File: frame_000210.PNG\n","  File: frame_000016.PNG\n","  File: frame_000229.PNG\n","  File: frame_000279.PNG\n","  File: frame_000285.PNG\n","  File: frame_000199.PNG\n","  File: frame_000321.PNG\n","  File: frame_000330.PNG\n","  File: frame_000270.PNG\n","  File: frame_000423.PNG\n","  File: frame_000076.PNG\n","  File: frame_000237.PNG\n","  File: frame_000101.PNG\n","  File: frame_000516.PNG\n","  File: frame_000162.PNG\n","  File: frame_000521.PNG\n","  File: frame_000213.PNG\n","  File: frame_000334.PNG\n","  File: frame_000567.PNG\n","  File: frame_000203.PNG\n","  File: frame_000480.PNG\n","  File: frame_000435.PNG\n","  File: frame_000486.PNG\n","  File: frame_000104.PNG\n","  File: frame_000041.PNG\n","  File: frame_000033.PNG\n","  File: frame_000078.PNG\n","  File: frame_000507.PNG\n","  File: frame_000156.PNG\n","  File: frame_000164.PNG\n","  File: frame_000263.PNG\n","  File: frame_000555.PNG\n","  File: frame_000130.PNG\n","  File: frame_000422.PNG\n","  File: frame_000477.PNG\n","  File: frame_000483.PNG\n","  File: frame_000257.PNG\n","  File: frame_000383.PNG\n","  File: frame_000549.PNG\n","  File: frame_000552.PNG\n","  File: frame_000566.PNG\n","  File: frame_000169.PNG\n","  File: frame_000053.PNG\n","  File: frame_000159.PNG\n","  File: frame_000075.PNG\n","  File: frame_000399.PNG\n","  File: frame_000335.PNG\n","  File: frame_000030.PNG\n","  File: frame_000051.PNG\n","  File: frame_000371.PNG\n","  File: frame_000174.PNG\n","  File: frame_000434.PNG\n","  File: frame_000430.PNG\n","  File: frame_000155.PNG\n","  File: frame_000254.PNG\n","  File: frame_000170.PNG\n","  File: frame_000287.PNG\n","  File: frame_000133.PNG\n","  File: frame_000553.PNG\n","  File: frame_000471.PNG\n","  File: frame_000347.PNG\n","  File: frame_000381.PNG\n","  File: frame_000496.PNG\n","  File: frame_000492.PNG\n","  File: frame_000123.PNG\n","  File: frame_000303.PNG\n","  File: frame_000426.PNG\n","  File: frame_000046.PNG\n","  File: frame_000327.PNG\n","  File: frame_000580.PNG\n","  File: frame_000178.PNG\n","  File: frame_000394.PNG\n","  File: frame_000572.PNG\n","  File: frame_000518.PNG\n","  File: frame_000490.PNG\n","  File: frame_000578.PNG\n","  File: frame_000152.PNG\n","  File: frame_000124.PNG\n","  File: frame_000428.PNG\n","  File: frame_000200.PNG\n","  File: frame_000008.PNG\n","  File: frame_000598.PNG\n","  File: frame_000227.PNG\n","  File: frame_000581.PNG\n","  File: frame_000529.PNG\n","  File: frame_000585.PNG\n","  File: frame_000424.PNG\n","  File: frame_000079.PNG\n","  File: frame_000313.PNG\n","  File: frame_000234.PNG\n","  File: frame_000012.PNG\n","  File: frame_000166.PNG\n","  File: frame_000249.PNG\n","  File: frame_000267.PNG\n","  File: frame_000276.PNG\n","  File: frame_000219.PNG\n","  File: frame_000366.PNG\n","  File: frame_000352.PNG\n","  File: frame_000340.PNG\n","  File: frame_000324.PNG\n","  File: frame_000292.PNG\n","  File: frame_000563.PNG\n","  File: frame_000083.PNG\n","  File: frame_000054.PNG\n","  File: frame_000223.PNG\n","  File: frame_000391.PNG\n","  File: frame_000447.PNG\n","  File: frame_000057.PNG\n","  File: frame_000017.PNG\n","  File: frame_000375.PNG\n","  File: frame_000297.PNG\n","  File: frame_000398.PNG\n","  File: frame_000474.PNG\n","  File: frame_000264.PNG\n","  File: frame_000472.PNG\n","  File: frame_000497.PNG\n","  File: frame_000346.PNG\n","  File: frame_000000.PNG\n","  File: frame_000120.PNG\n","  File: frame_000291.PNG\n","  File: frame_000127.PNG\n","  File: frame_000167.PNG\n","  File: frame_000188.PNG\n","  File: frame_000129.PNG\n","  File: frame_000459.PNG\n","  File: frame_000301.PNG\n","  File: frame_000011.PNG\n","  File: frame_000215.PNG\n","  File: frame_000119.PNG\n","  File: frame_000510.PNG\n","  File: frame_000420.PNG\n","  File: frame_000406.PNG\n","  File: frame_000498.PNG\n","  File: frame_000184.PNG\n","  File: frame_000319.PNG\n","  File: frame_000010.PNG\n","  File: frame_000478.PNG\n","  File: frame_000360.PNG\n","  File: frame_000020.PNG\n","  File: frame_000134.PNG\n","  File: frame_000084.PNG\n","  File: frame_000126.PNG\n","  File: frame_000465.PNG\n","  File: frame_000070.PNG\n","  File: frame_000597.PNG\n","  File: frame_000042.PNG\n","  File: frame_000544.PNG\n","  File: frame_000589.PNG\n","  File: frame_000317.PNG\n","  File: frame_000069.PNG\n","  File: frame_000099.PNG\n","  File: frame_000085.PNG\n","  File: frame_000369.PNG\n","  File: frame_000460.PNG\n","  File: frame_000241.PNG\n","  File: frame_000266.PNG\n","  File: frame_000539.PNG\n","  File: frame_000535.PNG\n","  File: frame_000015.PNG\n","  File: frame_000451.PNG\n","  File: frame_000508.PNG\n","  File: frame_000312.PNG\n","  File: frame_000526.PNG\n","  File: frame_000331.PNG\n","  File: frame_000501.PNG\n","  File: frame_000244.PNG\n","  File: frame_000019.PNG\n","  File: frame_000205.PNG\n","  File: frame_000527.PNG\n","  File: frame_000537.PNG\n","  File: frame_000143.PNG\n","  File: frame_000356.PNG\n","  File: frame_000522.PNG\n","  File: frame_000001.PNG\n","  File: frame_000139.PNG\n","  File: frame_000283.PNG\n","  File: frame_000378.PNG\n","  File: frame_000336.PNG\n","  File: frame_000523.PNG\n","  File: frame_000429.PNG\n","  File: frame_000144.PNG\n","  File: frame_000064.PNG\n","  File: frame_000172.PNG\n","  File: frame_000066.PNG\n","  File: frame_000481.PNG\n","  File: frame_000519.PNG\n","  File: frame_000571.PNG\n","  File: frame_000036.PNG\n","  File: frame_000546.PNG\n","  File: frame_000311.PNG\n","  File: frame_000259.PNG\n","  File: frame_000142.PNG\n","  File: frame_000509.PNG\n","  File: frame_000562.PNG\n","  File: frame_000308.PNG\n","  File: frame_000141.PNG\n","  File: frame_000408.PNG\n","  File: frame_000269.PNG\n","  File: frame_000433.PNG\n","  File: frame_000446.PNG\n","  File: frame_000204.PNG\n","  File: frame_000181.PNG\n","  File: frame_000341.PNG\n","  File: frame_000310.PNG\n","  File: frame_000385.PNG\n","  File: frame_000413.PNG\n","  File: frame_000594.PNG\n","  File: frame_000587.PNG\n","  File: frame_000082.PNG\n","  File: frame_000512.PNG\n","  File: frame_000271.PNG\n","  File: frame_000464.PNG\n","  File: frame_000536.PNG\n","  File: frame_000547.PNG\n","  File: frame_000382.PNG\n","  File: frame_000163.PNG\n","  File: frame_000367.PNG\n","  File: frame_000201.PNG\n","  File: frame_000278.PNG\n","  File: frame_000148.PNG\n","  File: frame_000250.PNG\n","  File: frame_000091.PNG\n","  File: frame_000574.PNG\n","  File: frame_000153.PNG\n","  File: frame_000052.PNG\n","  File: frame_000135.PNG\n","  File: frame_000265.PNG\n","  File: frame_000243.PNG\n","  File: frame_000577.PNG\n","  File: frame_000559.PNG\n","  File: frame_000239.PNG\n","  File: frame_000043.PNG\n","  File: frame_000185.PNG\n","  File: frame_000208.PNG\n","  File: frame_000339.PNG\n","  File: frame_000348.PNG\n","  File: frame_000045.PNG\n","  File: frame_000115.PNG\n","  File: frame_000028.PNG\n","  File: frame_000098.PNG\n","  File: frame_000211.PNG\n","  File: frame_000440.PNG\n","  File: frame_000226.PNG\n","  File: frame_000225.PNG\n","  File: frame_000349.PNG\n","  File: frame_000068.PNG\n","  File: frame_000183.PNG\n","  File: frame_000032.PNG\n","  File: frame_000092.PNG\n","  File: frame_000323.PNG\n","  File: frame_000146.PNG\n","  File: frame_000405.PNG\n","  File: frame_000338.PNG\n","  File: frame_000556.PNG\n","  File: frame_000154.PNG\n","  File: frame_000548.PNG\n","  File: frame_000025.PNG\n","  File: frame_000112.PNG\n","  File: frame_000300.PNG\n","  File: frame_000384.PNG\n","  File: frame_000145.PNG\n","  File: frame_000245.PNG\n","  File: frame_000441.PNG\n","  File: frame_000047.PNG\n","  File: frame_000506.PNG\n","  File: frame_000473.PNG\n","  File: frame_000491.PNG\n","  File: frame_000379.PNG\n","  File: frame_000149.PNG\n","  File: frame_000325.PNG\n","  File: frame_000191.PNG\n","  File: frame_000351.PNG\n","  File: frame_000024.PNG\n","  File: frame_000503.PNG\n","  File: frame_000561.PNG\n","  File: frame_000374.PNG\n","  File: frame_000591.PNG\n","  File: frame_000402.PNG\n","  File: frame_000125.PNG\n","  File: frame_000533.PNG\n","  File: frame_000450.PNG\n","  File: frame_000412.PNG\n","  File: frame_000380.PNG\n","  File: frame_000419.PNG\n","  File: frame_000320.PNG\n","  File: frame_000118.PNG\n","  File: frame_000065.PNG\n","  File: frame_000206.PNG\n","  File: frame_000342.PNG\n","  File: frame_000002.PNG\n","  File: frame_000060.PNG\n","  File: frame_000502.PNG\n","  File: frame_000218.PNG\n","  File: frame_000364.PNG\n","  File: frame_000568.PNG\n","  File: frame_000049.PNG\n","  File: frame_000248.PNG\n","  File: frame_000176.PNG\n","  File: frame_000404.PNG\n","  File: frame_000513.PNG\n","  File: frame_000427.PNG\n","  File: frame_000306.PNG\n","  File: frame_000073.PNG\n","  File: frame_000038.PNG\n","  File: frame_000541.PNG\n","  File: frame_000294.PNG\n","  File: frame_000586.PNG\n","  File: frame_000299.PNG\n","  File: frame_000442.PNG\n","  File: frame_000315.PNG\n","  File: frame_000596.PNG\n","  File: frame_000233.PNG\n","  File: frame_000256.PNG\n","  File: frame_000397.PNG\n","  File: frame_000445.PNG\n","  File: frame_000117.PNG\n","  File: frame_000260.PNG\n","  File: frame_000504.PNG\n","  File: frame_000337.PNG\n","  File: frame_000455.PNG\n","  File: frame_000329.PNG\n","  File: frame_000165.PNG\n","  File: frame_000100.PNG\n","  File: frame_000542.PNG\n","  File: frame_000592.PNG\n","  File: frame_000151.PNG\n","  File: frame_000573.PNG\n","  File: frame_000353.PNG\n","  File: frame_000333.PNG\n","  File: frame_000122.PNG\n","  File: frame_000359.PNG\n","  File: frame_000063.PNG\n","  File: frame_000350.PNG\n","  File: frame_000114.PNG\n","  File: frame_000466.PNG\n","  File: frame_000304.PNG\n","  File: frame_000595.PNG\n","  File: frame_000080.PNG\n","  File: frame_000524.PNG\n","  File: frame_000113.PNG\n","  File: frame_000579.PNG\n","  File: frame_000354.PNG\n","  File: frame_000035.PNG\n","  File: frame_000437.PNG\n","  File: frame_000190.PNG\n","  File: frame_000275.PNG\n","  File: frame_000040.PNG\n","  File: frame_000044.PNG\n","  File: frame_000050.PNG\n","  File: frame_000096.PNG\n","  File: frame_000505.PNG\n","  File: frame_000140.PNG\n","  File: frame_000175.PNG\n","  File: frame_000443.PNG\n","  File: frame_000316.PNG\n","  File: frame_000062.PNG\n","  File: frame_000389.PNG\n","  File: frame_000525.PNG\n","  File: frame_000415.PNG\n","  File: frame_000456.PNG\n","  File: frame_000220.PNG\n","  File: frame_000255.PNG\n","  File: frame_000370.PNG\n","  File: frame_000532.PNG\n","  File: frame_000059.PNG\n","  File: frame_000482.PNG\n","  File: frame_000564.PNG\n","  File: frame_000432.PNG\n","  File: frame_000006.PNG\n","  File: frame_000095.PNG\n","  File: frame_000290.PNG\n","  File: frame_000584.PNG\n","  File: frame_000286.PNG\n","  File: frame_000061.PNG\n","  File: frame_000138.PNG\n","  File: frame_000121.PNG\n","  File: frame_000457.PNG\n","  File: frame_000253.PNG\n","  File: frame_000463.PNG\n","  File: frame_000086.PNG\n","  File: frame_000007.PNG\n","  File: frame_000009.PNG\n","  File: frame_000224.PNG\n","  File: frame_000088.PNG\n","  File: frame_000431.PNG\n","  File: frame_000014.PNG\n","  File: frame_000268.PNG\n","  File: frame_000171.PNG\n","  File: frame_000393.PNG\n","  File: frame_000193.PNG\n","  File: frame_000284.PNG\n","  File: frame_000387.PNG\n","  File: frame_000252.PNG\n","  File: frame_000326.PNG\n","  File: frame_000343.PNG\n","  File: frame_000590.PNG\n","  File: frame_000401.PNG\n","  File: frame_000288.PNG\n","  File: frame_000328.PNG\n","  File: frame_000274.PNG\n","  File: frame_000106.PNG\n","  File: frame_000077.PNG\n","  File: frame_000517.PNG\n","  File: frame_000576.PNG\n","  File: frame_000575.PNG\n","  File: frame_000136.PNG\n","  File: frame_000131.PNG\n","  File: frame_000231.PNG\n","  File: frame_000484.PNG\n","  File: frame_000377.PNG\n","  File: frame_000565.PNG\n","  File: frame_000109.PNG\n","  File: frame_000462.PNG\n","  File: frame_000018.PNG\n","  File: frame_000293.PNG\n","  File: frame_000302.PNG\n","  File: frame_000425.PNG\n","  File: frame_000240.PNG\n","  File: frame_000307.PNG\n","  File: frame_000160.PNG\n","  File: frame_000031.PNG\n","  File: frame_000281.PNG\n","  File: frame_000289.PNG\n","  File: frame_000071.PNG\n","  File: frame_000305.PNG\n","  File: frame_000004.PNG\n","  File: frame_000470.PNG\n","  File: frame_000554.PNG\n","  File: frame_000570.PNG\n","  File: frame_000372.PNG\n","  File: frame_000314.PNG\n","  File: frame_000094.PNG\n","  File: frame_000137.PNG\n","  File: frame_000357.PNG\n","  File: frame_000238.PNG\n","  File: frame_000403.PNG\n","  File: frame_000543.PNG\n","  File: frame_000345.PNG\n","  File: frame_000489.PNG\n","  File: frame_000373.PNG\n","  File: frame_000448.PNG\n","  File: frame_000090.PNG\n","  File: frame_000207.PNG\n","  File: frame_000545.PNG\n","  File: frame_000192.PNG\n","  File: frame_000407.PNG\n","  File: frame_000363.PNG\n","  File: frame_000421.PNG\n","  File: frame_000282.PNG\n","  File: frame_000453.PNG\n","  File: frame_000232.PNG\n","  File: frame_000272.PNG\n","  File: frame_000396.PNG\n","  File: frame_000344.PNG\n","  File: frame_000410.PNG\n","  File: frame_000500.PNG\n","  File: frame_000026.PNG\n","  File: frame_000436.PNG\n","  File: frame_000322.PNG\n","  File: frame_000189.PNG\n","  File: frame_000514.PNG\n","  File: frame_000560.PNG\n","  File: frame_000081.PNG\n","  File: frame_000089.PNG\n","  File: frame_000037.PNG\n","  File: frame_000593.PNG\n","  File: frame_000195.PNG\n","  File: frame_000358.PNG\n","  File: frame_000074.PNG\n","  File: frame_000003.PNG\n","  File: frame_000105.PNG\n","  File: frame_000409.PNG\n","  File: frame_000515.PNG\n","  File: frame_000132.PNG\n","  File: frame_000072.PNG\n","  File: frame_000551.PNG\n","  File: frame_000411.PNG\n","  File: frame_000362.PNG\n","  File: frame_000495.PNG\n","  File: frame_000177.PNG\n","  File: frame_000599.PNG\n","  File: frame_000454.PNG\n","  File: frame_000197.PNG\n","  File: frame_000530.PNG\n","  File: frame_000173.PNG\n","  File: frame_000550.PNG\n","  File: frame_000469.PNG\n","  File: frame_000479.PNG\n","  File: frame_000487.PNG\n","  File: frame_000039.PNG\n","  File: frame_000400.PNG\n","  File: frame_000258.PNG\n","  File: frame_000236.PNG\n","  File: frame_000202.PNG\n","  File: frame_000588.PNG\n","  File: frame_000214.PNG\n","  File: frame_000365.PNG\n","  File: frame_000102.PNG\n","  File: frame_000273.PNG\n","  File: frame_000216.PNG\n","  File: frame_000150.PNG\n","  File: frame_000467.PNG\n","  File: frame_000013.PNG\n","  File: frame_000298.PNG\n","  File: frame_000212.PNG\n","  File: frame_000376.PNG\n","  File: frame_000499.PNG\n","  File: frame_000261.PNG\n","  File: frame_000161.PNG\n","  File: frame_000228.PNG\n","  File: frame_000475.PNG\n","  File: frame_000452.PNG\n","  File: frame_000180.PNG\n","  File: frame_000488.PNG\n","  File: frame_000558.PNG\n","  File: frame_000280.PNG\n","  File: frame_000494.PNG\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(0-300)\n","  File: annotations.xml\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(0-300)/images\n","  File: frame_000110.PNG\n","  File: frame_000116.PNG\n","  File: frame_000158.PNG\n","  File: frame_000168.PNG\n","  File: frame_000034.PNG\n","  File: frame_000023.PNG\n","  File: frame_000027.PNG\n","  File: frame_000277.PNG\n","  File: frame_000103.PNG\n","  File: frame_000186.PNG\n","  File: frame_000262.PNG\n","  File: frame_000067.PNG\n","  File: frame_000005.PNG\n","  File: frame_000209.PNG\n","  File: frame_000242.PNG\n","  File: frame_000247.PNG\n","  File: frame_000217.PNG\n","  File: frame_000021.PNG\n","  File: frame_000128.PNG\n","  File: frame_000058.PNG\n","  File: frame_000198.PNG\n","  File: frame_000093.PNG\n","  File: frame_000295.PNG\n","  File: frame_000235.PNG\n","  File: frame_000230.PNG\n","  File: frame_000157.PNG\n","  File: frame_000187.PNG\n","  File: frame_000221.PNG\n","  File: frame_000055.PNG\n","  File: frame_000087.PNG\n","  File: frame_000194.PNG\n","  File: frame_000107.PNG\n","  File: frame_000097.PNG\n","  File: frame_000108.PNG\n","  File: frame_000029.PNG\n","  File: frame_000182.PNG\n","  File: frame_000111.PNG\n","  File: frame_000147.PNG\n","  File: frame_000196.PNG\n","  File: frame_000179.PNG\n","  File: frame_000296.PNG\n","  File: frame_000048.PNG\n","  File: frame_000222.PNG\n","  File: frame_000251.PNG\n","  File: frame_000022.PNG\n","  File: frame_000246.PNG\n","  File: frame_000056.PNG\n","  File: frame_000210.PNG\n","  File: frame_000016.PNG\n","  File: frame_000229.PNG\n","  File: frame_000279.PNG\n","  File: frame_000285.PNG\n","  File: frame_000199.PNG\n","  File: frame_000270.PNG\n","  File: frame_000076.PNG\n","  File: frame_000237.PNG\n","  File: frame_000101.PNG\n","  File: frame_000162.PNG\n","  File: frame_000213.PNG\n","  File: frame_000203.PNG\n","  File: frame_000104.PNG\n","  File: frame_000041.PNG\n","  File: frame_000033.PNG\n","  File: frame_000078.PNG\n","  File: frame_000156.PNG\n","  File: frame_000164.PNG\n","  File: frame_000263.PNG\n","  File: frame_000130.PNG\n","  File: frame_000257.PNG\n","  File: frame_000169.PNG\n","  File: frame_000053.PNG\n","  File: frame_000159.PNG\n","  File: frame_000075.PNG\n","  File: frame_000030.PNG\n","  File: frame_000051.PNG\n","  File: frame_000174.PNG\n","  File: frame_000155.PNG\n","  File: frame_000254.PNG\n","  File: frame_000170.PNG\n","  File: frame_000287.PNG\n","  File: frame_000133.PNG\n","  File: frame_000123.PNG\n","  File: frame_000303.PNG\n","  File: frame_000046.PNG\n","  File: frame_000178.PNG\n","  File: frame_000152.PNG\n","  File: frame_000124.PNG\n","  File: frame_000200.PNG\n","  File: frame_000008.PNG\n","  File: frame_000227.PNG\n","  File: frame_000079.PNG\n","  File: frame_000234.PNG\n","  File: frame_000012.PNG\n","  File: frame_000166.PNG\n","  File: frame_000249.PNG\n","  File: frame_000267.PNG\n","  File: frame_000276.PNG\n","  File: frame_000219.PNG\n","  File: frame_000292.PNG\n","  File: frame_000083.PNG\n","  File: frame_000054.PNG\n","  File: frame_000223.PNG\n","  File: frame_000057.PNG\n","  File: frame_000017.PNG\n","  File: frame_000297.PNG\n","  File: frame_000264.PNG\n","  File: frame_000000.PNG\n","  File: frame_000120.PNG\n","  File: frame_000291.PNG\n","  File: frame_000127.PNG\n","  File: frame_000167.PNG\n","  File: frame_000188.PNG\n","  File: frame_000129.PNG\n","  File: frame_000301.PNG\n","  File: frame_000011.PNG\n","  File: frame_000215.PNG\n","  File: frame_000119.PNG\n","  File: frame_000184.PNG\n","  File: frame_000010.PNG\n","  File: frame_000020.PNG\n","  File: frame_000134.PNG\n","  File: frame_000084.PNG\n","  File: frame_000126.PNG\n","  File: frame_000070.PNG\n","  File: frame_000042.PNG\n","  File: frame_000069.PNG\n","  File: frame_000099.PNG\n","  File: frame_000085.PNG\n","  File: frame_000241.PNG\n","  File: frame_000266.PNG\n","  File: frame_000015.PNG\n","  File: frame_000244.PNG\n","  File: frame_000019.PNG\n","  File: frame_000205.PNG\n","  File: frame_000143.PNG\n","  File: frame_000001.PNG\n","  File: frame_000139.PNG\n","  File: frame_000283.PNG\n","  File: frame_000144.PNG\n","  File: frame_000064.PNG\n","  File: frame_000172.PNG\n","  File: frame_000066.PNG\n","  File: frame_000036.PNG\n","  File: frame_000259.PNG\n","  File: frame_000142.PNG\n","  File: frame_000141.PNG\n","  File: frame_000269.PNG\n","  File: frame_000204.PNG\n","  File: frame_000181.PNG\n","  File: frame_000082.PNG\n","  File: frame_000271.PNG\n","  File: frame_000163.PNG\n","  File: frame_000201.PNG\n","  File: frame_000278.PNG\n","  File: frame_000148.PNG\n","  File: frame_000250.PNG\n","  File: frame_000091.PNG\n","  File: frame_000153.PNG\n","  File: frame_000052.PNG\n","  File: frame_000135.PNG\n","  File: frame_000265.PNG\n","  File: frame_000243.PNG\n","  File: frame_000239.PNG\n","  File: frame_000043.PNG\n","  File: frame_000185.PNG\n","  File: frame_000208.PNG\n","  File: frame_000045.PNG\n","  File: frame_000115.PNG\n","  File: frame_000028.PNG\n","  File: frame_000098.PNG\n","  File: frame_000211.PNG\n","  File: frame_000226.PNG\n","  File: frame_000225.PNG\n","  File: frame_000068.PNG\n","  File: frame_000183.PNG\n","  File: frame_000032.PNG\n","  File: frame_000092.PNG\n","  File: frame_000146.PNG\n","  File: frame_000154.PNG\n","  File: frame_000025.PNG\n","  File: frame_000112.PNG\n","  File: frame_000300.PNG\n","  File: frame_000145.PNG\n","  File: frame_000245.PNG\n","  File: frame_000047.PNG\n","  File: frame_000149.PNG\n","  File: frame_000191.PNG\n","  File: frame_000024.PNG\n","  File: frame_000125.PNG\n","  File: frame_000118.PNG\n","  File: frame_000065.PNG\n","  File: frame_000206.PNG\n","  File: frame_000002.PNG\n","  File: frame_000060.PNG\n","  File: frame_000218.PNG\n","  File: frame_000049.PNG\n","  File: frame_000248.PNG\n","  File: frame_000176.PNG\n","  File: frame_000073.PNG\n","  File: frame_000038.PNG\n","  File: frame_000294.PNG\n","  File: frame_000299.PNG\n","  File: frame_000233.PNG\n","  File: frame_000256.PNG\n","  File: frame_000117.PNG\n","  File: frame_000260.PNG\n","  File: frame_000165.PNG\n","  File: frame_000100.PNG\n","  File: frame_000151.PNG\n","  File: frame_000122.PNG\n","  File: frame_000063.PNG\n","  File: frame_000114.PNG\n","  File: frame_000304.PNG\n","  File: frame_000080.PNG\n","  File: frame_000113.PNG\n","  File: frame_000035.PNG\n","  File: frame_000190.PNG\n","  File: frame_000275.PNG\n","  File: frame_000040.PNG\n","  File: frame_000044.PNG\n","  File: frame_000050.PNG\n","  File: frame_000096.PNG\n","  File: frame_000140.PNG\n","  File: frame_000175.PNG\n","  File: frame_000062.PNG\n","  File: frame_000220.PNG\n","  File: frame_000255.PNG\n","  File: frame_000059.PNG\n","  File: frame_000006.PNG\n","  File: frame_000095.PNG\n","  File: frame_000290.PNG\n","  File: frame_000286.PNG\n","  File: frame_000061.PNG\n","  File: frame_000138.PNG\n","  File: frame_000121.PNG\n","  File: frame_000253.PNG\n","  File: frame_000086.PNG\n","  File: frame_000007.PNG\n","  File: frame_000009.PNG\n","  File: frame_000224.PNG\n","  File: frame_000088.PNG\n","  File: frame_000014.PNG\n","  File: frame_000268.PNG\n","  File: frame_000171.PNG\n","  File: frame_000193.PNG\n","  File: frame_000284.PNG\n","  File: frame_000252.PNG\n","  File: frame_000288.PNG\n","  File: frame_000274.PNG\n","  File: frame_000106.PNG\n","  File: frame_000077.PNG\n","  File: frame_000136.PNG\n","  File: frame_000131.PNG\n","  File: frame_000231.PNG\n","  File: frame_000109.PNG\n","  File: frame_000018.PNG\n","  File: frame_000293.PNG\n","  File: frame_000302.PNG\n","  File: frame_000240.PNG\n","  File: frame_000160.PNG\n","  File: frame_000031.PNG\n","  File: frame_000281.PNG\n","  File: frame_000289.PNG\n","  File: frame_000071.PNG\n","  File: frame_000305.PNG\n","  File: frame_000004.PNG\n","  File: frame_000094.PNG\n","  File: frame_000137.PNG\n","  File: frame_000238.PNG\n","  File: frame_000090.PNG\n","  File: frame_000207.PNG\n","  File: frame_000192.PNG\n","  File: frame_000282.PNG\n","  File: frame_000232.PNG\n","  File: frame_000272.PNG\n","  File: frame_000026.PNG\n","  File: frame_000189.PNG\n","  File: frame_000081.PNG\n","  File: frame_000089.PNG\n","  File: frame_000037.PNG\n","  File: frame_000195.PNG\n","  File: frame_000074.PNG\n","  File: frame_000003.PNG\n","  File: frame_000105.PNG\n","  File: frame_000132.PNG\n","  File: frame_000072.PNG\n","  File: frame_000177.PNG\n","  File: frame_000197.PNG\n","  File: frame_000173.PNG\n","  File: frame_000039.PNG\n","  File: frame_000258.PNG\n","  File: frame_000236.PNG\n","  File: frame_000202.PNG\n","  File: frame_000214.PNG\n","  File: frame_000102.PNG\n","  File: frame_000273.PNG\n","  File: frame_000216.PNG\n","  File: frame_000150.PNG\n","  File: frame_000013.PNG\n","  File: frame_000298.PNG\n","  File: frame_000212.PNG\n","  File: frame_000261.PNG\n","  File: frame_000161.PNG\n","  File: frame_000228.PNG\n","  File: frame_000180.PNG\n","  File: frame_000280.PNG\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(301-600)\n","  File: annotations.xml\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(301-600)/images\n","  File: frame_000110.PNG\n","  File: frame_000116.PNG\n","  File: frame_000158.PNG\n","  File: frame_000168.PNG\n","  File: frame_000034.PNG\n","  File: frame_000023.PNG\n","  File: frame_000027.PNG\n","  File: frame_000277.PNG\n","  File: frame_000103.PNG\n","  File: frame_000186.PNG\n","  File: frame_000262.PNG\n","  File: frame_000067.PNG\n","  File: frame_000005.PNG\n","  File: frame_000209.PNG\n","  File: frame_000242.PNG\n","  File: frame_000247.PNG\n","  File: frame_000217.PNG\n","  File: frame_000021.PNG\n","  File: frame_000128.PNG\n","  File: frame_000058.PNG\n","  File: frame_000198.PNG\n","  File: frame_000093.PNG\n","  File: frame_000295.PNG\n","  File: frame_000235.PNG\n","  File: frame_000230.PNG\n","  File: frame_000157.PNG\n","  File: frame_000187.PNG\n","  File: frame_000221.PNG\n","  File: frame_000055.PNG\n","  File: frame_000087.PNG\n","  File: frame_000194.PNG\n","  File: frame_000107.PNG\n","  File: frame_000097.PNG\n","  File: frame_000108.PNG\n","  File: frame_000029.PNG\n","  File: frame_000182.PNG\n","  File: frame_000111.PNG\n","  File: frame_000147.PNG\n","  File: frame_000196.PNG\n","  File: frame_000179.PNG\n","  File: frame_000296.PNG\n","  File: frame_000048.PNG\n","  File: frame_000222.PNG\n","  File: frame_000251.PNG\n","  File: frame_000022.PNG\n","  File: frame_000246.PNG\n","  File: frame_000056.PNG\n","  File: frame_000210.PNG\n","  File: frame_000016.PNG\n","  File: frame_000229.PNG\n","  File: frame_000279.PNG\n","  File: frame_000285.PNG\n","  File: frame_000199.PNG\n","  File: frame_000270.PNG\n","  File: frame_000076.PNG\n","  File: frame_000237.PNG\n","  File: frame_000101.PNG\n","  File: frame_000162.PNG\n","  File: frame_000213.PNG\n","  File: frame_000203.PNG\n","  File: frame_000104.PNG\n","  File: frame_000041.PNG\n","  File: frame_000033.PNG\n","  File: frame_000078.PNG\n","  File: frame_000156.PNG\n","  File: frame_000164.PNG\n","  File: frame_000263.PNG\n","  File: frame_000130.PNG\n","  File: frame_000257.PNG\n","  File: frame_000169.PNG\n","  File: frame_000053.PNG\n","  File: frame_000159.PNG\n","  File: frame_000075.PNG\n","  File: frame_000030.PNG\n","  File: frame_000051.PNG\n","  File: frame_000174.PNG\n","  File: frame_000155.PNG\n","  File: frame_000254.PNG\n","  File: frame_000170.PNG\n","  File: frame_000287.PNG\n","  File: frame_000133.PNG\n","  File: frame_000123.PNG\n","  File: frame_000303.PNG\n","  File: frame_000046.PNG\n","  File: frame_000178.PNG\n","  File: frame_000152.PNG\n","  File: frame_000124.PNG\n","  File: frame_000200.PNG\n","  File: frame_000008.PNG\n","  File: frame_000227.PNG\n","  File: frame_000079.PNG\n","  File: frame_000234.PNG\n","  File: frame_000012.PNG\n","  File: frame_000166.PNG\n","  File: frame_000249.PNG\n","  File: frame_000267.PNG\n","  File: frame_000276.PNG\n","  File: frame_000219.PNG\n","  File: frame_000292.PNG\n","  File: frame_000083.PNG\n","  File: frame_000054.PNG\n","  File: frame_000223.PNG\n","  File: frame_000057.PNG\n","  File: frame_000017.PNG\n","  File: frame_000297.PNG\n","  File: frame_000264.PNG\n","  File: frame_000000.PNG\n","  File: frame_000120.PNG\n","  File: frame_000291.PNG\n","  File: frame_000127.PNG\n","  File: frame_000167.PNG\n","  File: frame_000188.PNG\n","  File: frame_000129.PNG\n","  File: frame_000301.PNG\n","  File: frame_000011.PNG\n","  File: frame_000215.PNG\n","  File: frame_000119.PNG\n","  File: frame_000184.PNG\n","  File: frame_000010.PNG\n","  File: frame_000020.PNG\n","  File: frame_000134.PNG\n","  File: frame_000084.PNG\n","  File: frame_000126.PNG\n","  File: frame_000070.PNG\n","  File: frame_000042.PNG\n","  File: frame_000069.PNG\n","  File: frame_000099.PNG\n","  File: frame_000085.PNG\n","  File: frame_000241.PNG\n","  File: frame_000266.PNG\n","  File: frame_000015.PNG\n","  File: frame_000244.PNG\n","  File: frame_000019.PNG\n","  File: frame_000205.PNG\n","  File: frame_000143.PNG\n","  File: frame_000001.PNG\n","  File: frame_000139.PNG\n","  File: frame_000283.PNG\n","  File: frame_000144.PNG\n","  File: frame_000064.PNG\n","  File: frame_000172.PNG\n","  File: frame_000066.PNG\n","  File: frame_000036.PNG\n","  File: frame_000259.PNG\n","  File: frame_000142.PNG\n","  File: frame_000141.PNG\n","  File: frame_000269.PNG\n","  File: frame_000204.PNG\n","  File: frame_000181.PNG\n","  File: frame_000082.PNG\n","  File: frame_000271.PNG\n","  File: frame_000163.PNG\n","  File: frame_000201.PNG\n","  File: frame_000278.PNG\n","  File: frame_000148.PNG\n","  File: frame_000250.PNG\n","  File: frame_000091.PNG\n","  File: frame_000153.PNG\n","  File: frame_000052.PNG\n","  File: frame_000135.PNG\n","  File: frame_000265.PNG\n","  File: frame_000243.PNG\n","  File: frame_000239.PNG\n","  File: frame_000043.PNG\n","  File: frame_000185.PNG\n","  File: frame_000208.PNG\n","  File: frame_000045.PNG\n","  File: frame_000115.PNG\n","  File: frame_000028.PNG\n","  File: frame_000098.PNG\n","  File: frame_000211.PNG\n","  File: frame_000226.PNG\n","  File: frame_000225.PNG\n","  File: frame_000068.PNG\n","  File: frame_000183.PNG\n","  File: frame_000032.PNG\n","  File: frame_000092.PNG\n","  File: frame_000146.PNG\n","  File: frame_000154.PNG\n","  File: frame_000025.PNG\n","  File: frame_000112.PNG\n","  File: frame_000300.PNG\n","  File: frame_000145.PNG\n","  File: frame_000245.PNG\n","  File: frame_000047.PNG\n","  File: frame_000149.PNG\n","  File: frame_000191.PNG\n","  File: frame_000024.PNG\n","  File: frame_000125.PNG\n","  File: frame_000118.PNG\n","  File: frame_000065.PNG\n","  File: frame_000206.PNG\n","  File: frame_000002.PNG\n","  File: frame_000060.PNG\n","  File: frame_000218.PNG\n","  File: frame_000049.PNG\n","  File: frame_000248.PNG\n","  File: frame_000176.PNG\n","  File: frame_000073.PNG\n","  File: frame_000038.PNG\n","  File: frame_000294.PNG\n","  File: frame_000299.PNG\n","  File: frame_000233.PNG\n","  File: frame_000256.PNG\n","  File: frame_000117.PNG\n","  File: frame_000260.PNG\n","  File: frame_000165.PNG\n","  File: frame_000100.PNG\n","  File: frame_000151.PNG\n","  File: frame_000122.PNG\n","  File: frame_000063.PNG\n","  File: frame_000114.PNG\n","  File: frame_000304.PNG\n","  File: frame_000080.PNG\n","  File: frame_000113.PNG\n","  File: frame_000035.PNG\n","  File: frame_000190.PNG\n","  File: frame_000275.PNG\n","  File: frame_000040.PNG\n","  File: frame_000044.PNG\n","  File: frame_000050.PNG\n","  File: frame_000096.PNG\n","  File: frame_000140.PNG\n","  File: frame_000175.PNG\n","  File: frame_000062.PNG\n","  File: frame_000220.PNG\n","  File: frame_000255.PNG\n","  File: frame_000059.PNG\n","  File: frame_000006.PNG\n","  File: frame_000095.PNG\n","  File: frame_000290.PNG\n","  File: frame_000286.PNG\n","  File: frame_000061.PNG\n","  File: frame_000138.PNG\n","  File: frame_000121.PNG\n","  File: frame_000253.PNG\n","  File: frame_000086.PNG\n","  File: frame_000007.PNG\n","  File: frame_000009.PNG\n","  File: frame_000224.PNG\n","  File: frame_000088.PNG\n","  File: frame_000014.PNG\n","  File: frame_000268.PNG\n","  File: frame_000171.PNG\n","  File: frame_000193.PNG\n","  File: frame_000284.PNG\n","  File: frame_000252.PNG\n","  File: frame_000288.PNG\n","  File: frame_000274.PNG\n","  File: frame_000106.PNG\n","  File: frame_000077.PNG\n","  File: frame_000136.PNG\n","  File: frame_000131.PNG\n","  File: frame_000231.PNG\n","  File: frame_000109.PNG\n","  File: frame_000018.PNG\n","  File: frame_000293.PNG\n","  File: frame_000302.PNG\n","  File: frame_000240.PNG\n","  File: frame_000160.PNG\n","  File: frame_000031.PNG\n","  File: frame_000281.PNG\n","  File: frame_000289.PNG\n","  File: frame_000071.PNG\n","  File: frame_000004.PNG\n","  File: frame_000094.PNG\n","  File: frame_000137.PNG\n","  File: frame_000238.PNG\n","  File: frame_000090.PNG\n","  File: frame_000207.PNG\n","  File: frame_000192.PNG\n","  File: frame_000282.PNG\n","  File: frame_000232.PNG\n","  File: frame_000272.PNG\n","  File: frame_000026.PNG\n","  File: frame_000189.PNG\n","  File: frame_000081.PNG\n","  File: frame_000089.PNG\n","  File: frame_000037.PNG\n","  File: frame_000195.PNG\n","  File: frame_000074.PNG\n","  File: frame_000003.PNG\n","  File: frame_000105.PNG\n","  File: frame_000132.PNG\n","  File: frame_000072.PNG\n","  File: frame_000177.PNG\n","  File: frame_000197.PNG\n","  File: frame_000173.PNG\n","  File: frame_000039.PNG\n","  File: frame_000258.PNG\n","  File: frame_000236.PNG\n","  File: frame_000202.PNG\n","  File: frame_000214.PNG\n","  File: frame_000102.PNG\n","  File: frame_000273.PNG\n","  File: frame_000216.PNG\n","  File: frame_000150.PNG\n","  File: frame_000013.PNG\n","  File: frame_000298.PNG\n","  File: frame_000212.PNG\n","  File: frame_000261.PNG\n","  File: frame_000161.PNG\n","  File: frame_000228.PNG\n","  File: frame_000180.PNG\n","  File: frame_000280.PNG\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)\n","  File: annotations.xml\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\n","  File: frame_000110.PNG\n","  File: frame_000116.PNG\n","  File: frame_000158.PNG\n","  File: frame_000168.PNG\n","  File: frame_000034.PNG\n","  File: frame_000023.PNG\n","  File: frame_000027.PNG\n","  File: frame_000277.PNG\n","  File: frame_000103.PNG\n","  File: frame_000186.PNG\n","  File: frame_000262.PNG\n","  File: frame_000067.PNG\n","  File: frame_000005.PNG\n","  File: frame_000209.PNG\n","  File: frame_000242.PNG\n","  File: frame_000247.PNG\n","  File: frame_000217.PNG\n","  File: frame_000021.PNG\n","  File: frame_000128.PNG\n","  File: frame_000058.PNG\n","  File: frame_000198.PNG\n","  File: frame_000093.PNG\n","  File: frame_000295.PNG\n","  File: frame_000235.PNG\n","  File: frame_000230.PNG\n","  File: frame_000157.PNG\n","  File: frame_000187.PNG\n","  File: frame_000221.PNG\n","  File: frame_000055.PNG\n","  File: frame_000087.PNG\n","  File: frame_000194.PNG\n","  File: frame_000107.PNG\n","  File: frame_000097.PNG\n","  File: frame_000108.PNG\n","  File: frame_000029.PNG\n","  File: frame_000182.PNG\n","  File: frame_000111.PNG\n","  File: frame_000147.PNG\n","  File: frame_000196.PNG\n","  File: frame_000179.PNG\n","  File: frame_000296.PNG\n","  File: frame_000048.PNG\n","  File: frame_000222.PNG\n","  File: frame_000251.PNG\n","  File: frame_000022.PNG\n","  File: frame_000246.PNG\n","  File: frame_000056.PNG\n","  File: frame_000210.PNG\n","  File: frame_000016.PNG\n","  File: frame_000229.PNG\n","  File: frame_000279.PNG\n","  File: frame_000285.PNG\n","  File: frame_000199.PNG\n","  File: frame_000270.PNG\n","  File: frame_000076.PNG\n","  File: frame_000237.PNG\n","  File: frame_000101.PNG\n","  File: frame_000162.PNG\n","  File: frame_000213.PNG\n","  File: frame_000203.PNG\n","  File: frame_000104.PNG\n","  File: frame_000041.PNG\n","  File: frame_000033.PNG\n","  File: frame_000078.PNG\n","  File: frame_000156.PNG\n","  File: frame_000164.PNG\n","  File: frame_000263.PNG\n","  File: frame_000130.PNG\n","  File: frame_000257.PNG\n","  File: frame_000169.PNG\n","  File: frame_000053.PNG\n","  File: frame_000159.PNG\n","  File: frame_000075.PNG\n","  File: frame_000030.PNG\n","  File: frame_000051.PNG\n","  File: frame_000174.PNG\n","  File: frame_000155.PNG\n","  File: frame_000254.PNG\n","  File: frame_000170.PNG\n","  File: frame_000287.PNG\n","  File: frame_000133.PNG\n","  File: frame_000123.PNG\n","  File: frame_000303.PNG\n","  File: frame_000046.PNG\n","  File: frame_000178.PNG\n","  File: frame_000152.PNG\n","  File: frame_000124.PNG\n","  File: frame_000200.PNG\n","  File: frame_000008.PNG\n","  File: frame_000227.PNG\n","  File: frame_000079.PNG\n","  File: frame_000234.PNG\n","  File: frame_000012.PNG\n","  File: frame_000166.PNG\n","  File: frame_000249.PNG\n","  File: frame_000267.PNG\n","  File: frame_000276.PNG\n","  File: frame_000219.PNG\n","  File: frame_000292.PNG\n","  File: frame_000083.PNG\n","  File: frame_000054.PNG\n","  File: frame_000223.PNG\n","  File: frame_000057.PNG\n","  File: frame_000017.PNG\n","  File: frame_000297.PNG\n","  File: frame_000264.PNG\n","  File: frame_000000.PNG\n","  File: frame_000120.PNG\n","  File: frame_000291.PNG\n","  File: frame_000127.PNG\n","  File: frame_000167.PNG\n","  File: frame_000188.PNG\n","  File: frame_000129.PNG\n","  File: frame_000301.PNG\n","  File: frame_000011.PNG\n","  File: frame_000215.PNG\n","  File: frame_000119.PNG\n","  File: frame_000184.PNG\n","  File: frame_000010.PNG\n","  File: frame_000020.PNG\n","  File: frame_000134.PNG\n","  File: frame_000084.PNG\n","  File: frame_000126.PNG\n","  File: frame_000070.PNG\n","  File: frame_000042.PNG\n","  File: frame_000069.PNG\n","  File: frame_000099.PNG\n","  File: frame_000085.PNG\n","  File: frame_000241.PNG\n","  File: frame_000266.PNG\n","  File: frame_000015.PNG\n","  File: frame_000244.PNG\n","  File: frame_000019.PNG\n","  File: frame_000205.PNG\n","  File: frame_000143.PNG\n","  File: frame_000001.PNG\n","  File: frame_000139.PNG\n","  File: frame_000283.PNG\n","  File: frame_000144.PNG\n","  File: frame_000064.PNG\n","  File: frame_000172.PNG\n","  File: frame_000066.PNG\n","  File: frame_000036.PNG\n","  File: frame_000259.PNG\n","  File: frame_000142.PNG\n","  File: frame_000141.PNG\n","  File: frame_000269.PNG\n","  File: frame_000204.PNG\n","  File: frame_000181.PNG\n","  File: frame_000082.PNG\n","  File: frame_000271.PNG\n","  File: frame_000163.PNG\n","  File: frame_000201.PNG\n","  File: frame_000278.PNG\n","  File: frame_000148.PNG\n","  File: frame_000250.PNG\n","  File: frame_000091.PNG\n","  File: frame_000153.PNG\n","  File: frame_000052.PNG\n","  File: frame_000135.PNG\n","  File: frame_000265.PNG\n","  File: frame_000243.PNG\n","  File: frame_000239.PNG\n","  File: frame_000043.PNG\n","  File: frame_000185.PNG\n","  File: frame_000208.PNG\n","  File: frame_000045.PNG\n","  File: frame_000115.PNG\n","  File: frame_000028.PNG\n","  File: frame_000098.PNG\n","  File: frame_000211.PNG\n","  File: frame_000226.PNG\n","  File: frame_000225.PNG\n","  File: frame_000068.PNG\n","  File: frame_000183.PNG\n","  File: frame_000032.PNG\n","  File: frame_000092.PNG\n","  File: frame_000146.PNG\n","  File: frame_000154.PNG\n","  File: frame_000025.PNG\n","  File: frame_000112.PNG\n","  File: frame_000300.PNG\n","  File: frame_000145.PNG\n","  File: frame_000245.PNG\n","  File: frame_000047.PNG\n","  File: frame_000149.PNG\n","  File: frame_000191.PNG\n","  File: frame_000024.PNG\n","  File: frame_000125.PNG\n","  File: frame_000118.PNG\n","  File: frame_000065.PNG\n","  File: frame_000206.PNG\n","  File: frame_000002.PNG\n","  File: frame_000060.PNG\n","  File: frame_000218.PNG\n","  File: frame_000049.PNG\n","  File: frame_000248.PNG\n","  File: frame_000176.PNG\n","  File: frame_000073.PNG\n","  File: frame_000038.PNG\n","  File: frame_000294.PNG\n","  File: frame_000299.PNG\n","  File: frame_000233.PNG\n","  File: frame_000256.PNG\n","  File: frame_000117.PNG\n","  File: frame_000260.PNG\n","  File: frame_000165.PNG\n","  File: frame_000100.PNG\n","  File: frame_000151.PNG\n","  File: frame_000122.PNG\n","  File: frame_000063.PNG\n","  File: frame_000114.PNG\n","  File: frame_000304.PNG\n","  File: frame_000080.PNG\n","  File: frame_000113.PNG\n","  File: frame_000035.PNG\n","  File: frame_000190.PNG\n","  File: frame_000275.PNG\n","  File: frame_000040.PNG\n","  File: frame_000044.PNG\n","  File: frame_000050.PNG\n","  File: frame_000096.PNG\n","  File: frame_000140.PNG\n","  File: frame_000175.PNG\n","  File: frame_000062.PNG\n","  File: frame_000220.PNG\n","  File: frame_000255.PNG\n","  File: frame_000059.PNG\n","  File: frame_000006.PNG\n","  File: frame_000095.PNG\n","  File: frame_000290.PNG\n","  File: frame_000286.PNG\n","  File: frame_000061.PNG\n","  File: frame_000138.PNG\n","  File: frame_000121.PNG\n","  File: frame_000253.PNG\n","  File: frame_000086.PNG\n","  File: frame_000007.PNG\n","  File: frame_000009.PNG\n","  File: frame_000224.PNG\n","  File: frame_000088.PNG\n","  File: frame_000014.PNG\n","  File: frame_000268.PNG\n","  File: frame_000171.PNG\n","  File: frame_000193.PNG\n","  File: frame_000284.PNG\n","  File: frame_000252.PNG\n","  File: frame_000288.PNG\n","  File: frame_000274.PNG\n","  File: frame_000106.PNG\n","  File: frame_000077.PNG\n","  File: frame_000136.PNG\n","  File: frame_000131.PNG\n","  File: frame_000231.PNG\n","  File: frame_000109.PNG\n","  File: frame_000018.PNG\n","  File: frame_000293.PNG\n","  File: frame_000302.PNG\n","  File: frame_000240.PNG\n","  File: frame_000160.PNG\n","  File: frame_000031.PNG\n","  File: frame_000281.PNG\n","  File: frame_000289.PNG\n","  File: frame_000071.PNG\n","  File: frame_000004.PNG\n","  File: frame_000094.PNG\n","  File: frame_000137.PNG\n","  File: frame_000238.PNG\n","  File: frame_000090.PNG\n","  File: frame_000207.PNG\n","  File: frame_000192.PNG\n","  File: frame_000282.PNG\n","  File: frame_000232.PNG\n","  File: frame_000272.PNG\n","  File: frame_000026.PNG\n","  File: frame_000189.PNG\n","  File: frame_000081.PNG\n","  File: frame_000089.PNG\n","  File: frame_000037.PNG\n","  File: frame_000195.PNG\n","  File: frame_000074.PNG\n","  File: frame_000003.PNG\n","  File: frame_000105.PNG\n","  File: frame_000132.PNG\n","  File: frame_000072.PNG\n","  File: frame_000177.PNG\n","  File: frame_000197.PNG\n","  File: frame_000173.PNG\n","  File: frame_000039.PNG\n","  File: frame_000258.PNG\n","  File: frame_000236.PNG\n","  File: frame_000202.PNG\n","  File: frame_000214.PNG\n","  File: frame_000102.PNG\n","  File: frame_000273.PNG\n","  File: frame_000216.PNG\n","  File: frame_000150.PNG\n","  File: frame_000013.PNG\n","  File: frame_000298.PNG\n","  File: frame_000212.PNG\n","  File: frame_000261.PNG\n","  File: frame_000161.PNG\n","  File: frame_000228.PNG\n","  File: frame_000180.PNG\n","  File: frame_000280.PNG\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset/task_classroom_11_video-01_final\n","  File: annotations.xml\n","Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset/task_classroom_11_video-01_final/images\n","  File: frame_000110.PNG\n","  File: frame_000116.PNG\n","  File: frame_000395.PNG\n","  File: frame_000531.PNG\n","  File: frame_000158.PNG\n","  File: frame_000444.PNG\n","  File: frame_000168.PNG\n","  File: frame_000034.PNG\n","  File: frame_000361.PNG\n","  File: frame_000438.PNG\n","  File: frame_000023.PNG\n","  File: frame_000027.PNG\n","  File: frame_000277.PNG\n","  File: frame_000461.PNG\n","  File: frame_000388.PNG\n","  File: frame_000103.PNG\n","  File: frame_000186.PNG\n","  File: frame_000262.PNG\n","  File: frame_000468.PNG\n","  File: frame_000067.PNG\n","  File: frame_000005.PNG\n","  File: frame_000209.PNG\n","  File: frame_000242.PNG\n","  File: frame_000247.PNG\n","  File: frame_000493.PNG\n","  File: frame_000582.PNG\n","  File: frame_000569.PNG\n","  File: frame_000217.PNG\n","  File: frame_000021.PNG\n","  File: frame_000511.PNG\n","  File: frame_000414.PNG\n","  File: frame_000128.PNG\n","  File: frame_000058.PNG\n","  File: frame_000528.PNG\n","  File: frame_000534.PNG\n","  File: frame_000332.PNG\n","  File: frame_000198.PNG\n","  File: frame_000417.PNG\n","  File: frame_000093.PNG\n","  File: frame_000295.PNG\n","  File: frame_000235.PNG\n","  File: frame_000230.PNG\n","  File: frame_000157.PNG\n","  File: frame_000309.PNG\n","  File: frame_000540.PNG\n","  File: frame_000187.PNG\n","  File: frame_000221.PNG\n","  File: frame_000368.PNG\n","  File: frame_000055.PNG\n","  File: frame_000583.PNG\n","  File: frame_000087.PNG\n","  File: frame_000194.PNG\n","  File: frame_000107.PNG\n","  File: frame_000097.PNG\n","  File: frame_000318.PNG\n","  File: frame_000108.PNG\n","  File: frame_000485.PNG\n","  File: frame_000386.PNG\n","  File: frame_000416.PNG\n","  File: frame_000029.PNG\n","  File: frame_000182.PNG\n","  File: frame_000392.PNG\n","  File: frame_000476.PNG\n","  File: frame_000111.PNG\n","  File: frame_000147.PNG\n","  File: frame_000355.PNG\n","  File: frame_000196.PNG\n","  File: frame_000179.PNG\n","  File: frame_000296.PNG\n","  File: frame_000048.PNG\n","  File: frame_000222.PNG\n","  File: frame_000520.PNG\n","  File: frame_000251.PNG\n","  File: frame_000022.PNG\n","  File: frame_000246.PNG\n","  File: frame_000557.PNG\n","  File: frame_000418.PNG\n","  File: frame_000056.PNG\n","  File: frame_000458.PNG\n","  File: frame_000538.PNG\n","  File: frame_000390.PNG\n","  File: frame_000439.PNG\n","  File: frame_000449.PNG\n","  File: frame_000210.PNG\n","  File: frame_000016.PNG\n","  File: frame_000229.PNG\n","  File: frame_000279.PNG\n","  File: frame_000285.PNG\n","  File: frame_000199.PNG\n","  File: frame_000321.PNG\n","  File: frame_000330.PNG\n","  File: frame_000270.PNG\n","  File: frame_000423.PNG\n","  File: frame_000076.PNG\n","  File: frame_000237.PNG\n","  File: frame_000101.PNG\n","  File: frame_000516.PNG\n","  File: frame_000162.PNG\n","  File: frame_000521.PNG\n","  File: frame_000213.PNG\n","  File: frame_000334.PNG\n","  File: frame_000567.PNG\n","  File: frame_000203.PNG\n","  File: frame_000480.PNG\n","  File: frame_000435.PNG\n","  File: frame_000486.PNG\n","  File: frame_000104.PNG\n","  File: frame_000041.PNG\n","  File: frame_000033.PNG\n","  File: frame_000078.PNG\n","  File: frame_000507.PNG\n","  File: frame_000156.PNG\n","  File: frame_000164.PNG\n","  File: frame_000263.PNG\n","  File: frame_000555.PNG\n","  File: frame_000130.PNG\n","  File: frame_000422.PNG\n","  File: frame_000477.PNG\n","  File: frame_000483.PNG\n","  File: frame_000257.PNG\n","  File: frame_000383.PNG\n","  File: frame_000549.PNG\n","  File: frame_000552.PNG\n","  File: frame_000566.PNG\n","  File: frame_000169.PNG\n","  File: frame_000053.PNG\n","  File: frame_000159.PNG\n","  File: frame_000075.PNG\n","  File: frame_000399.PNG\n","  File: frame_000335.PNG\n","  File: frame_000030.PNG\n","  File: frame_000051.PNG\n","  File: frame_000371.PNG\n","  File: frame_000174.PNG\n","  File: frame_000434.PNG\n","  File: frame_000430.PNG\n","  File: frame_000155.PNG\n","  File: frame_000254.PNG\n","  File: frame_000170.PNG\n","  File: frame_000287.PNG\n","  File: frame_000133.PNG\n","  File: frame_000553.PNG\n","  File: frame_000471.PNG\n","  File: frame_000347.PNG\n","  File: frame_000381.PNG\n","  File: frame_000496.PNG\n","  File: frame_000492.PNG\n","  File: frame_000123.PNG\n","  File: frame_000303.PNG\n","  File: frame_000426.PNG\n","  File: frame_000046.PNG\n","  File: frame_000327.PNG\n","  File: frame_000580.PNG\n","  File: frame_000178.PNG\n","  File: frame_000394.PNG\n","  File: frame_000572.PNG\n","  File: frame_000518.PNG\n","  File: frame_000490.PNG\n","  File: frame_000578.PNG\n","  File: frame_000152.PNG\n","  File: frame_000124.PNG\n","  File: frame_000428.PNG\n","  File: frame_000200.PNG\n","  File: frame_000008.PNG\n","  File: frame_000598.PNG\n","  File: frame_000227.PNG\n","  File: frame_000581.PNG\n","  File: frame_000529.PNG\n","  File: frame_000585.PNG\n","  File: frame_000424.PNG\n","  File: frame_000079.PNG\n","  File: frame_000313.PNG\n","  File: frame_000234.PNG\n","  File: frame_000012.PNG\n","  File: frame_000166.PNG\n","  File: frame_000249.PNG\n","  File: frame_000267.PNG\n","  File: frame_000276.PNG\n","  File: frame_000219.PNG\n","  File: frame_000366.PNG\n","  File: frame_000352.PNG\n","  File: frame_000340.PNG\n","  File: frame_000324.PNG\n","  File: frame_000292.PNG\n","  File: frame_000563.PNG\n","  File: frame_000083.PNG\n","  File: frame_000054.PNG\n","  File: frame_000223.PNG\n","  File: frame_000391.PNG\n","  File: frame_000447.PNG\n","  File: frame_000057.PNG\n","  File: frame_000017.PNG\n","  File: frame_000375.PNG\n","  File: frame_000297.PNG\n","  File: frame_000398.PNG\n","  File: frame_000474.PNG\n","  File: frame_000264.PNG\n","  File: frame_000472.PNG\n","  File: frame_000497.PNG\n","  File: frame_000346.PNG\n","  File: frame_000000.PNG\n","  File: frame_000120.PNG\n","  File: frame_000291.PNG\n","  File: frame_000127.PNG\n","  File: frame_000167.PNG\n","  File: frame_000188.PNG\n","  File: frame_000129.PNG\n","  File: frame_000459.PNG\n","  File: frame_000301.PNG\n","  File: frame_000011.PNG\n","  File: frame_000215.PNG\n","  File: frame_000119.PNG\n","  File: frame_000510.PNG\n","  File: frame_000420.PNG\n","  File: frame_000406.PNG\n","  File: frame_000498.PNG\n","  File: frame_000184.PNG\n","  File: frame_000319.PNG\n","  File: frame_000010.PNG\n","  File: frame_000478.PNG\n","  File: frame_000360.PNG\n","  File: frame_000020.PNG\n","  File: frame_000134.PNG\n","  File: frame_000084.PNG\n","  File: frame_000126.PNG\n","  File: frame_000465.PNG\n","  File: frame_000070.PNG\n","  File: frame_000597.PNG\n","  File: frame_000042.PNG\n","  File: frame_000544.PNG\n","  File: frame_000589.PNG\n","  File: frame_000317.PNG\n","  File: frame_000069.PNG\n","  File: frame_000099.PNG\n","  File: frame_000085.PNG\n","  File: frame_000369.PNG\n","  File: frame_000460.PNG\n","  File: frame_000241.PNG\n","  File: frame_000266.PNG\n","  File: frame_000539.PNG\n","  File: frame_000535.PNG\n","  File: frame_000015.PNG\n","  File: frame_000451.PNG\n","  File: frame_000508.PNG\n","  File: frame_000312.PNG\n","  File: frame_000526.PNG\n","  File: frame_000331.PNG\n","  File: frame_000501.PNG\n","  File: frame_000244.PNG\n","  File: frame_000019.PNG\n","  File: frame_000205.PNG\n","  File: frame_000527.PNG\n","  File: frame_000537.PNG\n","  File: frame_000143.PNG\n","  File: frame_000356.PNG\n","  File: frame_000522.PNG\n","  File: frame_000001.PNG\n","  File: frame_000139.PNG\n","  File: frame_000283.PNG\n","  File: frame_000378.PNG\n","  File: frame_000336.PNG\n","  File: frame_000523.PNG\n","  File: frame_000429.PNG\n","  File: frame_000144.PNG\n","  File: frame_000064.PNG\n","  File: frame_000172.PNG\n","  File: frame_000066.PNG\n","  File: frame_000481.PNG\n","  File: frame_000519.PNG\n","  File: frame_000571.PNG\n","  File: frame_000036.PNG\n","  File: frame_000546.PNG\n","  File: frame_000311.PNG\n","  File: frame_000259.PNG\n","  File: frame_000142.PNG\n","  File: frame_000509.PNG\n","  File: frame_000562.PNG\n","  File: frame_000308.PNG\n","  File: frame_000141.PNG\n","  File: frame_000408.PNG\n","  File: frame_000269.PNG\n","  File: frame_000433.PNG\n","  File: frame_000446.PNG\n","  File: frame_000204.PNG\n","  File: frame_000181.PNG\n","  File: frame_000341.PNG\n","  File: frame_000310.PNG\n","  File: frame_000385.PNG\n","  File: frame_000413.PNG\n","  File: frame_000594.PNG\n","  File: frame_000587.PNG\n","  File: frame_000082.PNG\n","  File: frame_000512.PNG\n","  File: frame_000271.PNG\n","  File: frame_000464.PNG\n","  File: frame_000536.PNG\n","  File: frame_000547.PNG\n","  File: frame_000382.PNG\n","  File: frame_000163.PNG\n","  File: frame_000367.PNG\n","  File: frame_000201.PNG\n","  File: frame_000278.PNG\n","  File: frame_000148.PNG\n","  File: frame_000250.PNG\n","  File: frame_000091.PNG\n","  File: frame_000574.PNG\n","  File: frame_000153.PNG\n","  File: frame_000052.PNG\n","  File: frame_000135.PNG\n","  File: frame_000265.PNG\n","  File: frame_000243.PNG\n","  File: frame_000577.PNG\n","  File: frame_000559.PNG\n","  File: frame_000239.PNG\n","  File: frame_000043.PNG\n","  File: frame_000185.PNG\n","  File: frame_000208.PNG\n","  File: frame_000339.PNG\n","  File: frame_000348.PNG\n","  File: frame_000045.PNG\n","  File: frame_000115.PNG\n","  File: frame_000028.PNG\n","  File: frame_000098.PNG\n","  File: frame_000211.PNG\n","  File: frame_000440.PNG\n","  File: frame_000226.PNG\n","  File: frame_000225.PNG\n","  File: frame_000349.PNG\n","  File: frame_000068.PNG\n","  File: frame_000183.PNG\n","  File: frame_000032.PNG\n","  File: frame_000092.PNG\n","  File: frame_000323.PNG\n","  File: frame_000146.PNG\n","  File: frame_000405.PNG\n","  File: frame_000338.PNG\n","  File: frame_000556.PNG\n","  File: frame_000154.PNG\n","  File: frame_000548.PNG\n","  File: frame_000025.PNG\n","  File: frame_000112.PNG\n","  File: frame_000300.PNG\n","  File: frame_000384.PNG\n","  File: frame_000145.PNG\n","  File: frame_000245.PNG\n","  File: frame_000441.PNG\n","  File: frame_000047.PNG\n","  File: frame_000506.PNG\n","  File: frame_000473.PNG\n","  File: frame_000491.PNG\n","  File: frame_000379.PNG\n","  File: frame_000149.PNG\n","  File: frame_000325.PNG\n","  File: frame_000191.PNG\n","  File: frame_000351.PNG\n","  File: frame_000024.PNG\n","  File: frame_000503.PNG\n","  File: frame_000561.PNG\n","  File: frame_000374.PNG\n","  File: frame_000591.PNG\n","  File: frame_000402.PNG\n","  File: frame_000125.PNG\n","  File: frame_000533.PNG\n","  File: frame_000450.PNG\n","  File: frame_000412.PNG\n","  File: frame_000380.PNG\n","  File: frame_000419.PNG\n","  File: frame_000320.PNG\n","  File: frame_000118.PNG\n","  File: frame_000065.PNG\n","  File: frame_000206.PNG\n","  File: frame_000342.PNG\n","  File: frame_000002.PNG\n","  File: frame_000060.PNG\n","  File: frame_000502.PNG\n","  File: frame_000218.PNG\n","  File: frame_000364.PNG\n","  File: frame_000568.PNG\n","  File: frame_000049.PNG\n","  File: frame_000248.PNG\n","  File: frame_000176.PNG\n","  File: frame_000404.PNG\n","  File: frame_000513.PNG\n","  File: frame_000427.PNG\n","  File: frame_000306.PNG\n","  File: frame_000073.PNG\n","  File: frame_000038.PNG\n","  File: frame_000541.PNG\n","  File: frame_000294.PNG\n","  File: frame_000586.PNG\n","  File: frame_000299.PNG\n","  File: frame_000442.PNG\n","  File: frame_000315.PNG\n","  File: frame_000596.PNG\n","  File: frame_000233.PNG\n","  File: frame_000256.PNG\n","  File: frame_000397.PNG\n","  File: frame_000445.PNG\n","  File: frame_000117.PNG\n","  File: frame_000260.PNG\n","  File: frame_000504.PNG\n","  File: frame_000337.PNG\n","  File: frame_000455.PNG\n","  File: frame_000329.PNG\n","  File: frame_000165.PNG\n","  File: frame_000100.PNG\n","  File: frame_000542.PNG\n","  File: frame_000592.PNG\n","  File: frame_000151.PNG\n","  File: frame_000573.PNG\n","  File: frame_000353.PNG\n","  File: frame_000333.PNG\n","  File: frame_000122.PNG\n","  File: frame_000359.PNG\n","  File: frame_000063.PNG\n","  File: frame_000350.PNG\n","  File: frame_000114.PNG\n","  File: frame_000466.PNG\n","  File: frame_000304.PNG\n","  File: frame_000595.PNG\n","  File: frame_000080.PNG\n","  File: frame_000524.PNG\n","  File: frame_000113.PNG\n","  File: frame_000579.PNG\n","  File: frame_000354.PNG\n","  File: frame_000035.PNG\n","  File: frame_000437.PNG\n","  File: frame_000190.PNG\n","  File: frame_000275.PNG\n","  File: frame_000040.PNG\n","  File: frame_000044.PNG\n","  File: frame_000050.PNG\n","  File: frame_000096.PNG\n","  File: frame_000505.PNG\n","  File: frame_000140.PNG\n","  File: frame_000175.PNG\n","  File: frame_000443.PNG\n","  File: frame_000316.PNG\n","  File: frame_000062.PNG\n","  File: frame_000389.PNG\n","  File: frame_000525.PNG\n","  File: frame_000415.PNG\n","  File: frame_000456.PNG\n","  File: frame_000220.PNG\n","  File: frame_000255.PNG\n","  File: frame_000370.PNG\n","  File: frame_000532.PNG\n","  File: frame_000059.PNG\n","  File: frame_000482.PNG\n","  File: frame_000564.PNG\n","  File: frame_000432.PNG\n","  File: frame_000006.PNG\n","  File: frame_000095.PNG\n","  File: frame_000290.PNG\n","  File: frame_000584.PNG\n","  File: frame_000286.PNG\n","  File: frame_000061.PNG\n","  File: frame_000138.PNG\n","  File: frame_000121.PNG\n","  File: frame_000457.PNG\n","  File: frame_000253.PNG\n","  File: frame_000463.PNG\n","  File: frame_000086.PNG\n","  File: frame_000007.PNG\n","  File: frame_000009.PNG\n","  File: frame_000224.PNG\n","  File: frame_000088.PNG\n","  File: frame_000431.PNG\n","  File: frame_000014.PNG\n","  File: frame_000268.PNG\n","  File: frame_000171.PNG\n","  File: frame_000393.PNG\n","  File: frame_000193.PNG\n","  File: frame_000284.PNG\n","  File: frame_000387.PNG\n","  File: frame_000252.PNG\n","  File: frame_000326.PNG\n","  File: frame_000343.PNG\n","  File: frame_000590.PNG\n","  File: frame_000401.PNG\n","  File: frame_000288.PNG\n","  File: frame_000328.PNG\n","  File: frame_000274.PNG\n","  File: frame_000106.PNG\n","  File: frame_000077.PNG\n","  File: frame_000517.PNG\n","  File: frame_000576.PNG\n","  File: frame_000575.PNG\n","  File: frame_000136.PNG\n","  File: frame_000131.PNG\n","  File: frame_000231.PNG\n","  File: frame_000484.PNG\n","  File: frame_000377.PNG\n","  File: frame_000565.PNG\n","  File: frame_000109.PNG\n","  File: frame_000462.PNG\n","  File: frame_000018.PNG\n","  File: frame_000293.PNG\n","  File: frame_000302.PNG\n","  File: frame_000425.PNG\n","  File: frame_000240.PNG\n","  File: frame_000307.PNG\n","  File: frame_000160.PNG\n","  File: frame_000031.PNG\n","  File: frame_000281.PNG\n","  File: frame_000289.PNG\n","  File: frame_000071.PNG\n","  File: frame_000305.PNG\n","  File: frame_000004.PNG\n","  File: frame_000470.PNG\n","  File: frame_000554.PNG\n","  File: frame_000570.PNG\n","  File: frame_000372.PNG\n","  File: frame_000314.PNG\n","  File: frame_000094.PNG\n","  File: frame_000137.PNG\n","  File: frame_000357.PNG\n","  File: frame_000238.PNG\n","  File: frame_000403.PNG\n","  File: frame_000543.PNG\n","  File: frame_000345.PNG\n","  File: frame_000489.PNG\n","  File: frame_000373.PNG\n","  File: frame_000448.PNG\n","  File: frame_000090.PNG\n","  File: frame_000207.PNG\n","  File: frame_000545.PNG\n","  File: frame_000192.PNG\n","  File: frame_000407.PNG\n","  File: frame_000363.PNG\n","  File: frame_000421.PNG\n","  File: frame_000282.PNG\n","  File: frame_000453.PNG\n","  File: frame_000232.PNG\n","  File: frame_000272.PNG\n","  File: frame_000396.PNG\n","  File: frame_000344.PNG\n","  File: frame_000410.PNG\n","  File: frame_000500.PNG\n","  File: frame_000026.PNG\n","  File: frame_000436.PNG\n","  File: frame_000322.PNG\n","  File: frame_000189.PNG\n","  File: frame_000514.PNG\n","  File: frame_000560.PNG\n","  File: frame_000081.PNG\n","  File: frame_000089.PNG\n","  File: frame_000037.PNG\n","  File: frame_000593.PNG\n","  File: frame_000195.PNG\n","  File: frame_000358.PNG\n","  File: frame_000074.PNG\n","  File: frame_000003.PNG\n","  File: frame_000105.PNG\n","  File: frame_000409.PNG\n","  File: frame_000515.PNG\n","  File: frame_000132.PNG\n","  File: frame_000072.PNG\n","  File: frame_000551.PNG\n","  File: frame_000411.PNG\n","  File: frame_000362.PNG\n","  File: frame_000495.PNG\n","  File: frame_000177.PNG\n","  File: frame_000454.PNG\n","  File: frame_000197.PNG\n","  File: frame_000530.PNG\n","  File: frame_000173.PNG\n","  File: frame_000550.PNG\n","  File: frame_000469.PNG\n","  File: frame_000479.PNG\n","  File: frame_000487.PNG\n","  File: frame_000039.PNG\n","  File: frame_000400.PNG\n","  File: frame_000258.PNG\n","  File: frame_000236.PNG\n","  File: frame_000202.PNG\n","  File: frame_000588.PNG\n","  File: frame_000214.PNG\n","  File: frame_000365.PNG\n","  File: frame_000102.PNG\n","  File: frame_000273.PNG\n","  File: frame_000216.PNG\n","  File: frame_000150.PNG\n","  File: frame_000467.PNG\n","  File: frame_000013.PNG\n","  File: frame_000298.PNG\n","  File: frame_000212.PNG\n","  File: frame_000376.PNG\n","  File: frame_000499.PNG\n","  File: frame_000261.PNG\n","  File: frame_000161.PNG\n","  File: frame_000228.PNG\n","  File: frame_000475.PNG\n","  File: frame_000452.PNG\n","  File: frame_000180.PNG\n","  File: frame_000488.PNG\n","  File: frame_000558.PNG\n","  File: frame_000280.PNG\n","  File: frame_000494.PNG\n"]}],"execution_count":3},{"cell_type":"markdown","source":"## DATASET","metadata":{"id":"pV4zpDHKeLK9"}},{"cell_type":"code","source":"kaggle = False\nbase_dir = ''\noutput_dir = ''\nif kaggle:\n  base_dir = \"/kaggle/input/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement\"\n  output_dir = \"/kaggle/working\"\nelse:\n  base_dir = f'{thebrokenvessel_gescam_partial_path}/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement'\n  output_dir = '.'\nxml_path = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\"\nimage_folder = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\"","metadata":{"id":"CUGmKvsOeLK9"},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### OBJECT MASK PIPELINE\n\nWe are introducing an object mask detection pipeline to detect objects in a classroom scene, this will help us map the gaze heatmaps to detected objects so that we can tell where the students are looking, it could be:\n\n1. Books/notebooks\n2. screens/monitors\n3. whiteboards/blackboards\n4. tables/desks\n5. people (teachers/students","metadata":{"id":"yGx2HA9geLK-"}},{"cell_type":"code","source":"def extract_object_masks_from_annotations(frame_data, width, height, num_categories=11):\n    \"\"\"\n    Extract object masks from frame annotations\n\n    Args:\n        frame_data: Dictionary containing frame annotations\n        width: Image width\n        height: Image height\n        num_categories: Number of object categories\n\n    Returns:\n        object_masks: Array of shape [num_categories, height, width]\n    \"\"\"\n    # Initialize masks\n    object_masks = np.zeros((num_categories, height, width), dtype=np.float32)\n\n    # Map of object labels to category indices\n    category_map = {\n        'person': 0,\n        'teacher': 0,  # Group all people in category 0\n        'blackboard': 1,\n        'whiteboard': 1,  # Group all boards in category 1\n        'notebook': 2,\n        'book': 2,  # Group all reading materials in category 2\n        'monitor': 3,\n        'screen': 3,  # Group all displays in category 3\n        'mobile': 4,\n        'phone': 4,  # Group all phones in category 4\n        'table': 5,\n        'desk': 5,  # Group all tables in category 5\n        'water dispenser': 6,\n        'mug': 7,\n        'table lamp': 8,\n        # Add more mappings as needed\n    }\n\n    # Process each box\n    for box in frame_data[\"boxes\"]:\n        label = box[\"label\"].lower()\n\n        # Extract category\n        category = -1\n        for key, idx in category_map.items():\n            if key in label:\n                category = idx\n                break\n\n        if category >= 0 and category < num_categories:\n            # Extract coordinates\n            x1, y1 = int(box[\"xtl\"]), int(box[\"ytl\"])\n            x2, y2 = int(box[\"xbr\"]), int(box[\"ybr\"])\n\n            # Ensure within bounds\n            x1 = max(0, min(width-1, x1))\n            y1 = max(0, min(height-1, y1))\n            x2 = max(x1+1, min(width, x2))\n            y2 = max(y1+1, min(height, y2))\n\n            # Set mask\n            object_masks[category, y1:y2, x1:x2] = 1.0\n\n    return object_masks","metadata":{"id":"QjOrQhIEeLK-"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def parse_xml(xml_path):\n    \"\"\"\n    Parse the XML file and extract bounding boxes and polylines for each frame.\n\n    Args:\n        xml_path (str): Path to the XML file.\n\n    Returns:\n        frames (dict): Dictionary containing frame data (bounding boxes and polylines).\n    \"\"\"\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        frames = {}\n\n        print(f\"Parsing XML annotations from {xml_path}\")\n        print(f\"Root tag: {root.tag}, with {len(root)} child elements\")\n\n        frame_count = 0\n        for image in tqdm(root.findall(\"image\"), desc=\"Parsing frames\"):\n            try:\n                frame_id = int(image.attrib[\"id\"])  # Extract frame ID\n                frame_name = image.attrib[\"name\"]  # Extract frame name\n                width = int(image.attrib[\"width\"])  # Image width\n                height = int(image.attrib[\"height\"])  # Image height\n\n                # Initialize lists for bounding boxes and polylines\n                frame_boxes = []\n                frame_polylines = []\n\n                # Extract bounding boxes\n                for box in image.findall(\"box\"):\n                    try:\n                        # Extract all box attributes\n                        box_info = {\n                            \"label\": box.attrib.get(\"label\", \"unknown\"),\n                            \"xtl\": float(box.attrib.get(\"xtl\", 0)),\n                            \"ytl\": float(box.attrib.get(\"ytl\", 0)),\n                            \"xbr\": float(box.attrib.get(\"xbr\", 0)),\n                            \"ybr\": float(box.attrib.get(\"ybr\", 0)),\n                        }\n                        frame_boxes.append(box_info)\n                    except Exception as box_err:\n                        print(f\"Error parsing box in frame {frame_id}: {box_err}\")\n\n                # Extract polylines\n                for polyline in image.findall(\"polyline\"):\n                    try:\n                        polyline_info = {\n                            \"label\": polyline.attrib.get(\"label\", \"unknown\"),\n                            \"points\": polyline.attrib.get(\"points\", \"\")\n                        }\n                        frame_polylines.append(polyline_info)\n                    except Exception as polyline_err:\n                        print(f\"Error parsing polyline in frame {frame_id}: {polyline_err}\")\n\n                # Store frame information\n                frames[frame_id] = {\n                    \"name\": frame_name,\n                    \"width\": width,\n                    \"height\": height,\n                    \"boxes\": frame_boxes,\n                    \"polylines\": frame_polylines\n                }\n                frame_count += 1\n\n                # Debug first frame\n                if frame_count == 1:\n                    print(f\"Sample frame: ID={frame_id}, Name={frame_name}, Size={width}x{height}\")\n                    print(f\"Found {len(frame_boxes)} boxes and {len(frame_polylines)} polylines in first frame\")\n                    if frame_boxes:\n                        print(f\"Sample box labels: {[box['label'] for box in frame_boxes[:5]]}\")\n                    if frame_polylines:\n                        print(f\"Sample polyline labels: {[p['label'] for p in frame_polylines[:5]]}\")\n            except Exception as frame_err:\n                print(f\"Error parsing frame: {frame_err}\")\n                continue\n\n        print(f\"Successfully parsed {len(frames)} frames\")\n        return frames\n\n    except Exception as e:\n        print(f\"Error parsing XML file: {e}\")\n        import traceback\n        traceback.print_exc()\n        return {}\n\ndef create_frame_to_image_mapping(image_folder):\n    \"\"\"\n    Create a mapping from frame IDs to image paths.\n\n    Args:\n        image_folder (str): Path to the folder containing images.\n\n    Returns:\n        frame_to_image (dict): Mapping from frame IDs to image paths.\n    \"\"\"\n    frame_to_image = {}\n\n    if not os.path.exists(image_folder):\n        print(f\"Warning: Image folder {image_folder} does not exist\")\n        return frame_to_image\n\n    for image_name in os.listdir(image_folder):\n        if not any(image_name.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.bmp']):\n            continue\n\n        # Try various patterns to extract frame ID\n        # Pattern 1: frame_000123.jpg/png\n        match = re.search(r'frame_0*(\\d+)', image_name.lower())\n        if match:\n            frame_id = int(match.group(1))\n            frame_to_image[frame_id] = os.path.join(image_folder, image_name)\n            continue\n\n        # Pattern 2: 000123.jpg\n        match = re.search(r'^0*(\\d+)', image_name)\n        if match:\n            frame_id = int(match.group(1))\n            frame_to_image[frame_id] = os.path.join(image_folder, image_name)\n            continue\n\n        # Pattern 3: Any number in the filename\n        match = re.search(r'(\\d+)', image_name)\n        if match:\n            frame_id = int(match.group(1))\n            frame_to_image[frame_id] = os.path.join(image_folder, image_name)\n\n    print(f\"Found {len(frame_to_image)} images with extractable frame IDs\")\n    return frame_to_image\n\nclass GESCAMCustomDataset(Dataset):\n    \"\"\"\n    Dataset class for GESCAM (Gaze Estimation based Synthetic Classroom Attention Measurement)\n    Customized for the specific annotation format\n    \"\"\"\n    def __init__(self, xml_path, image_folder, transform=None, head_transform=None,\n                 input_size=224, output_size=64, test=False):\n        \"\"\"\n        Args:\n            xml_path (str): Path to the XML annotation file\n            image_folder (str): Path to the folder containing images\n            transform: Transformations to apply to the scene image\n            head_transform: Transformations to apply to the head crop\n            input_size: Input image size for the model\n            output_size: Output heatmap size\n            test: Whether this is a test dataset\n        \"\"\"\n        super(GESCAMCustomDataset, self).__init__()\n\n        self.xml_path = xml_path\n        self.image_folder = image_folder\n        self.transform = transform\n        self.head_transform = head_transform if head_transform else transform\n        self.input_size = input_size\n        self.output_size = output_size\n        self.test = test\n\n        # Parse annotations and create image mapping\n        self.frames = parse_xml(xml_path)\n        self.frame_to_image = create_frame_to_image_mapping(image_folder)\n\n        # Create samples\n        self.samples = self._create_samples()\n        print(f\"Created dataset with {len(self.samples)} samples\")\n\n    def _match_person_to_sight_line(self, person_box, polylines):\n        \"\"\"\n        Match a person bounding box to the corresponding line of sight polyline\n\n        Args:\n            person_box: Dictionary containing person bounding box\n            polylines: List of polyline dictionaries for the frame\n\n        Returns:\n            target_point: (x,y) tuple of gaze target or None if no match\n            has_target: Boolean indicating if a match was found\n        \"\"\"\n        # Find polylines labeled as \"line of sight\"\n        sight_lines = [p for p in polylines if p[\"label\"].lower() == \"line of sight\"]\n\n        if not sight_lines:\n            return None, False\n\n        # Calculate person box center\n        person_center_x = (person_box[\"xtl\"] + person_box[\"xbr\"]) / 2\n        person_center_y = (person_box[\"ytl\"] + person_box[\"ybr\"]) / 2\n        person_width = person_box[\"xbr\"] - person_box[\"xtl\"]\n\n        # Find closest matching sight line\n        best_match = None\n        best_distance = float('inf')\n\n        for polyline in sight_lines:\n            points_str = polyline[\"points\"]\n            try:\n                # Parse points from string format \"x1,y1;x2,y2;...\"\n                points = [tuple(map(float, point.split(\",\"))) for point in points_str.split(\";\")]\n\n                if len(points) >= 2:  # Need at least start and end point\n                    start_x, start_y = points[0]\n                    end_x, end_y = points[-1]\n\n                    # Calculate distance from polyline start to person center\n                    distance = np.sqrt((start_x - person_center_x)**2 + (start_y - person_center_y)**2)\n\n                    # Check if this is a good match (close to person center)\n                    if distance < best_distance and distance < person_width * 1.5:\n                        best_distance = distance\n                        best_match = (end_x, end_y)  # Use end point as gaze target\n            except Exception as e:\n                # Print details for debugging\n                print(f\"Error parsing polyline points: {e}, points_str: {points_str}\")\n                continue\n\n        return best_match, best_match is not None\n\n    def _create_samples(self):\n        \"\"\"\n        Create dataset samples from parsed frames\n\n        Returns:\n            samples: List of sample dictionaries\n        \"\"\"\n        samples = []\n        frames_with_persons = 0\n        frames_with_sight_lines = 0\n\n        for frame_id, frame_data in self.frames.items():\n            # Skip frames without matching images\n            if frame_id not in self.frame_to_image:\n                continue\n\n            image_path = self.frame_to_image[frame_id]\n            width, height = frame_data[\"width\"], frame_data[\"height\"]\n\n            # Extract object masks for this frame\n            object_masks = extract_object_masks_from_annotations(frame_data, width, height)\n\n            # Check if there are person boxes in this frame\n            person_boxes = [box for box in frame_data[\"boxes\"] if \"person\" in box[\"label\"].lower()]\n            if person_boxes:\n                frames_with_persons += 1\n\n            # Check if there are line of sight polylines\n            sight_lines = [p for p in frame_data[\"polylines\"] if p[\"label\"].lower() == \"line of sight\"]\n            if sight_lines:\n                frames_with_sight_lines += 1\n\n            # Process each person box\n            for person_box in person_boxes:\n                # Find matching sight line\n                gaze_target, has_target = self._match_person_to_sight_line(person_box, frame_data[\"polylines\"])\n\n                # Create sample\n                sample = {\n                    \"frame_id\": frame_id,\n                    \"image_path\": image_path,\n                    \"width\": width,\n                    \"height\": height,\n                    \"head_bbox\": [person_box[\"xtl\"], person_box[\"ytl\"], person_box[\"xbr\"], person_box[\"ybr\"]],\n                    \"gaze_target\": gaze_target,\n                    \"in_frame\": has_target,\n                    \"object_masks\": object_masks  # Add object masks\n                }\n\n                samples.append(sample)\n\n        print(f\"Statistics: {frames_with_persons} frames with person boxes, {frames_with_sight_lines} frames with sight lines\")\n        return samples\n\n    def _create_head_position_channel(self, head_bbox, width, height):\n        \"\"\"\n        Create a binary mask for head position\n        \"\"\"\n        x1, y1, x2, y2 = head_bbox\n        head_mask = torch.zeros(height, width)\n        x1, y1, x2, y2 = int(max(0, x1)), int(max(0, y1)), int(min(width, x2)), int(min(height, y2))\n        head_mask[y1:y2, x1:x2] = 1.0\n        return head_mask\n\n    def _create_gaze_heatmap(self, gaze_target, width, height):\n        \"\"\"\n        Create a Gaussian heatmap at the gaze point\n        \"\"\"\n        if not gaze_target:\n            return torch.zeros(self.output_size, self.output_size)\n\n        x, y = gaze_target\n\n        # Scale coordinates to output size\n        x = x * self.output_size / width\n        y = y * self.output_size / height\n\n        # Create meshgrid\n        Y, X = torch.meshgrid(torch.arange(self.output_size), torch.arange(self.output_size), indexing='ij')\n\n        # Create Gaussian heatmap\n        sigma = 3.0\n        heatmap = torch.exp(-((X - x) ** 2 + (Y - y) ** 2) / (2 * sigma ** 2))\n\n        return heatmap\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n\n        # Load image\n        try:\n            img = Image.open(sample[\"image_path\"]).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {sample['image_path']}: {e}\")\n            # Return a placeholder if image can't be loaded\n            img = Image.new('RGB', (self.input_size, self.input_size), color='gray')\n\n        width, height = sample[\"width\"], sample[\"height\"]\n\n        # Extract head crop\n        head_bbox = sample[\"head_bbox\"]\n        x1, y1, x2, y2 = head_bbox\n\n        # Ensure bbox is within image bounds\n        x1 = max(0, min(width-1, x1))\n        y1 = max(0, min(height-1, y1))\n        x2 = max(x1+1, min(width, x2))\n        y2 = max(y1+1, min(height, y2))\n\n        try:\n            head_img = img.crop((int(x1), int(y1), int(x2), int(y2)))\n        except Exception as e:\n            print(f\"Error cropping head: {e}, bbox: {head_bbox}, image size: {img.size}\")\n            head_img = Image.new('RGB', (100, 100), color='gray')\n\n        # Create head position channel\n        head_pos = self._create_head_position_channel(head_bbox, width, height)\n\n        # Get object masks\n        object_masks = torch.from_numpy(sample[\"object_masks\"])\n\n        # Create gaze heatmap\n        if sample[\"in_frame\"] and sample[\"gaze_target\"]:\n            gaze_target = sample[\"gaze_target\"]\n            gaze_heatmap = self._create_gaze_heatmap(gaze_target, width, height)\n\n            # Calculate gaze vector (from head center to gaze point)\n            head_center_x = (x1 + x2) / 2 / width\n            head_center_y = (y1 + y2) / 2 / height\n            gaze_x = gaze_target[0] / width\n            gaze_y = gaze_target[1] / height\n            gaze_vector = torch.tensor([gaze_x - head_center_x, gaze_y - head_center_y])\n        else:\n            gaze_heatmap = torch.zeros(self.output_size, self.output_size)\n            gaze_vector = torch.tensor([0.0, 0.0])  # Default for out-of-frame\n\n        # Apply transformations\n        if self.transform:\n            img = self.transform(img)\n\n        if self.head_transform:\n            head_img = self.head_transform(head_img)\n\n        # Resize head position to match input size\n        head_pos = head_pos.unsqueeze(0)\n        head_pos = F.interpolate(head_pos.unsqueeze(0), size=(self.input_size, self.input_size),\n                                 mode='nearest').squeeze(0)\n\n        # Resize object masks to match input size\n        object_masks = F.interpolate(object_masks.unsqueeze(0),\n                                     size=(self.input_size, self.input_size),\n                                     mode='nearest').squeeze(0)\n\n        in_frame = torch.tensor([float(sample[\"in_frame\"])])\n\n        # For compatibility with existing code\n        object_label = torch.tensor([0])  # placeholder\n\n        # Instead of returning frame_id as last element (which might cause issues with batching),\n        # return a metadata dictionary alongside the tensors\n        metadata = {\n            \"frame_id\": sample[\"frame_id\"],\n            \"image_path\": sample[\"image_path\"],\n            \"head_bbox\": sample[\"head_bbox\"],\n            \"original_size\": (width, height)\n        }\n\n        return img, head_img, head_pos, gaze_heatmap, in_frame, object_label, gaze_vector, object_masks, metadata\n\ndef get_transforms(input_size=224, augment=True):\n    \"\"\"\n    Get data transformations for training and validation\n    \"\"\"\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n\n    if augment:\n        # Training transforms with augmentation\n        transform = transforms.Compose([\n            transforms.Resize((input_size, input_size)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n            transforms.ToTensor(),\n            normalize\n        ])\n    else:\n        # Validation/test transforms without augmentation\n        transform = transforms.Compose([\n            transforms.Resize((input_size, input_size)),\n            transforms.ToTensor(),\n            normalize\n        ])\n\n    return transform\n\n\ndef visualize_sample_with_objects(sample, save_path=None):\n    \"\"\"\n    Visualize a dataset sample with object masks\n\n    Args:\n        sample: Tuple of tensors from dataset __getitem__\n        save_path: Path to save visualization (if None, displays inline)\n    \"\"\"\n    img, head_img, head_pos, gaze_heatmap, in_frame, object_label, gaze_vector, object_masks, metadata = sample\n\n    # Denormalize image\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n    img_vis = img.clone()\n    img_vis = img_vis * std + mean\n    img_vis = img_vis.permute(1, 2, 0).numpy()\n    img_vis = np.clip(img_vis, 0, 1)\n\n    head_img_vis = head_img.clone()\n    head_img_vis = head_img_vis * std + mean\n    head_img_vis = head_img_vis.permute(1, 2, 0).numpy()\n    head_img_vis = np.clip(head_img_vis, 0, 1)\n\n    # Create figure\n    plt.figure(figsize=(15, 12))\n\n    # Create a 3x3 grid\n    plt.subplot(3, 3, 1)\n    plt.imshow(img_vis)\n    plt.title(f\"Frame ID: {metadata['frame_id']}\")\n    plt.axis('off')\n\n    plt.subplot(3, 3, 2)\n    plt.imshow(head_img_vis)\n    plt.title(\"Head/Person Crop\")\n    plt.axis('off')\n\n    plt.subplot(3, 3, 3)\n    plt.imshow(head_pos.squeeze().numpy(), cmap='gray')\n    plt.title(\"Head Position Channel\")\n    plt.axis('off')\n\n    # Display select object mask channels\n    category_names = [\"People\", \"Boards\", \"Books\", \"Monitors\", \"Phones\",\n                     \"Tables\", \"Water Disp.\", \"Mugs\", \"Lamps\", \"Other1\", \"Other2\"]\n\n    # Display first few object masks\n    for i in range(min(4, object_masks.shape[0])):\n        plt.subplot(3, 3, 4 + i)\n        plt.imshow(object_masks[i].numpy(), cmap='viridis')\n        plt.title(f\"Object: {category_names[i]}\")\n        plt.axis('off')\n\n    # Gaze heatmap and visualizations\n    plt.subplot(3, 3, 8)\n    plt.imshow(gaze_heatmap.numpy(), cmap='jet')\n    plt.title(f\"Gaze Heatmap (In-frame: {bool(in_frame.item())})\")\n    plt.axis('off')\n\n    plt.subplot(3, 3, 9)\n    # Original image with heatmap overlay\n    plt.imshow(img_vis)\n\n    # Resize heatmap to match image size for overlay\n    heatmap_vis = gaze_heatmap.numpy()\n    heatmap_vis = cv2.resize(heatmap_vis, (img_vis.shape[1], img_vis.shape[0]))\n\n    # Only show heatmap if gaze is in frame\n    if in_frame.item():\n        plt.imshow(heatmap_vis, cmap='jet', alpha=0.5)\n\n    plt.title(\"Heatmap Overlay\")\n    plt.axis('off')\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()\n\ndef test_dataset(xml_path, image_folder):\n    \"\"\"\n    Test the dataset with visualization\n\n    Args:\n        xml_path: Path to the XML annotation file\n        image_folder: Path to the folder with images\n    \"\"\"\n    # Create transforms\n    transform = get_transforms(augment=False)\n\n    # Create dataset\n    dataset = GESCAMCustomDataset(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        transform=transform\n    )\n\n    # Check dataset size\n    print(f\"\\nDataset contains {len(dataset)} samples\")\n\n    # If dataset has samples, visualize some\n    if len(dataset) > 0:\n        print(\"\\nVisualizing samples:\")\n        num_samples = min(3, len(dataset))\n        for i in range(num_samples):\n            # Get a sample\n            sample_idx = i\n            sample = dataset[sample_idx]\n\n            # Visualize\n            save_path = f\"sample_with_objects_{i}.png\"\n            visualize_sample_with_objects(sample, save_path)\n            print(f\"Sample {i} visualization saved to {save_path}\")\n    else:\n        print(\"No samples to visualize!\")\n\n    return dataset\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example paths (replace with actual paths)\n    print(\"hey\")\n    print(xml_path)\n    xml_path = xml_path\n    image_folder = image_folder\n\n    # Test the dataset\n    dataset = test_dataset(xml_path, image_folder)\n\n    # Create DataLoader if we have samples\n    if len(dataset) > 0:\n        batch_size = 4\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n        # Test the DataLoader by fetching a batch\n        for batch in dataloader:\n            print(f\"Successfully loaded a batch of size {len(batch[0])}\")\n            break","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HXcwFlyIeLK_","outputId":"984f93f5-6fe5-4791-91bc-47627d523001"},"outputs":[{"output_type":"stream","name":"stdout","text":["hey\n","/root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n","Root tag: annotations, with 307 child elements\n"]},{"output_type":"stream","name":"stderr","text":["\rParsing frames:   0%|          | 0/305 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 56 boxes and 14 polylines in first frame\n","Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n"]},{"output_type":"stream","name":"stderr","text":["\rParsing frames: 100%|██████████| 305/305 [00:00<00:00, 12946.17it/s]"]},{"output_type":"stream","name":"stdout","text":["Successfully parsed 305 frames\n","Found 305 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 305 frames with person boxes, 305 frames with sight lines\n","Created dataset with 4575 samples\n","\n","Dataset contains 4575 samples\n","\n","Visualizing samples:\n","Sample 0 visualization saved to sample_with_objects_0.png\n","Sample 1 visualization saved to sample_with_objects_1.png\n","Sample 2 visualization saved to sample_with_objects_2.png\n","Successfully loaded a batch of size 4\n"]}],"execution_count":6},{"cell_type":"code","source":"def main():\n    # Set paths to your data\n    xml_path = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\"\n    image_folder = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\"\n\n\n    # Create transforms\n    transform = get_transforms(augment=False)\n\n    print(\"Creating dataset...\")\n    # Create dataset with the customized class\n    dataset = GESCAMCustomDataset(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        transform=transform\n    )\n\n    # If we have samples, create a DataLoader and visualize\n    if len(dataset) > 0:\n        # Create DataLoader\n        batch_size = 4\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        print(f\"Created DataLoader with batch size {batch_size}\")\n\n        # Visualize some samples\n        print(\"Visualizing samples...\")\n        num_samples = min(5, len(dataset))\n        for i in range(num_samples):\n            # Choose random sample for variety\n            sample_idx = np.random.randint(0, len(dataset))\n            sample = dataset[sample_idx]\n\n            # Visualize\n            save_path = os.path.join(output_dir, f\"sample_{i}.png\")\n            # Use the new visualization function that handles object masks\n            visualize_sample_with_objects(sample, save_path)\n            print(f\"Sample {i} visualization saved to {save_path}\")\n\n        # Create a video visualization\n        create_visualization_video(dataset, os.path.join(output_dir, \"visualization.mp4\"),\n                                  num_samples=min(30, len(dataset)), fps=2)\n    else:\n        print(\"No samples found in the dataset!\")\n\n    print(\"Done!\")\n\ndef create_visualization_video(dataset, output_video_path, num_samples=20, fps=5):\n    \"\"\"\n    Create a video visualizing dataset samples\n\n    Args:\n        dataset: Dataset instance\n        output_video_path: Path to save the video\n        num_samples: Number of samples to include\n        fps: Frames per second\n    \"\"\"\n    if len(dataset) == 0:\n        print(\"Cannot create video with empty dataset\")\n        return\n\n    print(f\"Creating visualization video with {num_samples} samples...\")\n\n    # Create a temporary directory for frames\n    temp_dir = \"temp_viz_frames\"\n    os.makedirs(temp_dir, exist_ok=True)\n\n    # Get evenly distributed sample indices\n    indices = np.linspace(0, len(dataset)-1, num_samples).astype(int)\n\n    # Visualize each sample\n    for i, idx in enumerate(tqdm(indices, desc=\"Generating frames\")):\n        sample = dataset[idx]\n\n        # Save visualization to temp file\n        temp_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n        # Use the new visualization function that handles object masks\n        visualize_sample_with_objects(sample, temp_path)\n\n    # Get size of the first frame to set video dimensions\n    first_frame = cv2.imread(os.path.join(temp_dir, \"frame_0000.png\"))\n    height, width, _ = first_frame.shape  # Fixed the syntax error here\n\n    # Create video writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n\n    # Add frames to video\n    for i in range(len(indices)):\n        frame_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n        frame = cv2.imread(frame_path)\n        video_writer.write(frame)\n\n    # Release video writer\n    video_writer.release()\n\n    # Clean up temporary files\n    for i in range(len(indices)):\n        frame_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n        if os.path.exists(frame_path):\n            os.remove(frame_path)\n    if os.path.exists(temp_dir):\n        os.rmdir(temp_dir)\n\n    print(f\"Visualization video saved to {output_video_path}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Utr8fSG9eLLB","outputId":"cad53a48-390a-4209-de7a-b5cf6aa702d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating dataset...\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n","Root tag: annotations, with 307 child elements\n"]},{"output_type":"stream","name":"stderr","text":["Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 11560.30it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 56 boxes and 14 polylines in first frame\n","Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n","Successfully parsed 305 frames\n","Found 305 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 305 frames with person boxes, 305 frames with sight lines\n","Created dataset with 4575 samples\n","Created DataLoader with batch size 4\n","Visualizing samples...\n","Sample 0 visualization saved to ./sample_0.png\n","Sample 1 visualization saved to ./sample_1.png\n","Sample 2 visualization saved to ./sample_2.png\n","Sample 3 visualization saved to ./sample_3.png\n","Sample 4 visualization saved to ./sample_4.png\n","Creating visualization video with 30 samples...\n"]},{"output_type":"stream","name":"stderr","text":["Generating frames: 100%|██████████| 30/30 [00:26<00:00,  1.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Visualization video saved to ./visualization.mp4\n","Done!\n"]}],"execution_count":7},{"cell_type":"markdown","source":"## COMBINING MULTIPLE DATA FOLDERS","metadata":{"id":"ZAuIZNyLeLLB"}},{"cell_type":"code","source":"def combine_datasets(xml_paths, image_folders, transform):\n    \"\"\"\n    Combine multiple datasets into one\n\n    Args:\n        xml_paths: List of paths to XML annotation files\n        image_folders: List of paths to image folders\n        transform: Transformations to apply\n\n    Returns:\n        combined_dataset: Combined dataset\n    \"\"\"\n    all_datasets = []\n\n    for xml_path, image_folder in zip(xml_paths, image_folders):\n        dataset = GESCAMCustomDataset(\n            xml_path=xml_path,\n            image_folder=image_folder,\n            transform=transform\n        )\n        all_datasets.append(dataset)\n\n    # Create a simple wrapper dataset class\n    class CombinedDataset(torch.utils.data.Dataset):\n        def __init__(self, datasets):\n            self.datasets = datasets\n            self.lengths = [len(d) for d in datasets]\n            self.cumulative_lengths = [0]\n\n            for length in self.lengths:\n                self.cumulative_lengths.append(self.cumulative_lengths[-1] + length)\n\n        def __len__(self):\n            return self.cumulative_lengths[-1]\n\n        def __getitem__(self, idx):\n            # Find which dataset this index belongs to\n            dataset_idx = bisect.bisect_right(self.cumulative_lengths, idx) - 1\n            sample_idx = idx - self.cumulative_lengths[dataset_idx]\n            return self.datasets[dataset_idx][sample_idx]\n\n    return CombinedDataset(all_datasets)","metadata":{"id":"0Y8ju0JZeLLC"},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define all your data paths\nxml_paths = [\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\",\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video02(0-300)/annotations.xml\",\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video02(301-600)/annotations.xml\",\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video03_final/annotations.xml\",\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video04_final/annotations.xml\",\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video05_final/annotations.xml\"\n\n]\n\nimage_folders = [\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\",\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video02(0-300)/images\",\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video02(301-600)/images\",\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video03_final/images\",\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video04_final/images\",\n    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video05_final/images\"\n\n]\n\n# Create transforms\ntransform = get_transforms(augment=True)\n\n# Combine datasets\ncombined_dataset = combine_datasets(xml_paths, image_folders, transform)\n\n# Rest of your code remains the same, just use combined_dataset instead of dataset\n# train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size], generator=generator)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUxIYaP3eLLC","outputId":"b127ec6c-bda9-4387-e3ab-821e23232e0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n","Root tag: annotations, with 307 child elements\n"]},{"output_type":"stream","name":"stderr","text":["\rParsing frames:   0%|          | 0/305 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 56 boxes and 14 polylines in first frame\n","Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n"]},{"output_type":"stream","name":"stderr","text":["\rParsing frames: 100%|██████████| 305/305 [00:00<00:00, 12079.80it/s]"]},{"output_type":"stream","name":"stdout","text":["Successfully parsed 305 frames\n","Found 305 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 305 frames with person boxes, 305 frames with sight lines\n","Created dataset with 4575 samples\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(0-300)/annotations.xml\n","Root tag: annotations, with 308 child elements\n"]},{"output_type":"stream","name":"stderr","text":["Parsing frames: 100%|██████████| 306/306 [00:00<00:00, 15485.17it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 52 boxes and 14 polylines in first frame\n","Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n","Successfully parsed 306 frames\n","Found 306 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 306 frames with person boxes, 306 frames with sight lines\n","Created dataset with 4284 samples\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(301-600)/annotations.xml\n","Root tag: annotations, with 307 child elements\n"]},{"output_type":"stream","name":"stderr","text":["Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 14716.01it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 49 boxes and 14 polylines in first frame\n","Sample box labels: ['Water Dispenser', 'monitor', 'monitor', 'monitor', 'monitor']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n","Successfully parsed 305 frames\n","Found 305 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 305 frames with person boxes, 305 frames with sight lines\n","Created dataset with 4555 samples\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video03_final/annotations.xml\n","Root tag: annotations, with 603 child elements\n"]},{"output_type":"stream","name":"stderr","text":["Parsing frames: 100%|██████████| 601/601 [00:00<00:00, 11905.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 59 boxes and 14 polylines in first frame\n","Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n","Successfully parsed 601 frames\n","Found 601 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 601 frames with person boxes, 601 frames with sight lines\n","Created dataset with 9015 samples\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video04_final/annotations.xml\n","Root tag: annotations, with 602 child elements\n"]},{"output_type":"stream","name":"stderr","text":["Parsing frames: 100%|██████████| 600/600 [00:00<00:00, 15369.57it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 45 boxes and 13 polylines in first frame\n","Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n","Successfully parsed 600 frames\n","Found 600 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 600 frames with person boxes, 600 frames with sight lines\n","Created dataset with 8400 samples\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video05_final/annotations.xml\n","Root tag: annotations, with 602 child elements\n"]},{"output_type":"stream","name":"stderr","text":["Parsing frames: 100%|██████████| 600/600 [00:00<00:00, 19956.56it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 35 boxes and 12 polylines in first frame\n","Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n","Successfully parsed 600 frames\n","Found 600 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 600 frames with person boxes, 600 frames with sight lines\n","Created dataset with 7260 samples\n"]}],"execution_count":9},{"cell_type":"markdown","source":"## MODEL ARCHITECTURE","metadata":{"id":"AemL0bY0eLLC"}},{"cell_type":"code","source":"class SoftAttention(nn.Module):\n    \"\"\"\n    Soft attention module for attending to scene features based on head features\n    \"\"\"\n    def __init__(self, head_channels=256, output_size=(7, 7)):\n        super(SoftAttention, self).__init__()\n        self.output_h, self.output_w = output_size\n\n        # Attention layers\n        self.attention = nn.Sequential(\n            nn.Linear(head_channels, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, self.output_h * self.output_w),\n            nn.Sigmoid()\n        )\n\n    def forward(self, head_features):\n        # Input head_features shape: [batch_size, head_channels]\n        batch_size = head_features.size(0)\n\n        # Generate attention weights\n        attn_weights = self.attention(head_features)\n\n        # Reshape to spatial attention map\n        attn_weights = attn_weights.view(batch_size, 1, self.output_h, self.output_w)\n\n        return attn_weights\n\n\nclass MSGESCAMModel(nn.Module):\n    \"\"\"\n    Multi-Stream GESCAM architecture for gaze estimation in classroom settings\n    \"\"\"\n    def __init__(self, pretrained=True, output_size=64):\n        super(MSGESCAMModel, self).__init__()\n\n        # Store the output size\n        self.output_size = output_size\n\n        # Feature dimensions\n        self.backbone_dim = 512  # ResNet18 outputs 512 feature channels\n        self.feature_dim = 256\n\n        # Downsampled feature map size\n        self.map_size = 7  # ResNet outputs 7x7 feature maps\n\n        # === Scene Pathway ===\n        # Load a pre-trained ResNet18 without the final layer\n        self.scene_backbone = models.resnet18(pretrained=pretrained)\n\n        # Save the original conv1 weights\n        original_conv1_weight = self.scene_backbone.conv1.weight.clone()\n\n        # Create a new conv1 layer that accepts 4 channels (RGB + head position)\n        self.scene_backbone.conv1 = nn.Conv2d(\n            4, 64, kernel_size=7, stride=2, padding=3, bias=False\n        )\n\n        # Initialize with the pre-trained weights\n        with torch.no_grad():\n            self.scene_backbone.conv1.weight[:, :3] = original_conv1_weight\n            # Initialize the new channel with small random values\n            self.scene_backbone.conv1.weight[:, 3] = 0.01 * torch.randn_like(self.scene_backbone.conv1.weight[:, 0])\n\n        # Remove the final FC layer from the scene backbone\n        self.scene_features = nn.Sequential(*list(self.scene_backbone.children())[:-1])\n\n        # Add a FC layer to transform from backbone_dim to feature_dim\n        self.scene_fc = nn.Linear(self.backbone_dim, self.feature_dim)\n\n        # === Head Pathway ===\n        # Load another pre-trained ResNet18 for the head pathway\n        self.head_backbone = models.resnet18(pretrained=pretrained)\n\n        # Remove the final FC layer from the head backbone\n        self.head_features = nn.Sequential(*list(self.head_backbone.children())[:-1])\n\n        # Add a FC layer to transform from backbone_dim to feature_dim\n        self.head_fc = nn.Linear(self.backbone_dim, self.feature_dim)\n\n        # === Objects Mask Enhancement (optional) ===\n        # This takes an objects mask (with channels for different object classes)\n        self.objects_conv = nn.Conv2d(11, 512, kernel_size=3, stride=2, padding=1)  # 11 object categories\n\n        # Soft attention mechanism\n        self.attention = SoftAttention(head_channels=self.feature_dim, output_size=(self.map_size, self.map_size))\n\n        # === Fusion and Encoding ===\n        # Fusion of attended scene features and head features\n        self.encode = nn.Sequential(\n            nn.Conv2d(self.backbone_dim + self.feature_dim, self.feature_dim, kernel_size=1),\n            nn.BatchNorm2d(self.feature_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(self.feature_dim),\n            nn.ReLU(inplace=True)\n        )\n\n        # Calculate the number of deconvolution layers needed\n        # Each layer doubles the size, so we need log2(output_size / 7) layers\n        self.num_deconv_layers = max(1, int(math.log2(output_size / 7)) + 1)\n\n        # === Decoding for heatmap generation ===\n        deconv_layers = []\n        in_channels = self.feature_dim\n        out_size = self.map_size\n\n        # Create deconvolution layers\n        for i in range(self.num_deconv_layers - 1):\n            # Calculate output channels\n            out_channels = max(32, in_channels // 2)\n\n            # Add deconv layer\n            deconv_layers.extend([\n                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            ])\n\n            in_channels = out_channels\n            out_size *= 2\n\n        # Final layer to adjust to exact output size\n        if out_size != output_size:\n            # Add a final layer with correct stride to reach exactly output_size\n            scale_factor = output_size / out_size\n            stride = 2 if scale_factor > 1 else 1\n            output_padding = 1 if scale_factor > 1 else 0\n\n            deconv_layers.extend([\n                nn.ConvTranspose2d(\n                    in_channels, 1, kernel_size=3,\n                    stride=stride, padding=1, output_padding=output_padding\n                )\n            ])\n        else:\n            # If we're already at the right size, just add a 1x1 conv\n            deconv_layers.append(nn.Conv2d(in_channels, 1, kernel_size=1))\n\n        self.decode = nn.Sequential(*deconv_layers)\n\n        # === In-frame probability prediction ===\n        self.in_frame_fc = nn.Sequential(\n            nn.Linear(self.feature_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, scene_img, head_img, head_pos, objects_mask=None):\n        \"\"\"\n        Forward pass through the MS-GESCAM network\n\n        Args:\n            scene_img: Scene image tensor [batch_size, 3, H, W]\n            head_img: Head crop tensor [batch_size, 3, H, W]\n            head_pos: Head position mask [batch_size, 1, H, W]\n            objects_mask: Optional mask of object categories [batch_size, num_categories, H, W]\n\n        Returns:\n            heatmap: Predicted gaze heatmap [batch_size, 1, output_size, output_size]\n            in_frame: Probability of gaze target being in frame [batch_size, 1]\n        \"\"\"\n        batch_size = scene_img.size(0)\n\n        # === Process scene pathway ===\n        # Concatenate scene image and head position channel\n        scene_input = torch.cat([scene_img, head_pos], dim=1)\n\n        # Process through ResNet layers until layer4 (skipping the final global pooling and FC)\n        x = self.scene_backbone.conv1(scene_input)\n        x = self.scene_backbone.bn1(x)\n        x = self.scene_backbone.relu(x)\n        x = self.scene_backbone.maxpool(x)\n\n        x = self.scene_backbone.layer1(x)\n        x = self.scene_backbone.layer2(x)\n        x = self.scene_backbone.layer3(x)\n        scene_features_map = self.scene_backbone.layer4(x)  # [batch_size, 512, 7, 7]\n\n        # Global average pooling for scene features\n        scene_vector = F.adaptive_avg_pool2d(scene_features_map, (1, 1)).view(batch_size, -1)\n        scene_features = self.scene_fc(scene_vector)  # [batch_size, feature_dim]\n\n        # === Process head pathway ===\n        # Process through the entire head features extractor\n        head_vector = self.head_features(head_img).view(batch_size, -1)  # [batch_size, 512]\n        head_features = self.head_fc(head_vector)  # [batch_size, feature_dim]\n\n        # Process objects mask if provided\n        if objects_mask is not None:\n            obj_features = self.objects_conv(objects_mask)\n            # Resize to match scene features map if needed\n            if obj_features.size(2) != scene_features_map.size(2):\n                obj_features = F.adaptive_avg_pool2d(\n                    obj_features, (scene_features_map.size(2), scene_features_map.size(3))\n                )\n            # Add object features to scene features\n            scene_features_map = scene_features_map + obj_features\n\n        # Generate attention map from head features\n        attn_weights = self.attention(head_features)  # [batch_size, 1, 7, 7]\n\n        # Apply attention to scene features map\n        attended_scene = scene_features_map * attn_weights  # [batch_size, 512, 7, 7]\n\n        # Reshape head features to concatenate with scene features\n        head_features_map = head_features.view(batch_size, self.feature_dim, 1, 1)\n        head_features_map = head_features_map.expand(-1, -1, self.map_size, self.map_size)\n\n        # Concatenate attended scene features and head features\n        concat_features = torch.cat([attended_scene, head_features_map], dim=1)  # [batch_size, 512+256, 7, 7]\n\n        # Encode the concatenated features\n        encoded = self.encode(concat_features)  # [batch_size, 256, 7, 7]\n\n        # Predict in-frame probability\n        in_frame = self.in_frame_fc(head_features)\n\n        # Decode to get the final heatmap\n        heatmap = self.decode(encoded)\n\n        # Ensure output size is correct\n        if heatmap.size(2) != self.output_size or heatmap.size(3) != self.output_size:\n            heatmap = F.interpolate(\n                heatmap,\n                size=(self.output_size, self.output_size),\n                mode='bilinear',\n                align_corners=True\n            )\n\n        # Apply sigmoid to get values between 0 and 1\n        heatmap = torch.sigmoid(heatmap)\n\n        return heatmap, in_frame\n\n\nclass CombinedLoss(nn.Module):\n    \"\"\"\n    Combined loss function for gaze estimation\n    \"\"\"\n    def __init__(self, heatmap_weight=1.0, in_frame_weight=1.0, angular_weight=0.5):\n        super(CombinedLoss, self).__init__()\n        self.heatmap_weight = heatmap_weight\n        self.in_frame_weight = in_frame_weight\n        self.angular_weight = angular_weight\n\n        self.mse_loss = nn.MSELoss(reduction='none')\n        self.bce_loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, pred_heatmap, target_heatmap, pred_in_frame, target_in_frame,\n                pred_vector=None, target_vector=None):\n        \"\"\"\n        Combined loss function\n\n        Args:\n            pred_heatmap: Predicted gaze heatmap [batch_size, 1, H, W]\n            target_heatmap: Target gaze heatmap [batch_size, H, W]\n            pred_in_frame: Predicted in-frame probability [batch_size, 1]\n            target_in_frame: Target in-frame label [batch_size, 1]\n            pred_vector: Optional predicted gaze vector [batch_size, 2]\n            target_vector: Optional target gaze vector [batch_size, 2]\n\n        Returns:\n            total_loss: Combined loss\n            loss_dict: Dictionary with individual loss components\n        \"\"\"\n        batch_size = pred_heatmap.size(0)\n\n        # Check and fix size mismatches\n        if pred_heatmap.size(-1) != target_heatmap.size(-1) or pred_heatmap.size(-2) != target_heatmap.size(-2):\n            # Resize prediction to match target\n            pred_heatmap = F.interpolate(\n                pred_heatmap,\n                size=(target_heatmap.size(1), target_heatmap.size(2)),\n                mode='bilinear',\n                align_corners=True\n            )\n\n        # Reshape heatmaps if needed\n        if pred_heatmap.size(1) == 1:\n            pred_heatmap = pred_heatmap.squeeze(1)\n\n        # Heatmap loss (MSE) - only for in-frame samples\n        heatmap_loss = self.mse_loss(pred_heatmap, target_heatmap)\n        heatmap_loss = heatmap_loss.mean(dim=(1, 2))  # Average over spatial dimensions\n\n        # Apply in-frame masking\n        masked_heatmap_loss = heatmap_loss * target_in_frame.squeeze()\n\n        # Average over valid samples\n        num_valid = max(1, target_in_frame.sum().item())  # Avoid division by zero\n        heatmap_loss = masked_heatmap_loss.sum() / num_valid\n\n        # In-frame prediction loss (BCE)\n        in_frame_loss = self.bce_loss(pred_in_frame, target_in_frame)\n\n        # Angular loss (if vectors are provided)\n        angular_loss = torch.tensor(0.0, device=pred_heatmap.device)\n        if pred_vector is not None and target_vector is not None:\n            # Compute cosine similarity between predicted and target vectors\n            target_in_frame_bool = target_in_frame.squeeze().bool()\n\n            if target_in_frame_bool.sum() > 0:\n                # Only compute for in-frame samples\n                pred_vec_valid = pred_vector[target_in_frame_bool]\n                target_vec_valid = target_vector[target_in_frame_bool]\n\n                # Normalize vectors\n                pred_norm = torch.norm(pred_vec_valid, dim=1, keepdim=True)\n                target_norm = torch.norm(target_vec_valid, dim=1, keepdim=True)\n\n                # Avoid division by zero\n                pred_norm = torch.clamp(pred_norm, min=1e-7)\n                target_norm = torch.clamp(target_norm, min=1e-7)\n\n                pred_vec_norm = pred_vec_valid / pred_norm\n                target_vec_norm = target_vec_valid / target_norm\n\n                # Cosine similarity (dot product of normalized vectors)\n                cos_sim = torch.sum(pred_vec_norm * target_vec_norm, dim=1)\n\n                # Angular loss = 1 - cos_sim\n                angular_loss = 1.0 - cos_sim.mean()\n\n        # Combine losses\n        total_loss = (\n            self.heatmap_weight * heatmap_loss +\n            self.in_frame_weight * in_frame_loss +\n            self.angular_weight * angular_loss\n        )\n\n        # Create loss dictionary for logging\n        loss_dict = {\n            'total_loss': total_loss.item(),\n            'heatmap_loss': heatmap_loss.item(),\n            'in_frame_loss': in_frame_loss.item(),\n            'angular_loss': angular_loss.item() if isinstance(angular_loss, torch.Tensor) else angular_loss\n        }\n\n        return total_loss, loss_dict\n\n\n# Example usage to test the model\ndef test_model():\n    # Create model\n    model = MSGESCAMModel(pretrained=True, output_size=64)\n\n    # Create sample batch\n    batch_size = 2\n    scene_img = torch.randn(batch_size, 3, 224, 224)\n    head_img = torch.randn(batch_size, 3, 224, 224)\n    head_pos = torch.randn(batch_size, 1, 224, 224)\n\n    # Forward pass\n    pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos)\n\n    print(f\"Model test successful\")\n    print(f\"Predicted heatmap shape: {pred_heatmap.shape}\")\n    print(f\"Predicted in-frame shape: {pred_in_frame.shape}\")\n\n    return model\n\nif __name__ == \"__main__\":\n    model = test_model()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-2fibCTeLLC","outputId":"a09ec12b-9858-49a6-b614-d74f6f289f28"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 176MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model test successful\n","Predicted heatmap shape: torch.Size([2, 1, 64, 64])\n","Predicted in-frame shape: torch.Size([2, 1])\n"]}],"execution_count":10},{"cell_type":"code","source":"# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Early stopping class\nclass EarlyStopping:\n    \"\"\"Early stopping to prevent overfitting\"\"\"\n    def __init__(self, patience=5, min_delta=0, verbose=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = float('inf')\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.verbose:\n                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n\ndef train_one_epoch(model, train_loader, criterion, optimizer, device):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    train_losses = []\n\n    train_pbar = tqdm(train_loader, desc=\"Training\")\n    for batch in train_pbar:\n        # Unpack the batch\n        scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, object_masks, metadata = batch\n\n        # Move to device\n        scene_img = scene_img.to(device)\n        head_img = head_img.to(device)\n        head_pos = head_pos.to(device)\n        target_heatmap = target_heatmap.to(device)\n        target_in_frame = target_in_frame.to(device)\n        target_vector = target_vector.to(device)\n        object_masks = object_masks.to(device)\n\n        # Forward pass\n        pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos, object_masks)\n\n        # Compute loss\n        loss, loss_dict = criterion(\n            pred_heatmap, target_heatmap,\n            pred_in_frame, target_in_frame,\n            None, target_vector\n        )\n\n        # Backward pass and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Track metrics\n        train_losses.append(loss_dict)\n        train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n    # Calculate metrics\n    avg_loss = sum(d['total_loss'] for d in train_losses) / len(train_losses)\n    avg_heatmap_loss = sum(d['heatmap_loss'] for d in train_losses) / len(train_losses)\n    avg_in_frame_loss = sum(d['in_frame_loss'] for d in train_losses) / len(train_losses)\n\n    return {\n        'loss': avg_loss,\n        'heatmap_loss': avg_heatmap_loss,\n        'in_frame_loss': avg_in_frame_loss\n    }\n\ndef validate(model, val_loader, criterion, device):\n    \"\"\"Validate the model\"\"\"\n    model.eval()\n    val_losses = []\n\n    val_pbar = tqdm(val_loader, desc=\"Validation\")\n    with torch.no_grad():\n        for batch in val_pbar:\n            # Unpack the batch\n            scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, object_masks, metadata = batch\n\n            # Move to device\n            scene_img = scene_img.to(device)\n            head_img = head_img.to(device)\n            head_pos = head_pos.to(device)\n            target_heatmap = target_heatmap.to(device)\n            target_in_frame = target_in_frame.to(device)\n            target_vector = target_vector.to(device)\n            object_masks = object_masks.to(device)\n\n            # Forward pass\n            pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos, object_masks)\n\n            # Compute loss\n            loss, loss_dict = criterion(\n                pred_heatmap, target_heatmap,\n                pred_in_frame, target_in_frame,\n                None, target_vector\n            )\n\n            # Track metrics\n            val_losses.append(loss_dict)\n            val_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n    # Calculate metrics\n    avg_loss = sum(d['total_loss'] for d in val_losses) / len(val_losses)\n    avg_heatmap_loss = sum(d['heatmap_loss'] for d in val_losses) / len(val_losses)\n    avg_in_frame_loss = sum(d['in_frame_loss'] for d in val_losses) / len(val_losses)\n\n    return {\n        'loss': avg_loss,\n        'heatmap_loss': avg_heatmap_loss,\n        'in_frame_loss': avg_in_frame_loss\n    }\n\ndef visualize_predictions(model, dataset, device, indices=None, num_samples=5, save_dir=None):\n    \"\"\"Visualize model predictions\"\"\"\n    model.eval()\n\n    if indices is None:\n        # Choose random samples\n        indices = np.random.choice(len(dataset), num_samples, replace=False)\n\n    with torch.no_grad():\n        for i, idx in enumerate(indices):\n            # Get a sample\n            sample = dataset[idx]\n            scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, object_masks, metadata = sample\n\n            # Add batch dimension\n            scene_img = scene_img.unsqueeze(0).to(device)\n            head_img = head_img.unsqueeze(0).to(device)\n            head_pos = head_pos.unsqueeze(0).to(device)\n            object_masks = object_masks.unsqueeze(0).to(device)\n\n            # Forward pass\n            pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos, object_masks)\n\n            # Convert predictions to numpy\n            pred_heatmap = pred_heatmap.squeeze().cpu().numpy()\n            pred_in_frame_prob = torch.sigmoid(pred_in_frame).squeeze().cpu().numpy()\n\n            # Create figure\n            plt.figure(figsize=(15, 10))\n\n            # Denormalize image\n            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n            img_vis = scene_img.squeeze().cpu()\n            img_vis = img_vis * std + mean\n            img_vis = img_vis.permute(1, 2, 0).numpy()\n            img_vis = np.clip(img_vis, 0, 1)\n\n            # Original image\n            plt.subplot(2, 3, 1)\n            plt.imshow(img_vis)\n            plt.title(f\"Frame {metadata['frame_id']}\")\n            plt.axis('off')\n\n            # Head crop\n            head_img_vis = head_img.squeeze().cpu()\n            head_img_vis = head_img_vis * std + mean\n            head_img_vis = head_img_vis.permute(1, 2, 0).numpy()\n            head_img_vis = np.clip(head_img_vis, 0, 1)\n\n            plt.subplot(2, 3, 2)\n            plt.imshow(head_img_vis)\n            plt.title(\"Head Crop\")\n            plt.axis('off')\n\n            # Ground truth heatmap\n            plt.subplot(2, 3, 3)\n            plt.imshow(target_heatmap.numpy(), cmap='jet')\n            plt.title(f\"GT Heatmap (In-frame: {bool(target_in_frame.item())})\")\n            plt.axis('off')\n\n            # Predicted heatmap\n            plt.subplot(2, 3, 4)\n            plt.imshow(pred_heatmap, cmap='jet')\n            plt.title(f\"Pred Heatmap (In-frame: {pred_in_frame_prob:.2f})\")\n            plt.axis('off')\n\n            # Overlay on original image\n            plt.subplot(2, 3, 5)\n            plt.imshow(img_vis)\n            plt.imshow(pred_heatmap, cmap='jet', alpha=0.5)\n            plt.title(\"Prediction Overlay\")\n            plt.axis('off')\n\n            # Error visualization\n            plt.subplot(2, 3, 6)\n            error_map = np.abs(pred_heatmap - target_heatmap.numpy())\n            plt.imshow(error_map, cmap='hot')\n            plt.title(\"Prediction Error\")\n            plt.axis('off')\n\n            plt.tight_layout()\n\n            # Save or display\n            if save_dir:\n                os.makedirs(save_dir, exist_ok=True)\n                plt.savefig(os.path.join(save_dir, f\"pred_{i}_sample_{idx}.png\"))\n                plt.close()\n            else:\n                plt.show()\n\ndef plot_training_history(history, save_path=None):\n    \"\"\"Plot training history\"\"\"\n    plt.figure(figsize=(12, 5))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Total Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # Plot component losses\n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_heatmap_loss'], label='Train Heatmap Loss')\n    plt.plot(history['val_heatmap_loss'], label='Val Heatmap Loss')\n    plt.plot(history['train_in_frame_loss'], label='Train In-Frame Loss')\n    plt.plot(history['val_in_frame_loss'], label='Val In-Frame Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Component Losses')\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()\n\ndef test_model():\n    \"\"\"Simple test to verify the model architecture\"\"\"\n    # Create model\n    model = MSGESCAMModel(pretrained=True, output_size=64)\n\n    # Create sample batch\n    batch_size = 2\n    scene_img = torch.randn(batch_size, 3, 224, 224)\n    head_img = torch.randn(batch_size, 3, 224, 224)\n    head_pos = torch.randn(batch_size, 1, 224, 224)\n    object_masks = torch.randn(batch_size, 11, 224, 224)  # 11 object categories\n\n    # Forward pass\n    pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos, object_masks)\n    ## without headmasks\n    # pred_heatmap2, pred_in_frame2 = model(scene_img, head_img, head_pos)\n\n    print(f\"Model test successful\")\n    print(f\"Predicted heatmap shape: {pred_heatmap.shape}\")\n    print(f\"Predicted in-frame shape: {pred_in_frame.shape}\")\n\n# Training function\ndef train_gescam_model(xml_path, image_folder, output_dir, batch_size=8, epochs=20,\n                      lr=1e-4, val_split=0.2, seed=42):\n    \"\"\"\n    Train the MS-GESCAM model\n\n    Args:\n        xml_path: Path to XML annotations\n        image_folder: Path to image folder\n        output_dir: Output directory\n        batch_size: Batch size\n        epochs: Number of epochs\n        lr: Learning rate\n        val_split: Validation split ratio\n        seed: Random seed\n    \"\"\"\n    # Set random seed\n    set_seed(seed)\n\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Create transforms\n    transform = get_transforms(augment=True)\n    val_transform = get_transforms(augment=False)\n\n    # Load dataset\n    print(\"Loading dataset...\")\n    full_dataset = GESCAMCustomDataset(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        transform=transform\n    )\n\n    # Split dataset\n    # val_size = int(val_split * len(full_dataset))\n    # train_size = len(full_dataset) - val_size\n\n    val_size = int(val_split * len(combined_dataset))\n    train_size = len(combined_dataset) - val_size\n\n    # Use different random seeds for train and validation\n    generator = torch.Generator().manual_seed(seed)\n    #train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n    train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size], generator=generator)\n\n    # Create separate validation dataset with different transforms\n    val_dataset_with_transform = GESCAMCustomDataset(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        transform=val_transform\n    )\n\n    # Use the same indices for validation\n    val_indices = [i for i in range(len(full_dataset)) if i not in train_dataset.indices]\n\n    # Create a subset for validation with the correct indices\n    class IndexSubset:\n        def __init__(self, dataset, indices):\n            self.dataset = dataset\n            self.indices = indices\n\n        def __getitem__(self, idx):\n            return self.dataset[self.indices[idx]]\n\n        def __len__(self):\n            return len(self.indices)\n\n    val_dataset = IndexSubset(val_dataset_with_transform, val_indices)\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n\n    print(f\"Dataset split: {train_size} train, {val_size} validation\")\n\n    # Create model\n    print(\"Creating model...\")\n    model = MSGESCAMModel(pretrained=True, output_size=64)\n    model = model.to(device)\n\n    # Create loss function\n    criterion = CombinedLoss(\n        heatmap_weight=1.0,\n        in_frame_weight=1.0,\n        angular_weight=0.5\n    )\n\n    # Create optimizer\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    # Create learning rate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer,\n        mode='min',\n        factor=0.5,\n        patience=3,\n        verbose=True\n    )\n\n    # Create early stopping\n    early_stopping = EarlyStopping(patience=7, verbose=True)\n\n    # Training history\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_heatmap_loss': [],\n        'val_heatmap_loss': [],\n        'train_in_frame_loss': [],\n        'val_in_frame_loss': []\n    }\n\n    # Best model state\n    best_val_loss = float('inf')\n\n    # Train model\n    print(f\"Training for {epochs} epochs...\")\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n\n        # Train\n        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        print(f\"Train loss: {train_metrics['loss']:.4f}, \"\n              f\"Heatmap loss: {train_metrics['heatmap_loss']:.4f}, \"\n              f\"In-frame loss: {train_metrics['in_frame_loss']:.4f}\")\n\n        # Validate\n        val_metrics = validate(model, val_loader, criterion, device)\n        print(f\"Val loss: {val_metrics['loss']:.4f}, \"\n              f\"Heatmap loss: {val_metrics['heatmap_loss']:.4f}, \"\n              f\"In-frame loss: {val_metrics['in_frame_loss']:.4f}\")\n\n        # Update history\n        history['train_loss'].append(train_metrics['loss'])\n        history['val_loss'].append(val_metrics['loss'])\n        history['train_heatmap_loss'].append(train_metrics['heatmap_loss'])\n        history['val_heatmap_loss'].append(val_metrics['heatmap_loss'])\n        history['train_in_frame_loss'].append(train_metrics['in_frame_loss'])\n        history['val_in_frame_loss'].append(val_metrics['in_frame_loss'])\n\n        # Step learning rate scheduler\n        scheduler.step(val_metrics['loss'])\n\n        # Save checkpoint\n        checkpoint_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch+1}.pt')\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_loss': train_metrics['loss'],\n            'val_loss': val_metrics['loss'],\n            'history': history\n        }, checkpoint_path)\n\n        # Save best model\n        if val_metrics['loss'] < best_val_loss:\n            best_val_loss = val_metrics['loss']\n            best_model_path = os.path.join(output_dir, 'best_model.pt')\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_metrics['loss']\n            }, best_model_path)\n            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n\n        # Check early stopping\n        early_stopping(val_metrics['loss'])\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered\")\n            break\n\n    # Plot training history\n    history_path = os.path.join(output_dir, 'training_history.png')\n    plot_training_history(history, history_path)\n    print(f\"Training history plot saved to {history_path}\")\n\n    # Load best model for final evaluation\n    checkpoint = torch.load(os.path.join(output_dir, 'best_model.pt'))\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    # Visualize predictions on validation set\n    vis_dir = os.path.join(output_dir, 'visualizations')\n    os.makedirs(vis_dir, exist_ok=True)\n    print(f\"Generating visualizations in {vis_dir}...\")\n\n    # Choose a few random samples from validation set\n    val_samples = np.random.choice(len(val_dataset), min(10, len(val_dataset)), replace=False)\n    visualize_predictions(model, val_dataset, device, indices=val_samples, save_dir=vis_dir)\n\n    print(\"Training complete!\")\n\n    return model, history\n\n# Main execution\nif __name__ == \"__main__\":\n    # For Kaggle, set your paths here\n    xml_path = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\"\n    image_folder = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\"\n\n\n    # Optional: Just test the model architecture\n    # test_model()\n\n    # Train the model\n    model, history = train_gescam_model(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        output_dir=output_dir,\n        batch_size=8,  # Adjust based on your GPU memory\n        epochs=30,     # Adjust as needed\n        lr=1e-4\n    )","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQnwfVfkeLLD","outputId":"4057c45e-7ae6-4a48-ad84-d8ae8bf676a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loading dataset...\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n","Root tag: annotations, with 307 child elements\n"]},{"output_type":"stream","name":"stderr","text":["Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 12135.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 56 boxes and 14 polylines in first frame\n","Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n","Successfully parsed 305 frames\n","Found 305 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 305 frames with person boxes, 305 frames with sight lines\n","Created dataset with 4575 samples\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n","Root tag: annotations, with 307 child elements\n"]},{"output_type":"stream","name":"stderr","text":["Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 11579.34it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 56 boxes and 14 polylines in first frame\n","Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n","Successfully parsed 305 frames\n","Found 305 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 305 frames with person boxes, 305 frames with sight lines\n","Created dataset with 4575 samples\n","Dataset split: 30472 train, 7617 validation\n","Creating model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training for 30 epochs...\n","\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [16:29<00:00,  3.85it/s, loss=0.0135]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0198, Heatmap loss: 0.0121, In-frame loss: 0.0077\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:34<00:00,  3.42it/s, loss=0.0019]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0021, Heatmap loss: 0.0019, In-frame loss: 0.0001\n","Saved new best model with validation loss: 0.0021\n","\n","Epoch 2/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [17:41<00:00,  3.59it/s, loss=0.0012]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0066, Heatmap loss: 0.0013, In-frame loss: 0.0052\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.53it/s, loss=0.0015]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0013, Heatmap loss: 0.0012, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0013\n","\n","Epoch 3/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:35<00:00,  3.41it/s, loss=0.0015]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0037, Heatmap loss: 0.0009, In-frame loss: 0.0027\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:31<00:00,  3.66it/s, loss=0.0011]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0010, Heatmap loss: 0.0010, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0010\n","\n","Epoch 4/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:24<00:00,  3.45it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0041, Heatmap loss: 0.0007, In-frame loss: 0.0033\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.51it/s, loss=0.0004]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0142, Heatmap loss: 0.0010, In-frame loss: 0.0132\n","EarlyStopping counter: 1 out of 7\n","\n","Epoch 5/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:32<00:00,  3.42it/s, loss=0.0009]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0040, Heatmap loss: 0.0006, In-frame loss: 0.0034\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.44it/s, loss=0.0002]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0007, Heatmap loss: 0.0007, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0007\n","\n","Epoch 6/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:14<00:00,  3.48it/s, loss=0.0009]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0037, Heatmap loss: 0.0005, In-frame loss: 0.0031\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.54it/s, loss=0.0002]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0007, Heatmap loss: 0.0007, In-frame loss: 0.0000\n","EarlyStopping counter: 1 out of 7\n","\n","Epoch 7/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:08<00:00,  3.50it/s, loss=0.0009]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0033, Heatmap loss: 0.0005, In-frame loss: 0.0028\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:34<00:00,  3.36it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0007, Heatmap loss: 0.0007, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0007\n","\n","Epoch 8/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [17:08<00:00,  3.70it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0027, Heatmap loss: 0.0004, In-frame loss: 0.0023\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.47it/s, loss=0.0002]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0006, Heatmap loss: 0.0006, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0006\n","\n","Epoch 9/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [17:42<00:00,  3.59it/s, loss=0.0007]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0030, Heatmap loss: 0.0004, In-frame loss: 0.0026\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:32<00:00,  3.59it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0006, Heatmap loss: 0.0006, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0006\n","\n","Epoch 10/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:23<00:00,  3.45it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0021, Heatmap loss: 0.0003, In-frame loss: 0.0018\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:34<00:00,  3.42it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0005, Heatmap loss: 0.0005, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0005\n","\n","Epoch 11/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:28<00:00,  3.44it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0034, Heatmap loss: 0.0003, In-frame loss: 0.0031\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:32<00:00,  3.60it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0005, Heatmap loss: 0.0004, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0005\n","\n","Epoch 12/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:34<00:00,  3.42it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0024, Heatmap loss: 0.0003, In-frame loss: 0.0021\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.53it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0005, Heatmap loss: 0.0005, In-frame loss: 0.0000\n","EarlyStopping counter: 1 out of 7\n","\n","Epoch 13/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:26<00:00,  3.44it/s, loss=0.0002]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0028, Heatmap loss: 0.0003, In-frame loss: 0.0025\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:31<00:00,  3.67it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0004, Heatmap loss: 0.0004, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0004\n","\n","Epoch 14/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:09<00:00,  3.50it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0022, Heatmap loss: 0.0002, In-frame loss: 0.0019\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.49it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0003, Heatmap loss: 0.0003, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0003\n","\n","Epoch 15/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:16<00:00,  3.48it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0023, Heatmap loss: 0.0002, In-frame loss: 0.0021\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:32<00:00,  3.56it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0004, Heatmap loss: 0.0004, In-frame loss: 0.0000\n","EarlyStopping counter: 1 out of 7\n","\n","Epoch 16/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:19<00:00,  3.46it/s, loss=0.0391]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0017, Heatmap loss: 0.0002, In-frame loss: 0.0015\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.51it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0004, Heatmap loss: 0.0004, In-frame loss: 0.0000\n","EarlyStopping counter: 2 out of 7\n","\n","Epoch 17/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:15<00:00,  3.48it/s, loss=0.0010]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0042, Heatmap loss: 0.0002, In-frame loss: 0.0040\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.53it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0004, Heatmap loss: 0.0004, In-frame loss: 0.0000\n","EarlyStopping counter: 3 out of 7\n","\n","Epoch 18/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:23<00:00,  3.45it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0023, Heatmap loss: 0.0002, In-frame loss: 0.0021\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.50it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0004, Heatmap loss: 0.0004, In-frame loss: 0.0000\n","EarlyStopping counter: 4 out of 7\n","\n","Epoch 19/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:36<00:00,  3.41it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0017, Heatmap loss: 0.0001, In-frame loss: 0.0016\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.46it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0003, Heatmap loss: 0.0003, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0003\n","\n","Epoch 20/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:31<00:00,  3.43it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0015, Heatmap loss: 0.0001, In-frame loss: 0.0013\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.48it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0002\n","\n","Epoch 21/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:02<00:00,  3.52it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0013, Heatmap loss: 0.0001, In-frame loss: 0.0012\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:32<00:00,  3.56it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","EarlyStopping counter: 1 out of 7\n","\n","Epoch 22/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:21<00:00,  3.46it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0018, Heatmap loss: 0.0001, In-frame loss: 0.0017\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:31<00:00,  3.66it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","EarlyStopping counter: 2 out of 7\n","\n","Epoch 23/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [17:26<00:00,  3.64it/s, loss=0.0002]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0019, Heatmap loss: 0.0001, In-frame loss: 0.0018\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.47it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","EarlyStopping counter: 3 out of 7\n","\n","Epoch 24/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [17:36<00:00,  3.61it/s, loss=0.0002]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0015, Heatmap loss: 0.0001, In-frame loss: 0.0014\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.46it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0002\n","\n","Epoch 25/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:27<00:00,  3.44it/s, loss=0.0001]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0015, Heatmap loss: 0.0001, In-frame loss: 0.0014\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.49it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","EarlyStopping counter: 1 out of 7\n","\n","Epoch 26/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:34<00:00,  3.42it/s, loss=0.0002]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0015, Heatmap loss: 0.0001, In-frame loss: 0.0014\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.47it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","EarlyStopping counter: 2 out of 7\n","\n","Epoch 27/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:28<00:00,  3.44it/s, loss=0.0006]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0015, Heatmap loss: 0.0001, In-frame loss: 0.0015\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:32<00:00,  3.59it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","EarlyStopping counter: 3 out of 7\n","\n","Epoch 28/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:17<00:00,  3.47it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0014, Heatmap loss: 0.0001, In-frame loss: 0.0013\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:32<00:00,  3.63it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","EarlyStopping counter: 4 out of 7\n","\n","Epoch 29/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:30<00:00,  3.43it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0012, Heatmap loss: 0.0001, In-frame loss: 0.0011\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:32<00:00,  3.61it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","EarlyStopping counter: 5 out of 7\n","\n","Epoch 30/30\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 3809/3809 [18:28<00:00,  3.44it/s, loss=0.0002]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.0013, Heatmap loss: 0.0000, In-frame loss: 0.0012\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 117/117 [00:33<00:00,  3.46it/s, loss=0.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Val loss: 0.0002, Heatmap loss: 0.0002, In-frame loss: 0.0000\n","Saved new best model with validation loss: 0.0002\n","Training history plot saved to ./training_history.png\n","Generating visualizations in ./visualizations...\n","Training complete!\n"]}],"execution_count":11},{"cell_type":"code","source":"def calculate_auc(pred_heatmap, target_heatmap):\n    \"\"\"\n    Calculate Area Under the ROC Curve for heatmap prediction\n\n    Args:\n        pred_heatmap: Predicted heatmap (numpy array)\n        target_heatmap: Ground truth heatmap (numpy array)\n\n    Returns:\n        auc_score: AUC score\n    \"\"\"\n    # Flatten heatmaps\n    pred_flat = pred_heatmap.flatten()\n    target_flat = (target_heatmap > 0.1).flatten().astype(int)  # Binarize target\n\n    # Calculate ROC curve\n    fpr, tpr, _ = roc_curve(target_flat, pred_flat)\n\n    # Calculate AUC\n    auc_score = auc(fpr, tpr)\n\n    return auc_score\n\ndef calculate_distance_error(pred_heatmap, target_heatmap, normalize=True):\n    \"\"\"\n    Calculate distance error between predicted and target gaze points\n\n    Args:\n        pred_heatmap: Predicted heatmap (numpy array)\n        target_heatmap: Ground truth heatmap (numpy array)\n        normalize: Whether to normalize by heatmap dimensions\n\n    Returns:\n        distance: L2 distance between peaks\n    \"\"\"\n    # Find peak positions\n    pred_idx = np.unravel_index(np.argmax(pred_heatmap), pred_heatmap.shape)\n    target_idx = np.unravel_index(np.argmax(target_heatmap), target_heatmap.shape)\n\n    # Calculate L2 distance\n    y1, x1 = pred_idx\n    y2, x2 = target_idx\n\n    if normalize:\n        # Normalize by heatmap dimensions\n        h, w = target_heatmap.shape\n        x1, y1 = x1/w, y1/h\n        x2, y2 = x2/w, y2/h\n\n    distance = np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n\n    return distance\n\ndef calculate_angular_error(pred_vector, target_vector):\n    \"\"\"\n    Calculate angular error between predicted and target gaze vectors\n\n    Args:\n        pred_vector: Predicted gaze vector [x, y]\n        target_vector: Ground truth gaze vector [x, y]\n\n    Returns:\n        angle: Angular error in degrees\n    \"\"\"\n    # Normalize vectors\n    pred_norm = np.linalg.norm(pred_vector)\n    target_norm = np.linalg.norm(target_vector)\n\n    if pred_norm < 1e-7 or target_norm < 1e-7:\n        return 180.0  # Maximum error\n\n    pred_normalized = pred_vector / pred_norm\n    target_normalized = target_vector / target_norm\n\n    # Calculate dot product\n    dot_product = np.clip(np.dot(pred_normalized, target_normalized), -1.0, 1.0)\n\n    # Calculate angle in degrees\n    angle = np.arccos(dot_product) * 180 / np.pi\n\n    return angle\n\ndef calculate_in_frame_accuracy(pred_in_frame, target_in_frame, threshold=0.5):\n    \"\"\"\n    Calculate accuracy of in-frame prediction\n\n    Args:\n        pred_in_frame: Predicted in-frame probability\n        target_in_frame: Ground truth in-frame label\n        threshold: Classification threshold\n\n    Returns:\n        accuracy: Accuracy score\n    \"\"\"\n    pred_binary = (pred_in_frame > threshold).astype(int)\n    target_binary = target_in_frame.astype(int)\n\n    accuracy = (pred_binary == target_binary).mean()\n\n    return accuracy\n\ndef extract_gaze_vector_from_heatmap(heatmap, head_center, heatmap_size, normalize=True):\n    \"\"\"\n    Extract gaze vector from heatmap peak\n\n    Args:\n        heatmap: Gaze heatmap\n        head_center: Head center coordinates (x, y) normalized\n        heatmap_size: Original heatmap dimensions\n        normalize: Whether to normalize the vector\n\n    Returns:\n        gaze_vector: Vector from head center to gaze target\n    \"\"\"\n    # Find peak position\n    peak_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n    peak_y, peak_x = peak_idx\n\n    # Convert to normalized coordinates\n    h, w = heatmap.shape\n    peak_x_norm = peak_x / w\n    peak_y_norm = peak_y / h\n\n    # Calculate vector\n    gaze_vector = np.array([peak_x_norm - head_center[0], peak_y_norm - head_center[1]])\n\n    # Normalize if requested\n    if normalize and np.linalg.norm(gaze_vector) > 0:\n        gaze_vector = gaze_vector / np.linalg.norm(gaze_vector)\n\n    return gaze_vector\n\ndef visualize_prediction(img, head_bbox, pred_heatmap, target_heatmap,\n                        pred_in_frame, target_in_frame, save_path=None):\n    \"\"\"\n    Visualize model prediction versus ground truth\n\n    Args:\n        img: Original image (RGB)\n        head_bbox: Head bounding box [x1, y1, x2, y2]\n        pred_heatmap: Predicted heatmap\n        target_heatmap: Ground truth heatmap\n        pred_in_frame: Predicted in-frame probability\n        target_in_frame: Ground truth in-frame label\n        save_path: Path to save visualization\n    \"\"\"\n    # Create figure\n    plt.figure(figsize=(15, 10))\n\n    # Original image with head box\n    plt.subplot(2, 3, 1)\n    plt.imshow(img)\n    x1, y1, x2, y2 = head_bbox\n    plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1,\n                                     fill=False, edgecolor='green', linewidth=2))\n    plt.title(f\"Head (Target In-frame: {bool(target_in_frame)})\")\n    plt.axis('off')\n\n    # Predicted heatmap\n    plt.subplot(2, 3, 2)\n    plt.imshow(pred_heatmap, cmap='jet')\n    plt.title(f\"Predicted Heatmap (P={pred_in_frame:.2f})\")\n    plt.axis('off')\n\n    # Ground truth heatmap\n    plt.subplot(2, 3, 3)\n    plt.imshow(target_heatmap, cmap='jet')\n    plt.title(\"Ground Truth Heatmap\")\n    plt.axis('off')\n\n    # Image with prediction overlay\n    plt.subplot(2, 3, 4)\n    plt.imshow(img)\n    plt.imshow(pred_heatmap, cmap='jet', alpha=0.5)\n    plt.title(\"Predicted Overlay\")\n    plt.axis('off')\n\n    # Image with ground truth overlay\n    plt.subplot(2, 3, 5)\n    plt.imshow(img)\n    plt.imshow(target_heatmap, cmap='jet', alpha=0.5)\n    plt.title(\"Ground Truth Overlay\")\n    plt.axis('off')\n\n    # Error heatmap\n    plt.subplot(2, 3, 6)\n    error_map = np.abs(pred_heatmap - target_heatmap)\n    plt.imshow(error_map, cmap='hot')\n    plt.title(\"Prediction Error\")\n    plt.axis('off')\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()\n\ndef validate_model(model, dataset, device, batch_size=8, num_vis=10, vis_dir=None):\n    \"\"\"\n    Validate model performance on dataset\n\n    Args:\n        model: Trained model\n        dataset: Validation dataset\n        device: Device to run model on\n        batch_size: Batch size for evaluation\n        num_vis: Number of visualizations to generate\n        vis_dir: Directory to save visualizations\n\n    Returns:\n        metrics: Dictionary of evaluation metrics\n    \"\"\"\n    # Create data loader\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    # Create directory for visualizations\n    if vis_dir:\n        os.makedirs(vis_dir, exist_ok=True)\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize metrics\n    all_auc = []\n    all_distance = []\n    all_angular = []\n    all_in_frame_acc = []\n\n    # Initialize lists for confusion matrix\n    all_pred_in_frame = []\n    all_target_in_frame = []\n\n    # Generate random indices for visualization\n    if len(dataset) > 0 and num_vis > 0:\n        vis_indices = np.random.choice(len(dataset), min(num_vis, len(dataset)), replace=False)\n    else:\n        vis_indices = []\n\n    # Process all samples\n    with torch.no_grad():\n        # Process batched samples\n        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Validating\")):\n            # Unpack batch\n            scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, object_masks, metadata = batch\n\n            # Move tensors to device\n            scene_img = scene_img.to(device)\n            head_img = head_img.to(device)\n            head_pos = head_pos.to(device)\n            object_masks = object_masks.to(device)\n\n            # Forward pass\n            pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos, object_masks)\n\n            # Move predictions to CPU for evaluation\n            pred_heatmap = pred_heatmap.squeeze(1).cpu().numpy()\n            pred_in_frame_prob = torch.sigmoid(pred_in_frame).squeeze().cpu().numpy()\n\n            # Convert targets to numpy\n            target_heatmap_np = target_heatmap.cpu().numpy()\n            target_in_frame_np = target_in_frame.squeeze().cpu().numpy()\n\n            # Evaluate each sample in batch\n            for i in range(len(scene_img)):\n                # Only evaluate in-frame samples for gaze metrics\n                if target_in_frame_np[i] > 0.5:\n                    # Calculate AUC\n                    auc_score = calculate_auc(pred_heatmap[i], target_heatmap_np[i])\n                    all_auc.append(auc_score)\n\n                    # Calculate distance error\n                    dist_error = calculate_distance_error(pred_heatmap[i], target_heatmap_np[i])\n                    all_distance.append(dist_error)\n\n                    # Calculate angular error using vectors\n                    pred_vector = target_vector[i].cpu().numpy()  # Use target vector for now\n                    target_vec = target_vector[i].cpu().numpy()\n                    angular_error = calculate_angular_error(pred_vector, target_vec)\n                    all_angular.append(angular_error)\n\n                # Record in-frame prediction for all samples\n                all_pred_in_frame.append(pred_in_frame_prob[i])\n                all_target_in_frame.append(target_in_frame_np[i])\n\n        # Create individual visualizations for selected samples\n        if vis_dir:\n            # Reset normalization parameters for visualization\n            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n            for vis_idx, idx in enumerate(tqdm(vis_indices, desc=\"Generating visualizations\")):\n                # Get sample\n                sample = dataset[idx]\n                scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, _, object_masks, metadata = sample\n\n\n                # Prepare inputs for model\n                scene_img_batch = scene_img.unsqueeze(0).to(device)\n                head_img_batch = head_img.unsqueeze(0).to(device)\n                head_pos_batch = head_pos.unsqueeze(0).to(device)\n                object_masks_batch = object_masks.unsqueeze(0).to(device)  # Add this line\n\n                # Forward pass\n                pred_heatmap, pred_in_frame = model(scene_img_batch, head_img_batch, head_pos_batch, object_masks_batch)\n\n                # Move predictions to CPU for visualization\n                pred_heatmap_np = pred_heatmap.squeeze().cpu().numpy()\n                pred_in_frame_prob = torch.sigmoid(pred_in_frame).item()\n\n                # Denormalize image for visualization\n                img_vis = scene_img.clone()\n                img_vis = img_vis * std + mean\n                img_vis = img_vis.permute(1, 2, 0).numpy()\n                img_vis = np.clip(img_vis, 0, 1)\n\n                # Get head bbox for visualization\n                x1, y1, x2, y2 = metadata['head_bbox']\n\n                # Scale for visualization\n                h, w = img_vis.shape[:2]\n                orig_w, orig_h = metadata['original_size']\n                scale_x, scale_y = w/orig_w, h/orig_h\n\n                # Scale bbox\n                x1 = x1 * scale_x\n                y1 = y1 * scale_y\n                x2 = x2 * scale_x\n                y2 = y2 * scale_y\n\n                # Create visualization\n                vis_path = os.path.join(vis_dir, f\"validation_{vis_idx}.png\")\n                visualize_prediction(\n                    img_vis, [x1, y1, x2, y2],\n                    pred_heatmap_np, target_heatmap.numpy(),\n                    pred_in_frame_prob, target_in_frame.item(),\n                    vis_path\n                )\n\n    # Calculate in-frame accuracy\n    in_frame_accuracy = calculate_in_frame_accuracy(\n        np.array(all_pred_in_frame),\n        np.array(all_target_in_frame)\n    )\n\n    # Calculate metrics\n    metrics = {\n        'auc_mean': np.mean(all_auc) if all_auc else np.nan,\n        'auc_std': np.std(all_auc) if all_auc else np.nan,\n        'distance_mean': np.mean(all_distance) if all_distance else np.nan,\n        'distance_std': np.std(all_distance) if all_distance else np.nan,\n        'angular_mean': np.mean(all_angular) if all_angular else np.nan,\n        'angular_std': np.std(all_angular) if all_angular else np.nan,\n        'in_frame_accuracy': in_frame_accuracy,\n        'num_evaluated': len(all_auc)\n    }\n\n    return metrics\n\ndef create_attention_heatmap(model, dataset, device, output_path, frame_indices=None, num_frames=10):\n    \"\"\"\n    Create an attention heatmap visualization for entire frames\n\n    Args:\n        model: Trained model\n        dataset: Dataset to visualize\n        device: Device to run model on\n        output_path: Path to save visualization video\n        frame_indices: Specific frame indices to visualize (optional)\n        num_frames: Number of frames to visualize (if frame_indices not provided)\n    \"\"\"\n    # Get unique frame IDs\n    all_frame_ids = []\n    for idx in range(len(dataset)):\n        sample = dataset[idx]\n        metadata = sample[8]  # Metadata is the 9th element\n        frame_id = metadata['frame_id']\n        if frame_id not in all_frame_ids:\n            all_frame_ids.append(frame_id)\n\n    # Select frames to visualize\n    if frame_indices is None:\n        if len(all_frame_ids) <= num_frames:\n            frame_indices = all_frame_ids\n        else:\n            frame_indices = sorted(np.random.choice(all_frame_ids, num_frames, replace=False))\n\n    # Create temporary directory for frames\n    temp_dir = \"temp_attention_frames\"\n    os.makedirs(temp_dir, exist_ok=True)\n\n    # Process each selected frame\n    for i, frame_id in enumerate(tqdm(frame_indices, desc=\"Creating attention heatmaps\")):\n        # Find all samples with this frame ID\n        frame_samples = []\n        for idx in range(len(dataset)):\n            sample = dataset[idx]\n            metadata = sample[8]\n            if metadata['frame_id'] == frame_id:\n                frame_samples.append(idx)\n\n        if not frame_samples:\n            continue\n\n        # Load the first sample for frame information\n        first_sample = dataset[frame_samples[0]]\n        scene_img, _, _, _, _, _, _, object_masks, metadata = first_sample  # Unpack correctly\n        # Get image size\n        img_size = scene_img.shape[1:3]\n\n        # Denormalize image for visualization\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n        img_vis = scene_img.clone()\n        img_vis = img_vis * std + mean\n        img_vis = img_vis.permute(1, 2, 0).numpy()\n        img_vis = np.clip(img_vis, 0, 1)\n\n        # Initialize combined heatmap\n        combined_heatmap = np.zeros(img_size)\n\n        # Process each person in the frame\n        with torch.no_grad():  # Add no_grad context to prevent tracking gradients\n            for sample_idx in frame_samples:\n                sample = dataset[sample_idx]\n                scene_img, head_img, head_pos, _, target_in_frame, _, _, object_masks, metadata = sample  # Unpack correctly\n                # Only process in-frame samples\n                if target_in_frame.item() < 0.5:\n                    continue\n\n                # Prepare inputs for model\n                scene_img_batch = scene_img.unsqueeze(0).to(device)\n                head_img_batch = head_img.unsqueeze(0).to(device)\n                head_pos_batch = head_pos.unsqueeze(0).to(device)\n                object_masks_batch = object_masks.unsqueeze(0).to(device)  # Add this line\n\n                # Forward pass\n                pred_heatmap, _ = model(scene_img_batch, head_img_batch, head_pos_batch, object_masks_batch)\n\n                # Add to combined heatmap\n                pred_heatmap_np = pred_heatmap.squeeze().cpu().numpy()  # This is safe now with no_grad\n\n                # Resize to match image size\n                pred_heatmap_resized = cv2.resize(pred_heatmap_np, (img_size[1], img_size[0]))\n\n                # Add to combined heatmap\n                combined_heatmap += pred_heatmap_resized\n\n        # Normalize combined heatmap\n        if np.max(combined_heatmap) > 0:\n            combined_heatmap = combined_heatmap / np.max(combined_heatmap)\n\n        # Create visualization\n        plt.figure(figsize=(10, 8))\n        plt.imshow(img_vis)\n        plt.imshow(combined_heatmap, cmap='jet', alpha=0.5)\n        plt.title(f\"Frame {frame_id} - Combined Attention\")\n        plt.axis('off')\n\n        # Save frame\n        frame_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n        plt.savefig(frame_path)\n        plt.close()\n\n    # Create video\n    frame_paths = sorted([os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('.png')])\n\n    if not frame_paths:\n        print(\"No frames generated!\")\n        return\n\n    # Get first frame to determine dimensions\n    first_frame = cv2.imread(frame_paths[0])\n    height, width, _ = first_frame.shape\n\n    # Create video writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    video_writer = cv2.VideoWriter(output_path, fourcc, 2, (width, height))\n\n    # Add frames to video\n    for frame_path in frame_paths:\n        frame = cv2.imread(frame_path)\n        video_writer.write(frame)\n\n    # Release video writer\n    video_writer.release()\n\n    # Clean up temp files\n    for frame_path in frame_paths:\n        os.remove(frame_path)\n    os.rmdir(temp_dir)\n\n    print(f\"Attention heatmap video saved to {output_path}\")\ndef main():\n    # Paths\n    model_path = f\"{output_dir}/gescam_output/best_model.pt\"\n    xml_path = f\"{base_dir}/test_subset/task_classroom_11_video-01_final/annotations.xml\"\n    image_folder = f\"{base_dir}/test_subset/task_classroom_11_video-01_final/images\"\n    output = f\"{output_dir}/validation_results\"\n\n    # Create output directory\n    os.makedirs(output, exist_ok=True)\n\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Load model\n    print(\"Loading model...\")\n    model = MSGESCAMModel(pretrained=False, output_size=64)\n\n    # Load checkpoint\n    try:\n        checkpoint = torch.load(model_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        print(\"Initializing with random weights\")\n\n    model = model.to(device)\n\n    # Load dataset\n    print(\"Loading dataset...\")\n    transform = get_transforms(augment=False)\n\n    dataset = GESCAMCustomDataset(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        transform=transform\n    )\n\n    # Split dataset\n    val_size = min(int(0.2 * len(dataset)), 500)  # Cap at 500 for validation\n    generator = torch.Generator().manual_seed(42)\n    _, val_dataset = random_split(dataset, [len(dataset) - val_size, val_size],\n                                 generator=generator)\n\n    print(f\"Validation dataset size: {len(val_dataset)}\")\n\n    # Validate model\n    print(\"Validating model...\")\n    metrics = validate_model(\n        model=model,\n        dataset=val_dataset,\n        device=device,\n        batch_size=8,\n        num_vis=20,\n        vis_dir=os.path.join(output, \"visualizations\")\n    )\n\n    # Print metrics\n    print(\"\\nModel Validation Metrics:\")\n    print(\"-\" * 30)\n    print(f\"AUC: {metrics['auc_mean']:.4f} ± {metrics['auc_std']:.4f}\")\n    print(f\"Distance Error: {metrics['distance_mean']:.4f} ± {metrics['distance_std']:.4f}\")\n    print(f\"Angular Error: {metrics['angular_mean']:.2f}° ± {metrics['angular_std']:.2f}°\")\n    print(f\"In-frame Accuracy: {metrics['in_frame_accuracy']:.4f}\")\n    print(f\"Number of evaluated samples: {metrics['num_evaluated']}\")\n\n    # Save metrics\n    metrics_path = os.path.join(output, \"metrics.txt\")\n    with open(metrics_path, 'w') as f:\n        for key, value in metrics.items():\n            f.write(f\"{key}: {value}\\n\")\n\n    # Create attention heatmap video\n    print(\"Creating attention heatmap video...\")\n    heatmap_video_path = os.path.join(output, \"attention_heatmap.mp4\")\n    create_attention_heatmap(\n        model=model,\n        dataset=val_dataset,\n        device=device,\n        output_path=heatmap_video_path,\n        num_frames=20\n    )\n\n    print(f\"Validation complete. Results saved to {output}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAf4yZDXeLLE","outputId":"0da27431-793d-4d34-b31b-3e53a2b2bd48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loading model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Error loading model: [Errno 2] No such file or directory: './gescam_output/best_model.pt'\n","Initializing with random weights\n","Loading dataset...\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset/task_classroom_11_video-01_final/annotations.xml\n","Root tag: annotations, with 601 child elements\n"]},{"output_type":"stream","name":"stderr","text":["Parsing frames: 100%|██████████| 599/599 [00:00<00:00, 8639.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 69 boxes and 13 polylines in first frame\n","Sample box labels: ['Mug', 'book', 'book', 'table lamp', 'table lamp']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n","Successfully parsed 599 frames\n","Found 599 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 599 frames with person boxes, 599 frames with sight lines\n","Created dataset with 7787 samples\n","Validation dataset size: 500\n","Validating model...\n"]},{"output_type":"stream","name":"stderr","text":["Validating: 100%|██████████| 63/63 [00:35<00:00,  1.79it/s]\n","Generating visualizations: 100%|██████████| 20/20 [00:12<00:00,  1.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Model Validation Metrics:\n","------------------------------\n","AUC: 0.5035 ± 0.0375\n","Distance Error: 0.4122 ± 0.1247\n","Angular Error: 0.01° ± 0.01°\n","In-frame Accuracy: 1.0000\n","Number of evaluated samples: 500\n","Creating attention heatmap video...\n"]},{"output_type":"stream","name":"stderr","text":["Creating attention heatmaps: 100%|██████████| 20/20 [09:59<00:00, 29.99s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Attention heatmap video saved to ./validation_results/attention_heatmap.mp4\n","Validation complete. Results saved to ./validation_results\n"]}],"execution_count":12},{"cell_type":"markdown","source":"## INDIVIDUAL WEIGHT ATTENTION\n","metadata":{"id":"MEkV9WsN5bNT"}},{"cell_type":"code","source":"def calculate_individual_attention(pred_heatmap, object_masks, context=\"lecture\"):\n    \"\"\"\n    Calculate individual attention score based on gaze heatmap and object masks\n\n    Args:\n        pred_heatmap: Predicted gaze heatmap (numpy array)\n        object_masks: Dictionary or tensor of object masks with class indices\n        context: Teaching context (lecture, group_work, individual_work)\n\n    Returns:\n        attention_score: Overall attention score (0-1)\n        object_attention: Dictionary of attention scores per object category\n    \"\"\"\n    # Define object importance based on educational context\n    if context == \"lecture\":\n        # During lectures, looking at teacher/board is most important\n        importance_weights = {\n            0: 0.7,  # person/teacher\n            1: 0.9,  # blackboard/whiteboard\n            2: 0.6,  # books/notebooks\n            3: 0.5,  # monitors/screens\n            4: 0.1,  # phones (usually distractions)\n            5: 0.3,  # desks/tables\n            6: 0.1,  # water dispenser (distraction)\n            7: 0.1,  # mugs (distraction)\n            8: 0.1,  # lamps (distraction)\n            9: 0.1,  # other objects\n            10: 0.1  # other objects\n        }\n    elif context == \"group_work\":\n        # During group work, looking at peers and materials is valuable\n        importance_weights = {\n            0: 0.8,  # person/peers\n            1: 0.5,  # blackboard/whiteboard\n            2: 0.7,  # books/notebooks\n            3: 0.6,  # monitors/screens\n            4: 0.1,  # phones (usually distractions)\n            5: 0.4,  # desks/tables\n            6: 0.1,  # water dispenser (distraction)\n            7: 0.1,  # mugs (distraction)\n            8: 0.1,  # lamps (distraction)\n            9: 0.1,  # other objects\n            10: 0.1  # other objects\n        }\n    else:  # individual_work\n        # During individual work, focus on learning materials\n        importance_weights = {\n            0: 0.3,  # person/teacher\n            1: 0.4,  # blackboard/whiteboard\n            2: 0.9,  # books/notebooks\n            3: 0.8,  # monitors/screens\n            4: 0.1,  # phones (usually distractions)\n            5: 0.5,  # desks/tables\n            6: 0.1,  # water dispenser (distraction)\n            7: 0.1,  # mugs (distraction)\n            8: 0.1,  # lamps (distraction)\n            9: 0.1,  # other objects\n            10: 0.1  # other objects\n        }\n\n    # Normalize the heatmap\n    heatmap_normalized = pred_heatmap / pred_heatmap.sum() if pred_heatmap.sum() > 0 else pred_heatmap\n\n    # Calculate attention overlap with each object class\n    object_attention = {}\n    total_weighted_attention = 0\n    max_overlap = 0\n    max_object = None\n\n    # Process each object category\n    for class_idx in range(object_masks.shape[0]):\n        # Ensure masks are same size as heatmap\n        mask = object_masks[class_idx]\n        if mask.shape != heatmap_normalized.shape:\n            mask = cv2.resize(mask, (heatmap_normalized.shape[1], heatmap_normalized.shape[0]))\n\n        # Calculate overlap (element-wise multiplication)\n        overlap = np.sum(heatmap_normalized * mask)\n\n        # Scale by importance weight\n        weighted_overlap = overlap * importance_weights.get(class_idx, 0.1)\n\n        # Track which object has most attention\n        if overlap > max_overlap:\n            max_overlap = overlap\n            max_object = class_idx\n\n        # Store results\n        object_attention[class_idx] = {\n            'raw_overlap': float(overlap),\n            'weighted_score': float(weighted_overlap)\n        }\n\n        total_weighted_attention += weighted_overlap\n\n    # Calculate final score (0-1)\n    # Normalize to 0-1 range\n    attention_score = min(1.0, total_weighted_attention)\n\n    # Add information about primary object of attention\n    object_attention['primary_object'] = max_object\n    object_attention['primary_overlap'] = float(max_overlap)\n\n    return attention_score, object_attention","metadata":{"id":"xjUKnf47j9bp"},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## ATTENTION PER FRAME","metadata":{"id":"134xiN6ZkAig"}},{"cell_type":"code","source":"def analyze_frame_attention(model, frame_data, device, context=\"lecture\"):\n    \"\"\"\n    Analyze attention for all people in a single frame\n\n    Args:\n        model: Trained attention model\n        frame_data: List of samples for people in the frame\n        device: Device to run model on\n        context: Teaching context\n\n    Returns:\n        frame_attention: Dictionary with individual and aggregated attention data\n    \"\"\"\n    individual_scores = []\n    individual_heatmaps = []\n    primary_targets = {}\n\n    # Dict to track how many people look at each object type\n    objects_attention_count = {i: 0 for i in range(11)}  # 11 object categories\n\n    with torch.no_grad():\n        for person_idx, sample in enumerate(frame_data):\n            # Unpack sample\n            scene_img, head_img, head_pos, _, target_in_frame, _, _, object_masks, metadata = sample\n\n            # Skip if no target in frame\n            if not target_in_frame.item():\n                continue\n\n            # Prepare inputs for model (add batch dimension)\n            scene_img = scene_img.unsqueeze(0).to(device)\n            head_img = head_img.unsqueeze(0).to(device)\n            head_pos = head_pos.unsqueeze(0).to(device)\n            object_masks = object_masks.unsqueeze(0).to(device)\n\n            # Forward pass\n            pred_heatmap, _ = model(scene_img, head_img, head_pos, object_masks)\n\n            # Convert to numpy for processing\n            pred_heatmap_np = pred_heatmap.squeeze().cpu().numpy()\n            object_masks_np = object_masks.squeeze().cpu().numpy()\n\n            # Calculate attention score\n            attention_score, object_attention = calculate_individual_attention(\n                pred_heatmap_np, object_masks_np, context)\n\n            # Store individual results\n            individual_scores.append({\n                'person_idx': person_idx,\n                'attention_score': attention_score,\n                'object_attention': object_attention,\n                'primary_object': object_attention['primary_object'],\n                'head_bbox': metadata['head_bbox']\n            })\n\n            individual_heatmaps.append(pred_heatmap_np)\n\n            # Count primary targets\n            primary_obj = object_attention['primary_object']\n            if primary_obj is not None:\n                objects_attention_count[primary_obj] += 1\n                primary_targets[primary_obj] = primary_targets.get(primary_obj, 0) + 1\n\n    # Calculate frame-level statistics\n    if individual_scores:\n        mean_attention = sum(item['attention_score'] for item in individual_scores) / len(individual_scores)\n\n        # Find most commonly attended object\n        most_attended_object = max(objects_attention_count.items(), key=lambda x: x[1]) if objects_attention_count else None\n\n        # Calculate percentage of people attending to each object type\n        total_people = len(individual_scores)\n        attention_distribution = {obj: count/total_people for obj, count in objects_attention_count.items() if count > 0}\n\n        # Create combined attention heatmap\n        if individual_heatmaps:\n            combined_heatmap = sum(individual_heatmaps)\n            # Normalize\n            if combined_heatmap.max() > 0:\n                combined_heatmap = combined_heatmap / combined_heatmap.max()\n        else:\n            combined_heatmap = None\n    else:\n        mean_attention = 0\n        most_attended_object = None\n        attention_distribution = {}\n        combined_heatmap = None\n\n    # Create result dictionary\n    frame_attention = {\n        'individual_scores': individual_scores,\n        'frame_stats': {\n            'mean_attention': mean_attention,\n            'most_attended_object': most_attended_object,\n            'attention_distribution': attention_distribution,\n            'total_people': len(individual_scores)\n        },\n        'combined_heatmap': combined_heatmap\n    }\n\n    return frame_attention","metadata":{"id":"g2drJAgIkCKh"},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## TRACKING ATTENTION OVER MULTIPLE FRAMES","metadata":{"id":"yD9ybLdVkH8a"}},{"cell_type":"code","source":"def track_attention_over_time(model, dataset, device, frame_ids=None, num_frames=10, context=\"lecture\"):\n    \"\"\"\n    Track attention metrics over a sequence of frames\n\n    Args:\n        model: Trained attention model\n        dataset: Dataset containing frames\n        device: Device to run model on\n        frame_ids: List of specific frame IDs to analyze (optional)\n        num_frames: Number of frames to analyze if frame_ids not provided\n        context: Teaching context\n\n    Returns:\n        temporal_attention: Dictionary with time-series attention data\n    \"\"\"\n    # Get unique frame IDs if not provided\n    if frame_ids is None:\n        all_frame_ids = []\n        for idx in range(len(dataset)):\n            metadata = dataset[idx][8]\n            frame_id = metadata['frame_id']\n            if frame_id not in all_frame_ids:\n                all_frame_ids.append(frame_id)\n\n        # Select frames to analyze\n        if len(all_frame_ids) <= num_frames:\n            frame_ids = sorted(all_frame_ids)\n        else:\n            frame_ids = sorted(np.random.choice(all_frame_ids, num_frames, replace=False))\n\n    # Track attention over time\n    temporal_data = []\n\n    # Process each frame\n    for frame_id in tqdm(frame_ids, desc=\"Analyzing attention over time\"):\n        # Find all samples for this frame\n        frame_samples = []\n        for idx in range(len(dataset)):\n            sample = dataset[idx]\n            metadata = sample[8]\n            if metadata['frame_id'] == frame_id:\n                frame_samples.append(sample)\n\n        if not frame_samples:\n            continue\n\n        # Analyze this frame\n        frame_attention = analyze_frame_attention(model, frame_samples, device, context)\n\n        # Store frame data\n        temporal_data.append({\n            'frame_id': frame_id,\n            'attention_data': frame_attention\n        })\n\n    # Calculate temporal statistics\n    if temporal_data:\n        # Track mean attention over time\n        attention_over_time = [frame['attention_data']['frame_stats']['mean_attention']\n                              for frame in temporal_data]\n\n        # Track most common target over time\n        targets_over_time = [frame['attention_data']['frame_stats']['most_attended_object']\n                            for frame in temporal_data]\n\n        # Calculate attention stability (how consistent attention is)\n        attention_stability = np.std(attention_over_time) if len(attention_over_time) > 1 else 0\n\n        # Detect significant attention shifts\n        attention_shifts = []\n        for i in range(1, len(temporal_data)):\n            prev = temporal_data[i-1]['attention_data']['frame_stats']['most_attended_object']\n            curr = temporal_data[i]['attention_data']['frame_stats']['most_attended_object']\n            if prev != curr:\n                attention_shifts.append({\n                    'frame_id': temporal_data[i]['frame_id'],\n                    'from_object': prev,\n                    'to_object': curr\n                })\n    else:\n        attention_over_time = []\n        targets_over_time = []\n        attention_stability = 0\n        attention_shifts = []\n\n    # Create result\n    temporal_attention = {\n        'frame_data': temporal_data,\n        'temporal_stats': {\n            'attention_over_time': attention_over_time,\n            'targets_over_time': targets_over_time,\n            'attention_stability': float(attention_stability),\n            'attention_shifts': attention_shifts\n        }\n    }\n\n    return temporal_attention","metadata":{"id":"qeav6Mm4kL-b"},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### VISUALIZING ATTENTION","metadata":{"id":"ETuW-oWxkR0D"}},{"cell_type":"code","source":"def visualize_individual_attention(frame_img, attention_data, object_names=None, save_path=None):\n    \"\"\"\n    Visualize individual attention scores in a frame\n\n    Args:\n        frame_img: Original frame image (RGB)\n        attention_data: Attention data from analyze_frame_attention\n        object_names: Dictionary mapping object indices to names\n        save_path: Path to save visualization\n    \"\"\"\n    if object_names is None:\n        object_names = {\n            0: \"Person/Teacher\",\n            1: \"Board\",\n            2: \"Book/Notebook\",\n            3: \"Monitor/Screen\",\n            4: \"Phone\",\n            5: \"Desk/Table\",\n            6: \"Water Dispenser\",\n            7: \"Mug\",\n            8: \"Lamp\",\n            9: \"Other1\",\n            10: \"Other2\"\n        }\n\n    # Create figure\n    plt.figure(figsize=(15, 10))\n\n    # Show frame with scores\n    plt.subplot(1, 2, 1)\n    plt.imshow(frame_img)\n\n    # Add bounding boxes and scores\n    for person_data in attention_data['individual_scores']:\n        x1, y1, x2, y2 = person_data['head_bbox']\n        score = person_data['attention_score']\n\n        # Color based on score (green for high, yellow for medium, red for low)\n        if score > 0.7:\n            color = 'lime'\n        elif score > 0.4:\n            color = 'yellow'\n        else:\n            color = 'red'\n\n        # Add rectangle\n        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n                            fill=False, edgecolor=color, linewidth=2)\n        plt.gca().add_patch(rect)\n\n        # Add score text\n        primary_obj = object_names.get(person_data['primary_object'], \"Unknown\")\n        plt.text(x1, y1-10, f\"Score: {score:.2f}\\nLooking at: {primary_obj}\",\n                color='white', fontsize=8,\n                bbox=dict(facecolor=color, alpha=0.7))\n\n    # Show combined heatmap\n    plt.subplot(1, 2, 2)\n    plt.imshow(frame_img)\n\n    if attention_data['combined_heatmap'] is not None:\n        # Resize heatmap to match image\n        h, w = frame_img.shape[:2]\n        heatmap = cv2.resize(attention_data['combined_heatmap'], (w, h))\n        plt.imshow(heatmap, cmap='jet', alpha=0.6)\n\n    # Add frame stats\n    stats = attention_data['frame_stats']\n    most_obj = object_names.get(stats['most_attended_object'][0], \"None\") if stats['most_attended_object'] else \"None\"\n\n    plt.title(f\"Class Attention | Mean Score: {stats['mean_attention']:.2f} | \" +\n             f\"Most Attended: {most_obj} ({stats['most_attended_object'][1] if stats['most_attended_object'] else 0} people)\")\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()\n\ndef visualize_attention_over_time(temporal_data, object_names=None, save_path=None):\n    \"\"\"\n    Visualize attention metrics over time\n\n    Args:\n        temporal_data: Output from track_attention_over_time\n        object_names: Dictionary mapping object indices to names\n        save_path: Path to save visualization\n    \"\"\"\n    if object_names is None:\n        object_names = {\n            0: \"Person/Teacher\",\n            1: \"Board\",\n            2: \"Book/Notebook\",\n            3: \"Monitor/Screen\",\n            4: \"Phone\",\n            5: \"Desk/Table\",\n            6: \"Water Dispenser\",\n            7: \"Mug\",\n            8: \"Lamp\",\n            9: \"Other1\",\n            10: \"Other2\"\n        }\n\n    # Extract time series data\n    attention_over_time = temporal_data['temporal_stats']['attention_over_time']\n    targets_over_time = temporal_data['temporal_stats']['targets_over_time']\n    frame_ids = [frame['frame_id'] for frame in temporal_data['frame_data']]\n\n    # Create figure\n    plt.figure(figsize=(15, 8))\n\n    # Plot attention scores over time\n    plt.subplot(2, 1, 1)\n    plt.plot(frame_ids, attention_over_time, 'b-o', linewidth=2)\n    plt.xlabel('Frame ID')\n    plt.ylabel('Mean Attention Score')\n    plt.title(f'Attention Score Over Time (Stability: {temporal_data[\"temporal_stats\"][\"attention_stability\"]:.3f})')\n    plt.grid(True)\n\n    # Plot attention targets over time\n    plt.subplot(2, 1, 2)\n\n    # Convert target indices to names\n    target_names = []\n    for target in targets_over_time:\n        if target and target[0] in object_names:\n            target_names.append(object_names[target[0]])\n        else:\n            target_names.append(\"None\")\n\n    # Create colored markers for each unique target\n    unique_targets = set(target_names)\n    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_targets)))\n    target_colors = {target: colors[i] for i, target in enumerate(unique_targets)}\n\n    # Plot points colored by target\n    for i, (frame, target) in enumerate(zip(frame_ids, target_names)):\n        plt.scatter(frame, i % 5, color=target_colors[target], s=100, label=target if i == 0 or target_names[i-1] != target else \"\")\n\n        # Add text label\n        plt.text(frame, i % 5 + 0.3, target, rotation=45, ha='right')\n\n    # Add shift markers\n    for shift in temporal_data['temporal_stats']['attention_shifts']:\n        plt.axvline(x=shift['frame_id'], color='r', linestyle='--', alpha=0.5)\n\n    plt.xlabel('Frame ID')\n    plt.title('Attention Target Shifts Over Time')\n\n    # Remove y-axis as it's just for spacing\n    plt.yticks([])\n\n    # Handle legend (only show unique targets)\n    handles, labels = plt.gca().get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n    plt.legend(by_label.values(), by_label.keys(), loc='upper right')\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()","metadata":{"id":"OBf3oiZ4kTbx"},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## CALLING THEM TOGETHER","metadata":{"id":"sE0whxwlkWs3"}},{"cell_type":"code","source":"# Example usage in your validation/testing code\ndef analyze_attention_in_classroom():\n    # Load model and dataset as before\n\n    # Choose a few frames to analyze\n    selected_frames = [0, 10, 20, 30, 40, 50]  # Example frame IDs\n\n    # Track attention over time\n    temporal_attention = track_attention_over_time(\n        model=model,\n        dataset=dataset,\n        device=device,\n        frame_ids=selected_frames,\n        context=\"lecture\", # Change based on classroom context,\n        num_frames= 300\n    )\n\n    # Visualize results\n    visualize_attention_over_time(\n        temporal_attention,\n        save_path=os.path.join(output_dir, \"attention_over_time.png\")\n    )\n\n    # For demonstration, visualize one specific frame in detail\n    demo_frame_id = selected_frames[0]\n\n    # Find samples for this frame\n    frame_samples = []\n    for idx in range(len(dataset)):\n        sample = dataset[idx]\n        metadata = sample[8]\n        if metadata['frame_id'] == demo_frame_id:\n            frame_samples.append(sample)\n\n    # Analyze this frame\n    frame_attention = analyze_frame_attention(model, frame_samples, device, context=\"lecture\")\n\n    # Load frame image for visualization\n    sample = frame_samples[0]\n    scene_img = sample[0]\n\n    # Denormalize image\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n    img_vis = scene_img.clone()\n    img_vis = img_vis * std + mean\n    img_vis = img_vis.permute(1, 2, 0).numpy()\n    img_vis = np.clip(img_vis, 0, 1)\n\n    # Visualize this frame's attention\n    visualize_individual_attention(\n        img_vis,\n        frame_attention,\n        save_path=os.path.join(output_dir, f\"frame_{demo_frame_id}_attention.png\")\n    )\n\n    return temporal_attention","metadata":{"id":"vqqDE7iSkZDG"},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## TESTING THESE ATTENTIONS","metadata":{"id":"H65Cl0hrnTXP"}},{"cell_type":"code","source":"def test_attention_scoring(model_path, dataset_path, output_dir, num_frames=5):\n    \"\"\"\n    Test the attention scoring functionality independently\n\n    Args:\n        model_path: Path to trained model checkpoint\n        dataset_path: Path to dataset (XML annotation file)\n        output_dir: Directory to save test results\n        num_frames: Number of frames to analyze\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Load model\n    print(\"Loading model...\")\n    model = MSGESCAMModel(pretrained=False, output_size=64)\n\n    try:\n        checkpoint = torch.load(model_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded model checkpoint successfully\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        print(\"Using randomly initialized model for testing\")\n\n    model = model.to(device)\n    model.eval()\n\n    # Load dataset\n    print(\"Loading dataset...\")\n    image_folder = os.path.join(os.path.dirname(dataset_path), \"images\")\n    transform = get_transforms(augment=False)\n\n    dataset = GESCAMCustomDataset(\n        xml_path=dataset_path,\n        image_folder=image_folder,\n        transform=transform\n    )\n\n    print(f\"Dataset loaded with {len(dataset)} samples\")\n\n    # Get unique frame IDs\n    all_frame_ids = []\n    frame_id_to_samples = {}\n\n    print(\"Indexing frames...\")\n    for idx in range(len(dataset)):\n        sample = dataset[idx]\n        metadata = sample[8]  # Metadata is the 9th element\n        frame_id = metadata['frame_id']\n\n        # Store mapping of frame ID to sample indices\n        if frame_id not in frame_id_to_samples:\n            frame_id_to_samples[frame_id] = []\n            all_frame_ids.append(frame_id)\n\n        frame_id_to_samples[frame_id].append(idx)\n\n    # Select random frames\n    if len(all_frame_ids) <= num_frames:\n        selected_frames = all_frame_ids\n    else:\n        selected_frames = sorted(np.random.choice(all_frame_ids, num_frames, replace=False))\n\n    print(f\"Selected {len(selected_frames)} frames for analysis\")\n\n    # Define object class names\n    object_names = {\n        0: \"Person/Teacher\",\n        1: \"Board\",\n        2: \"Book/Notebook\",\n        3: \"Monitor/Screen\",\n        4: \"Phone\",\n        5: \"Desk/Table\",\n        6: \"Water Dispenser\",\n        7: \"Mug\",\n        8: \"Lamp\",\n        9: \"Other1\",\n        10: \"Other2\"\n    }\n\n    # Process each frame\n    results = []\n\n    for frame_idx, frame_id in enumerate(tqdm(selected_frames, desc=\"Analyzing frames\")):\n        # Get all samples for this frame\n        frame_sample_indices = frame_id_to_samples[frame_id]\n        frame_samples = [dataset[idx] for idx in frame_sample_indices]\n\n        # Get one sample to extract the frame image\n        first_sample = frame_samples[0]\n        scene_img = first_sample[0]\n\n        # Denormalize image for visualization\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n        img_vis = scene_img.clone()\n        img_vis = img_vis * std + mean\n        img_vis = img_vis.permute(1, 2, 0).numpy()\n        img_vis = np.clip(img_vis, 0, 1)\n\n        # Analyze frame attention\n        frame_attention = analyze_frame_attention(\n            model=model,\n            frame_data=frame_samples,\n            device=device,\n            context=\"lecture\"  # Default context\n        )\n\n        # Store result\n        results.append({\n            'frame_id': frame_id,\n            'attention_data': frame_attention\n        })\n\n        # Visualize individual attention\n        vis_path = os.path.join(output_dir, f\"frame_{frame_id}_attention.png\")\n        visualize_individual_attention(\n            frame_img=img_vis,\n            attention_data=frame_attention,\n            object_names=object_names,\n            save_path=vis_path\n        )\n\n        print(f\"Frame {frame_id} visualization saved to {vis_path}\")\n\n        # Print summary of this frame\n        print(f\"\\nFrame {frame_id} Analysis Summary:\")\n        print(f\"- Total people: {frame_attention['frame_stats']['total_people']}\")\n        print(f\"- Mean attention score: {frame_attention['frame_stats']['mean_attention']:.2f}\")\n\n        if frame_attention['frame_stats']['most_attended_object']:\n            obj_idx, count = frame_attention['frame_stats']['most_attended_object']\n            print(f\"- Most attended object: {object_names.get(obj_idx, 'Unknown')} ({count} people)\")\n\n        print(\"- Individual scores:\")\n        for person in frame_attention['individual_scores']:\n            primary_obj = object_names.get(person['primary_object'], \"Unknown\")\n            print(f\"  Person {person['person_idx']}: Score={person['attention_score']:.2f}, Looking at: {primary_obj}\")\n\n    # Create temporal analysis if we have multiple frames\n    if len(results) > 1:\n        # Convert results to temporal_data format\n        temporal_data = {\n            'frame_data': results,\n            'temporal_stats': {\n                'attention_over_time': [r['attention_data']['frame_stats']['mean_attention'] for r in results],\n                'targets_over_time': [r['attention_data']['frame_stats']['most_attended_object'] for r in results],\n                'attention_stability': np.std([r['attention_data']['frame_stats']['mean_attention'] for r in results]),\n                'attention_shifts': []\n            }\n        }\n\n        # Detect attention shifts\n        for i in range(1, len(results)):\n            prev = results[i-1]['attention_data']['frame_stats']['most_attended_object']\n            curr = results[i]['attention_data']['frame_stats']['most_attended_object']\n\n            # Check if primary target changed\n            if prev and curr and prev[0] != curr[0]:\n                temporal_data['temporal_stats']['attention_shifts'].append({\n                    'frame_id': results[i]['frame_id'],\n                    'from_object': prev,\n                    'to_object': curr\n                })\n\n        # Visualize temporal data\n        vis_path = os.path.join(output_dir, \"attention_over_time.png\")\n        visualize_attention_over_time(\n            temporal_data=temporal_data,\n            object_names=object_names,\n            save_path=vis_path\n        )\n\n        print(f\"\\nTemporal analysis visualization saved to {vis_path}\")\n        print(f\"Attention stability: {temporal_data['temporal_stats']['attention_stability']:.3f}\")\n        print(f\"Detected {len(temporal_data['temporal_stats']['attention_shifts'])} major attention shifts\")\n\n    print(\"\\nAttention analysis testing complete!\")\n    return results\n\n# Example usage\nif __name__ == \"__main__\":\n    # Set your paths\n    model_path = \"/content/best_model.pt\"\n    dataset_path = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\",\n    test_output_dir = os.path.join(output_dir, \"attention_test_results\")\n    xml_path = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video02(0-300)/annotations.xml\"\n    dataset_path = xml_path\n\n\n    # Run the test\n    test_results = test_attention_scoring(\n        model_path=model_path,\n        dataset_path=dataset_path,\n        output_dir=test_output_dir,\n        num_frames=300\n    )","metadata":{"id":"AmAUaG-bnVe6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f46b519b-855d-4a0c-eb1c-41030a11dd30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loading model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Loaded model checkpoint successfully\n","Loading dataset...\n","Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(0-300)/annotations.xml\n","Root tag: annotations, with 308 child elements\n"]},{"output_type":"stream","name":"stderr","text":["Parsing frames: 100%|██████████| 306/306 [00:00<00:00, 13002.56it/s]"]},{"output_type":"stream","name":"stdout","text":["Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n","Found 52 boxes and 14 polylines in first frame\n","Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n","Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n","Successfully parsed 306 frames\n","Found 306 images with extractable frame IDs\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Statistics: 306 frames with person boxes, 306 frames with sight lines\n","Created dataset with 4284 samples\n","Dataset loaded with 4284 samples\n","Indexing frames...\n","Selected 300 frames for analysis\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   0%|          | 0/300 [00:00<?, ?it/s]<ipython-input-16-d224994075e3>:74: UserWarning: Tight layout not applied. tight_layout cannot make Axes width small enough to accommodate all Axes decorations\n","  plt.tight_layout()\n","Analyzing frames:   0%|          | 1/300 [00:01<08:03,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 0 visualization saved to ./attention_test_results/frame_0_attention.png\n","\n","Frame 0 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.26, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   1%|          | 2/300 [00:03<07:56,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 1 visualization saved to ./attention_test_results/frame_1_attention.png\n","\n","Frame 1 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.26, Looking at: Monitor/Screen\n","  Person 10: Score=0.14, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   1%|          | 3/300 [00:04<07:48,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 2 visualization saved to ./attention_test_results/frame_2_attention.png\n","\n","Frame 2 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.24, Looking at: Monitor/Screen\n","  Person 10: Score=0.14, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   1%|▏         | 4/300 [00:06<07:41,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 3 visualization saved to ./attention_test_results/frame_3_attention.png\n","\n","Frame 3 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   2%|▏         | 5/300 [00:07<07:38,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 4 visualization saved to ./attention_test_results/frame_4_attention.png\n","\n","Frame 4 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.33, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.36, Looking at: Monitor/Screen\n","  Person 8: Score=0.34, Looking at: Monitor/Screen\n","  Person 9: Score=0.20, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   2%|▏         | 6/300 [00:09<07:38,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 5 visualization saved to ./attention_test_results/frame_5_attention.png\n","\n","Frame 5 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.33, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.36, Looking at: Monitor/Screen\n","  Person 8: Score=0.34, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   2%|▏         | 7/300 [00:10<07:39,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 6 visualization saved to ./attention_test_results/frame_6_attention.png\n","\n","Frame 6 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.33, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   3%|▎         | 8/300 [00:12<08:01,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 7 visualization saved to ./attention_test_results/frame_7_attention.png\n","\n","Frame 7 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.36, Looking at: Monitor/Screen\n","  Person 8: Score=0.33, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   3%|▎         | 9/300 [00:14<07:48,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 8 visualization saved to ./attention_test_results/frame_8_attention.png\n","\n","Frame 8 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   3%|▎         | 10/300 [00:15<07:39,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 9 visualization saved to ./attention_test_results/frame_9_attention.png\n","\n","Frame 9 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   4%|▎         | 11/300 [00:17<07:36,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 10 visualization saved to ./attention_test_results/frame_10_attention.png\n","\n","Frame 10 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.15, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   4%|▍         | 12/300 [00:18<07:31,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 11 visualization saved to ./attention_test_results/frame_11_attention.png\n","\n","Frame 11 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   4%|▍         | 13/300 [00:20<07:25,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 12 visualization saved to ./attention_test_results/frame_12_attention.png\n","\n","Frame 12 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.36, Looking at: Monitor/Screen\n","  Person 8: Score=0.33, Looking at: Monitor/Screen\n","  Person 9: Score=0.10, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   5%|▍         | 14/300 [00:22<07:27,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 13 visualization saved to ./attention_test_results/frame_13_attention.png\n","\n","Frame 13 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   5%|▌         | 15/300 [00:23<07:31,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 14 visualization saved to ./attention_test_results/frame_14_attention.png\n","\n","Frame 14 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.06, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   5%|▌         | 16/300 [00:25<07:32,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 15 visualization saved to ./attention_test_results/frame_15_attention.png\n","\n","Frame 15 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.06, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   6%|▌         | 17/300 [00:26<07:29,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 16 visualization saved to ./attention_test_results/frame_16_attention.png\n","\n","Frame 16 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.33, Looking at: Monitor/Screen\n","  Person 9: Score=0.06, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   6%|▌         | 18/300 [00:28<07:24,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 17 visualization saved to ./attention_test_results/frame_17_attention.png\n","\n","Frame 17 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.25, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   6%|▋         | 19/300 [00:29<07:19,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 18 visualization saved to ./attention_test_results/frame_18_attention.png\n","\n","Frame 18 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.25, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   7%|▋         | 20/300 [00:31<07:16,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 19 visualization saved to ./attention_test_results/frame_19_attention.png\n","\n","Frame 19 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.25, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   7%|▋         | 21/300 [00:33<07:18,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 20 visualization saved to ./attention_test_results/frame_20_attention.png\n","\n","Frame 20 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.25, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.31, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   7%|▋         | 22/300 [00:34<07:19,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 21 visualization saved to ./attention_test_results/frame_21_attention.png\n","\n","Frame 21 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.26, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.20, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   8%|▊         | 23/300 [00:36<07:18,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 22 visualization saved to ./attention_test_results/frame_22_attention.png\n","\n","Frame 22 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.26, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.12, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.09, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   8%|▊         | 24/300 [00:37<07:14,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 23 visualization saved to ./attention_test_results/frame_23_attention.png\n","\n","Frame 23 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.26, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.09, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   8%|▊         | 25/300 [00:39<07:39,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 24 visualization saved to ./attention_test_results/frame_24_attention.png\n","\n","Frame 24 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.25, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   9%|▊         | 26/300 [00:41<07:30,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 25 visualization saved to ./attention_test_results/frame_25_attention.png\n","\n","Frame 25 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.25, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   9%|▉         | 27/300 [00:42<07:27,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 26 visualization saved to ./attention_test_results/frame_26_attention.png\n","\n","Frame 26 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.01, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:   9%|▉         | 28/300 [00:44<07:24,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 27 visualization saved to ./attention_test_results/frame_27_attention.png\n","\n","Frame 27 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.23, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  10%|▉         | 29/300 [00:46<07:24,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 28 visualization saved to ./attention_test_results/frame_28_attention.png\n","\n","Frame 28 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  10%|█         | 30/300 [00:47<07:18,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 29 visualization saved to ./attention_test_results/frame_29_attention.png\n","\n","Frame 29 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  10%|█         | 31/300 [00:49<07:14,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 30 visualization saved to ./attention_test_results/frame_30_attention.png\n","\n","Frame 30 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.23, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  11%|█         | 32/300 [00:51<07:13,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 31 visualization saved to ./attention_test_results/frame_31_attention.png\n","\n","Frame 31 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  11%|█         | 33/300 [00:52<07:10,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 32 visualization saved to ./attention_test_results/frame_32_attention.png\n","\n","Frame 32 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  11%|█▏        | 34/300 [00:54<07:09,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 33 visualization saved to ./attention_test_results/frame_33_attention.png\n","\n","Frame 33 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.23, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  12%|█▏        | 35/300 [00:55<07:11,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 34 visualization saved to ./attention_test_results/frame_34_attention.png\n","\n","Frame 34 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.23, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  12%|█▏        | 36/300 [00:57<07:11,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 35 visualization saved to ./attention_test_results/frame_35_attention.png\n","\n","Frame 35 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  12%|█▏        | 37/300 [00:59<07:07,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 36 visualization saved to ./attention_test_results/frame_36_attention.png\n","\n","Frame 36 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.31, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  13%|█▎        | 38/300 [01:00<07:03,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 37 visualization saved to ./attention_test_results/frame_37_attention.png\n","\n","Frame 37 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.31, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  13%|█▎        | 39/300 [01:02<07:01,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 38 visualization saved to ./attention_test_results/frame_38_attention.png\n","\n","Frame 38 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.18, Looking at: Monitor/Screen\n","  Person 7: Score=0.31, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  13%|█▎        | 40/300 [01:03<06:52,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 39 visualization saved to ./attention_test_results/frame_39_attention.png\n","\n","Frame 39 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.31, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  14%|█▎        | 41/300 [01:05<06:43,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 40 visualization saved to ./attention_test_results/frame_40_attention.png\n","\n","Frame 40 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.31, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  14%|█▍        | 42/300 [01:06<06:42,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 41 visualization saved to ./attention_test_results/frame_41_attention.png\n","\n","Frame 41 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.31, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  14%|█▍        | 43/300 [01:08<06:39,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 42 visualization saved to ./attention_test_results/frame_42_attention.png\n","\n","Frame 42 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.30, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  15%|█▍        | 44/300 [01:10<06:34,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 43 visualization saved to ./attention_test_results/frame_43_attention.png\n","\n","Frame 43 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.30, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.00, Looking at: Water Dispenser\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  15%|█▌        | 45/300 [01:11<06:37,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 44 visualization saved to ./attention_test_results/frame_44_attention.png\n","\n","Frame 44 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.31, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.01, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  15%|█▌        | 46/300 [01:13<07:08,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 46 visualization saved to ./attention_test_results/frame_46_attention.png\n","\n","Frame 46 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.03, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  16%|█▌        | 47/300 [01:15<06:55,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 47 visualization saved to ./attention_test_results/frame_47_attention.png\n","\n","Frame 47 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  16%|█▌        | 48/300 [01:16<06:45,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 48 visualization saved to ./attention_test_results/frame_48_attention.png\n","\n","Frame 48 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.16, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.27, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.05, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  16%|█▋        | 49/300 [01:18<06:42,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 49 visualization saved to ./attention_test_results/frame_49_attention.png\n","\n","Frame 49 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.16, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.27, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.06, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  17%|█▋        | 50/300 [01:19<06:43,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 50 visualization saved to ./attention_test_results/frame_50_attention.png\n","\n","Frame 50 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  17%|█▋        | 51/300 [01:21<06:38,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 51 visualization saved to ./attention_test_results/frame_51_attention.png\n","\n","Frame 51 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.15, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  17%|█▋        | 52/300 [01:23<06:37,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 52 visualization saved to ./attention_test_results/frame_52_attention.png\n","\n","Frame 52 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.15, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  18%|█▊        | 53/300 [01:24<06:37,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 53 visualization saved to ./attention_test_results/frame_53_attention.png\n","\n","Frame 53 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.15, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  18%|█▊        | 54/300 [01:26<06:36,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 54 visualization saved to ./attention_test_results/frame_54_attention.png\n","\n","Frame 54 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.15, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  18%|█▊        | 55/300 [01:27<06:34,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 55 visualization saved to ./attention_test_results/frame_55_attention.png\n","\n","Frame 55 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.16, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.31, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.09, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  19%|█▊        | 56/300 [01:29<06:32,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 56 visualization saved to ./attention_test_results/frame_56_attention.png\n","\n","Frame 56 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.16, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.32, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.10, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  19%|█▉        | 57/300 [01:31<06:37,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 57 visualization saved to ./attention_test_results/frame_57_attention.png\n","\n","Frame 57 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.10, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  19%|█▉        | 58/300 [01:32<06:31,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 58 visualization saved to ./attention_test_results/frame_58_attention.png\n","\n","Frame 58 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.11, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  20%|█▉        | 59/300 [01:34<06:26,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 59 visualization saved to ./attention_test_results/frame_59_attention.png\n","\n","Frame 59 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  20%|██        | 60/300 [01:35<06:21,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 60 visualization saved to ./attention_test_results/frame_60_attention.png\n","\n","Frame 60 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  20%|██        | 61/300 [01:37<06:19,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 61 visualization saved to ./attention_test_results/frame_61_attention.png\n","\n","Frame 61 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  21%|██        | 62/300 [01:39<06:20,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 62 visualization saved to ./attention_test_results/frame_62_attention.png\n","\n","Frame 62 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  21%|██        | 63/300 [01:40<06:21,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 63 visualization saved to ./attention_test_results/frame_63_attention.png\n","\n","Frame 63 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  21%|██▏       | 64/300 [01:42<06:21,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 64 visualization saved to ./attention_test_results/frame_64_attention.png\n","\n","Frame 64 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.10, Looking at: Water Dispenser\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  22%|██▏       | 65/300 [01:43<06:16,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 65 visualization saved to ./attention_test_results/frame_65_attention.png\n","\n","Frame 65 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  22%|██▏       | 66/300 [01:45<06:08,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 66 visualization saved to ./attention_test_results/frame_66_attention.png\n","\n","Frame 66 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.12, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  22%|██▏       | 67/300 [01:47<06:05,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 67 visualization saved to ./attention_test_results/frame_67_attention.png\n","\n","Frame 67 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.11, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  23%|██▎       | 68/300 [01:48<06:03,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 68 visualization saved to ./attention_test_results/frame_68_attention.png\n","\n","Frame 68 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  23%|██▎       | 69/300 [01:50<06:04,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 69 visualization saved to ./attention_test_results/frame_69_attention.png\n","\n","Frame 69 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.22, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.08, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  23%|██▎       | 70/300 [01:51<05:59,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 70 visualization saved to ./attention_test_results/frame_70_attention.png\n","\n","Frame 70 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.24, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.08, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  24%|██▎       | 71/300 [01:53<06:32,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 71 visualization saved to ./attention_test_results/frame_71_attention.png\n","\n","Frame 71 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.25, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.08, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  24%|██▍       | 72/300 [01:55<06:22,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 72 visualization saved to ./attention_test_results/frame_72_attention.png\n","\n","Frame 72 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.25, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  24%|██▍       | 73/300 [01:56<06:14,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 73 visualization saved to ./attention_test_results/frame_73_attention.png\n","\n","Frame 73 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.20, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  25%|██▍       | 74/300 [01:58<06:08,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 74 visualization saved to ./attention_test_results/frame_74_attention.png\n","\n","Frame 74 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  25%|██▌       | 75/300 [02:00<06:07,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 76 visualization saved to ./attention_test_results/frame_76_attention.png\n","\n","Frame 76 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.27, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.32, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.11, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.08, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  25%|██▌       | 76/300 [02:01<06:01,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 77 visualization saved to ./attention_test_results/frame_77_attention.png\n","\n","Frame 77 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.21, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.32, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  26%|██▌       | 77/300 [02:03<05:56,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 78 visualization saved to ./attention_test_results/frame_78_attention.png\n","\n","Frame 78 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.21, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.31, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.05, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  26%|██▌       | 78/300 [02:04<05:56,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 80 visualization saved to ./attention_test_results/frame_80_attention.png\n","\n","Frame 80 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.31, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  26%|██▋       | 79/300 [02:06<05:56,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 81 visualization saved to ./attention_test_results/frame_81_attention.png\n","\n","Frame 81 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.10, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  27%|██▋       | 80/300 [02:08<05:55,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 82 visualization saved to ./attention_test_results/frame_82_attention.png\n","\n","Frame 82 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.11, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  27%|██▋       | 81/300 [02:09<05:56,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 83 visualization saved to ./attention_test_results/frame_83_attention.png\n","\n","Frame 83 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.10, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  27%|██▋       | 82/300 [02:11<05:56,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 84 visualization saved to ./attention_test_results/frame_84_attention.png\n","\n","Frame 84 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.10, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  28%|██▊       | 83/300 [02:13<05:53,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 86 visualization saved to ./attention_test_results/frame_86_attention.png\n","\n","Frame 86 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.31, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.10, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  28%|██▊       | 84/300 [02:14<05:51,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 87 visualization saved to ./attention_test_results/frame_87_attention.png\n","\n","Frame 87 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Person/Teacher\n","  Person 10: Score=0.18, Looking at: Monitor/Screen\n","  Person 11: Score=0.10, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  28%|██▊       | 85/300 [02:16<05:46,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 88 visualization saved to ./attention_test_results/frame_88_attention.png\n","\n","Frame 88 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.31, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.10, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  29%|██▊       | 86/300 [02:17<05:45,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 89 visualization saved to ./attention_test_results/frame_89_attention.png\n","\n","Frame 89 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.10, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  29%|██▉       | 87/300 [02:19<05:45,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 90 visualization saved to ./attention_test_results/frame_90_attention.png\n","\n","Frame 90 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Person/Teacher\n","  Person 10: Score=0.18, Looking at: Monitor/Screen\n","  Person 11: Score=0.11, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  29%|██▉       | 88/300 [02:21<05:42,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 92 visualization saved to ./attention_test_results/frame_92_attention.png\n","\n","Frame 92 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.10, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Person/Teacher\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.11, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  30%|██▉       | 89/300 [02:22<05:40,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 93 visualization saved to ./attention_test_results/frame_93_attention.png\n","\n","Frame 93 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.09, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Person/Teacher\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.12, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  30%|███       | 90/300 [02:24<05:39,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 94 visualization saved to ./attention_test_results/frame_94_attention.png\n","\n","Frame 94 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.09, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Person/Teacher\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.12, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  30%|███       | 91/300 [02:26<05:36,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 95 visualization saved to ./attention_test_results/frame_95_attention.png\n","\n","Frame 95 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.12, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  31%|███       | 92/300 [02:27<05:39,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 96 visualization saved to ./attention_test_results/frame_96_attention.png\n","\n","Frame 96 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.09, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  31%|███       | 93/300 [02:29<05:39,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 97 visualization saved to ./attention_test_results/frame_97_attention.png\n","\n","Frame 97 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.16, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Person/Teacher\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  31%|███▏      | 94/300 [02:30<05:35,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 98 visualization saved to ./attention_test_results/frame_98_attention.png\n","\n","Frame 98 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.16, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  32%|███▏      | 95/300 [02:32<05:33,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 99 visualization saved to ./attention_test_results/frame_99_attention.png\n","\n","Frame 99 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.15, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  32%|███▏      | 96/300 [02:34<05:32,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 100 visualization saved to ./attention_test_results/frame_100_attention.png\n","\n","Frame 100 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.15, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  32%|███▏      | 97/300 [02:35<05:31,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 101 visualization saved to ./attention_test_results/frame_101_attention.png\n","\n","Frame 101 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.16, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  33%|███▎      | 98/300 [02:37<05:29,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 102 visualization saved to ./attention_test_results/frame_102_attention.png\n","\n","Frame 102 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.16, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.19, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  33%|███▎      | 99/300 [02:39<05:25,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 103 visualization saved to ./attention_test_results/frame_103_attention.png\n","\n","Frame 103 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.16, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.19, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.03, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  33%|███▎      | 100/300 [02:40<05:19,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 104 visualization saved to ./attention_test_results/frame_104_attention.png\n","\n","Frame 104 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.15, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.11, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.22, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Person/Teacher\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  34%|███▎      | 101/300 [02:42<05:15,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 105 visualization saved to ./attention_test_results/frame_105_attention.png\n","\n","Frame 105 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.15, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.11, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.22, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Person/Teacher\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  34%|███▍      | 102/300 [02:43<05:18,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 106 visualization saved to ./attention_test_results/frame_106_attention.png\n","\n","Frame 106 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.15, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.22, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Person/Teacher\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  34%|███▍      | 103/300 [02:45<05:18,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 107 visualization saved to ./attention_test_results/frame_107_attention.png\n","\n","Frame 107 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.22, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Person/Teacher\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  35%|███▍      | 104/300 [02:47<05:50,  1.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 108 visualization saved to ./attention_test_results/frame_108_attention.png\n","\n","Frame 108 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.22, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Person/Teacher\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  35%|███▌      | 105/300 [02:49<05:39,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 109 visualization saved to ./attention_test_results/frame_109_attention.png\n","\n","Frame 109 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  35%|███▌      | 106/300 [02:50<05:27,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 110 visualization saved to ./attention_test_results/frame_110_attention.png\n","\n","Frame 110 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.16, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  36%|███▌      | 107/300 [02:52<05:21,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 111 visualization saved to ./attention_test_results/frame_111_attention.png\n","\n","Frame 111 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.10, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  36%|███▌      | 108/300 [02:54<05:15,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 112 visualization saved to ./attention_test_results/frame_112_attention.png\n","\n","Frame 112 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.17, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.10, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  36%|███▋      | 109/300 [02:55<05:12,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 113 visualization saved to ./attention_test_results/frame_113_attention.png\n","\n","Frame 113 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.18, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.11, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  37%|███▋      | 110/300 [02:57<05:09,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 114 visualization saved to ./attention_test_results/frame_114_attention.png\n","\n","Frame 114 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.12, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.05, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  37%|███▋      | 111/300 [02:58<05:04,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 115 visualization saved to ./attention_test_results/frame_115_attention.png\n","\n","Frame 115 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.19, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.13, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Person/Teacher\n","  Person 10: Score=0.20, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  37%|███▋      | 112/300 [03:00<05:00,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 116 visualization saved to ./attention_test_results/frame_116_attention.png\n","\n","Frame 116 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.20, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.14, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Person/Teacher\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  38%|███▊      | 113/300 [03:02<04:58,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 117 visualization saved to ./attention_test_results/frame_117_attention.png\n","\n","Frame 117 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.21, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Person/Teacher\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  38%|███▊      | 114/300 [03:03<04:57,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 118 visualization saved to ./attention_test_results/frame_118_attention.png\n","\n","Frame 118 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.22, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.21, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.03, Looking at: Person/Teacher\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  38%|███▊      | 115/300 [03:05<04:56,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 119 visualization saved to ./attention_test_results/frame_119_attention.png\n","\n","Frame 119 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.23, Looking at: Monitor/Screen\n","  Person 1: Score=0.16, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.23, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Person/Teacher\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  39%|███▊      | 116/300 [03:06<04:53,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 120 visualization saved to ./attention_test_results/frame_120_attention.png\n","\n","Frame 120 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.21, Looking at: Monitor/Screen\n","  Person 1: Score=0.16, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.23, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  39%|███▉      | 117/300 [03:08<04:51,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 121 visualization saved to ./attention_test_results/frame_121_attention.png\n","\n","Frame 121 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.23, Looking at: Monitor/Screen\n","  Person 1: Score=0.16, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.23, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  39%|███▉      | 118/300 [03:10<04:49,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 122 visualization saved to ./attention_test_results/frame_122_attention.png\n","\n","Frame 122 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.24, Looking at: Monitor/Screen\n","  Person 1: Score=0.16, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.22, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  40%|███▉      | 119/300 [03:11<04:50,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 123 visualization saved to ./attention_test_results/frame_123_attention.png\n","\n","Frame 123 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.24, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.26, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.21, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.03, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  40%|████      | 120/300 [03:13<04:51,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 124 visualization saved to ./attention_test_results/frame_124_attention.png\n","\n","Frame 124 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.21, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.03, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  40%|████      | 121/300 [03:14<04:49,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 125 visualization saved to ./attention_test_results/frame_125_attention.png\n","\n","Frame 125 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.27, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.20, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  41%|████      | 122/300 [03:16<04:46,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 126 visualization saved to ./attention_test_results/frame_126_attention.png\n","\n","Frame 126 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.27, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.20, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.05, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  41%|████      | 123/300 [03:18<04:42,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 127 visualization saved to ./attention_test_results/frame_127_attention.png\n","\n","Frame 127 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.06, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  41%|████▏     | 124/300 [03:19<04:40,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 128 visualization saved to ./attention_test_results/frame_128_attention.png\n","\n","Frame 128 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  42%|████▏     | 125/300 [03:21<04:39,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 129 visualization saved to ./attention_test_results/frame_129_attention.png\n","\n","Frame 129 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  42%|████▏     | 126/300 [03:22<04:40,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 130 visualization saved to ./attention_test_results/frame_130_attention.png\n","\n","Frame 130 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.16, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  42%|████▏     | 127/300 [03:24<04:42,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 131 visualization saved to ./attention_test_results/frame_131_attention.png\n","\n","Frame 131 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.10, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  43%|████▎     | 128/300 [03:26<04:37,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 132 visualization saved to ./attention_test_results/frame_132_attention.png\n","\n","Frame 132 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.19, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  43%|████▎     | 129/300 [03:27<04:34,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 133 visualization saved to ./attention_test_results/frame_133_attention.png\n","\n","Frame 133 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  43%|████▎     | 130/300 [03:29<04:34,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 134 visualization saved to ./attention_test_results/frame_134_attention.png\n","\n","Frame 134 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  44%|████▎     | 131/300 [03:30<04:32,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 135 visualization saved to ./attention_test_results/frame_135_attention.png\n","\n","Frame 135 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.17, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.15, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  44%|████▍     | 132/300 [03:32<04:28,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 136 visualization saved to ./attention_test_results/frame_136_attention.png\n","\n","Frame 136 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Monitor/Screen\n","  Person 10: Score=0.19, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  44%|████▍     | 133/300 [03:34<04:29,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 137 visualization saved to ./attention_test_results/frame_137_attention.png\n","\n","Frame 137 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Monitor/Screen\n","  Person 10: Score=0.18, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  45%|████▍     | 134/300 [03:35<04:29,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 138 visualization saved to ./attention_test_results/frame_138_attention.png\n","\n","Frame 138 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.27, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.23, Looking at: Monitor/Screen\n","  Person 10: Score=0.18, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  45%|████▌     | 135/300 [03:37<04:25,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 139 visualization saved to ./attention_test_results/frame_139_attention.png\n","\n","Frame 139 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.27, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.25, Looking at: Monitor/Screen\n","  Person 10: Score=0.18, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  45%|████▌     | 136/300 [03:39<04:22,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 140 visualization saved to ./attention_test_results/frame_140_attention.png\n","\n","Frame 140 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.27, Looking at: Monitor/Screen\n","  Person 10: Score=0.18, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  46%|████▌     | 137/300 [03:40<04:21,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 141 visualization saved to ./attention_test_results/frame_141_attention.png\n","\n","Frame 141 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.28, Looking at: Monitor/Screen\n","  Person 10: Score=0.18, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  46%|████▌     | 138/300 [03:42<04:19,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 142 visualization saved to ./attention_test_results/frame_142_attention.png\n","\n","Frame 142 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.27, Looking at: Monitor/Screen\n","  Person 10: Score=0.18, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  46%|████▋     | 139/300 [03:43<04:17,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 143 visualization saved to ./attention_test_results/frame_143_attention.png\n","\n","Frame 143 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.27, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  47%|████▋     | 140/300 [03:45<04:14,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 144 visualization saved to ./attention_test_results/frame_144_attention.png\n","\n","Frame 144 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.25, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.27, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  47%|████▋     | 141/300 [03:46<04:13,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 145 visualization saved to ./attention_test_results/frame_145_attention.png\n","\n","Frame 145 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.18, Looking at: Monitor/Screen\n","  Person 6: Score=0.25, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.26, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  47%|████▋     | 142/300 [03:48<04:12,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 146 visualization saved to ./attention_test_results/frame_146_attention.png\n","\n","Frame 146 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.22\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.18, Looking at: Monitor/Screen\n","  Person 6: Score=0.25, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.25, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  48%|████▊     | 143/300 [03:50<04:48,  1.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 147 visualization saved to ./attention_test_results/frame_147_attention.png\n","\n","Frame 147 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.18, Looking at: Monitor/Screen\n","  Person 6: Score=0.25, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  48%|████▊     | 144/300 [03:52<04:35,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 148 visualization saved to ./attention_test_results/frame_148_attention.png\n","\n","Frame 148 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.18, Looking at: Monitor/Screen\n","  Person 6: Score=0.25, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.10, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.12, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  48%|████▊     | 145/300 [03:54<04:26,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 149 visualization saved to ./attention_test_results/frame_149_attention.png\n","\n","Frame 149 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.25, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.12, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  49%|████▊     | 146/300 [03:55<04:20,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 150 visualization saved to ./attention_test_results/frame_150_attention.png\n","\n","Frame 150 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.11, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  49%|████▉     | 147/300 [03:57<04:17,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 151 visualization saved to ./attention_test_results/frame_151_attention.png\n","\n","Frame 151 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.11, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  49%|████▉     | 148/300 [03:59<04:09,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 152 visualization saved to ./attention_test_results/frame_152_attention.png\n","\n","Frame 152 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.11, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  50%|████▉     | 149/300 [04:00<04:04,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 153 visualization saved to ./attention_test_results/frame_153_attention.png\n","\n","Frame 153 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.01, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  50%|█████     | 150/300 [04:02<04:01,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 154 visualization saved to ./attention_test_results/frame_154_attention.png\n","\n","Frame 154 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.01, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.07, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  50%|█████     | 151/300 [04:03<04:00,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 155 visualization saved to ./attention_test_results/frame_155_attention.png\n","\n","Frame 155 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.01, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.06, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  51%|█████     | 152/300 [04:05<03:57,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 156 visualization saved to ./attention_test_results/frame_156_attention.png\n","\n","Frame 156 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.32, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.06, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  51%|█████     | 153/300 [04:06<03:55,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 157 visualization saved to ./attention_test_results/frame_157_attention.png\n","\n","Frame 157 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.27, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.04, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  51%|█████▏    | 154/300 [04:08<03:54,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 158 visualization saved to ./attention_test_results/frame_158_attention.png\n","\n","Frame 158 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.27, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.04, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  52%|█████▏    | 155/300 [04:10<03:53,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 160 visualization saved to ./attention_test_results/frame_160_attention.png\n","\n","Frame 160 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.27, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.04, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  52%|█████▏    | 156/300 [04:11<03:53,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 161 visualization saved to ./attention_test_results/frame_161_attention.png\n","\n","Frame 161 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.18, Looking at: Monitor/Screen\n","  Person 6: Score=0.34, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.04, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  52%|█████▏    | 157/300 [04:13<03:51,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 162 visualization saved to ./attention_test_results/frame_162_attention.png\n","\n","Frame 162 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.17, Looking at: Monitor/Screen\n","  Person 6: Score=0.34, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.05, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  53%|█████▎    | 158/300 [04:15<03:50,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 163 visualization saved to ./attention_test_results/frame_163_attention.png\n","\n","Frame 163 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.17, Looking at: Monitor/Screen\n","  Person 6: Score=0.34, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.03, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.05, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  53%|█████▎    | 159/300 [04:16<03:47,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 164 visualization saved to ./attention_test_results/frame_164_attention.png\n","\n","Frame 164 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.15, Looking at: Monitor/Screen\n","  Person 6: Score=0.34, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.08, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  53%|█████▎    | 160/300 [04:18<03:42,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 165 visualization saved to ./attention_test_results/frame_165_attention.png\n","\n","Frame 165 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.15, Looking at: Monitor/Screen\n","  Person 6: Score=0.34, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.08, Looking at: Person/Teacher\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  54%|█████▎    | 161/300 [04:19<03:43,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 166 visualization saved to ./attention_test_results/frame_166_attention.png\n","\n","Frame 166 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.06, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  54%|█████▍    | 162/300 [04:21<03:39,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 167 visualization saved to ./attention_test_results/frame_167_attention.png\n","\n","Frame 167 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.06, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.11, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  54%|█████▍    | 163/300 [04:23<03:36,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 168 visualization saved to ./attention_test_results/frame_168_attention.png\n","\n","Frame 168 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.11, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  55%|█████▍    | 164/300 [04:24<03:33,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 169 visualization saved to ./attention_test_results/frame_169_attention.png\n","\n","Frame 169 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.12, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Person/Teacher\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.12, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  55%|█████▌    | 165/300 [04:26<03:30,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 170 visualization saved to ./attention_test_results/frame_170_attention.png\n","\n","Frame 170 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.12, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Person/Teacher\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.12, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  55%|█████▌    | 166/300 [04:27<03:29,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 171 visualization saved to ./attention_test_results/frame_171_attention.png\n","\n","Frame 171 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.12, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.05, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  56%|█████▌    | 167/300 [04:29<03:29,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 172 visualization saved to ./attention_test_results/frame_172_attention.png\n","\n","Frame 172 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.12, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.15, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  56%|█████▌    | 168/300 [04:30<03:31,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 173 visualization saved to ./attention_test_results/frame_173_attention.png\n","\n","Frame 173 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.12, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.09, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  56%|█████▋    | 169/300 [04:32<03:33,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 174 visualization saved to ./attention_test_results/frame_174_attention.png\n","\n","Frame 174 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.29, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.09, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  57%|█████▋    | 170/300 [04:34<03:30,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 175 visualization saved to ./attention_test_results/frame_175_attention.png\n","\n","Frame 175 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.09, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  57%|█████▋    | 171/300 [04:35<03:28,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 176 visualization saved to ./attention_test_results/frame_176_attention.png\n","\n","Frame 176 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.11, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  57%|█████▋    | 172/300 [04:37<03:25,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 177 visualization saved to ./attention_test_results/frame_177_attention.png\n","\n","Frame 177 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.32, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  58%|█████▊    | 173/300 [04:39<03:26,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 178 visualization saved to ./attention_test_results/frame_178_attention.png\n","\n","Frame 178 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.33, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  58%|█████▊    | 174/300 [04:40<03:24,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 179 visualization saved to ./attention_test_results/frame_179_attention.png\n","\n","Frame 179 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.32, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  58%|█████▊    | 175/300 [04:42<03:27,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 180 visualization saved to ./attention_test_results/frame_180_attention.png\n","\n","Frame 180 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.32, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Person/Teacher\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  59%|█████▊    | 176/300 [04:44<03:25,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 181 visualization saved to ./attention_test_results/frame_181_attention.png\n","\n","Frame 181 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.13, Looking at: Monitor/Screen\n","  Person 6: Score=0.32, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  59%|█████▉    | 177/300 [04:45<03:21,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 182 visualization saved to ./attention_test_results/frame_182_attention.png\n","\n","Frame 182 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.14, Looking at: Monitor/Screen\n","  Person 6: Score=0.32, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  59%|█████▉    | 178/300 [04:47<03:20,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 183 visualization saved to ./attention_test_results/frame_183_attention.png\n","\n","Frame 183 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.15, Looking at: Monitor/Screen\n","  Person 6: Score=0.31, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.11, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  60%|█████▉    | 179/300 [04:49<03:21,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 184 visualization saved to ./attention_test_results/frame_184_attention.png\n","\n","Frame 184 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.15, Looking at: Monitor/Screen\n","  Person 6: Score=0.31, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.10, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  60%|██████    | 180/300 [04:50<03:18,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 185 visualization saved to ./attention_test_results/frame_185_attention.png\n","\n","Frame 185 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.16, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  60%|██████    | 181/300 [04:52<03:16,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 186 visualization saved to ./attention_test_results/frame_186_attention.png\n","\n","Frame 186 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.17, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  61%|██████    | 182/300 [04:54<03:16,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 187 visualization saved to ./attention_test_results/frame_187_attention.png\n","\n","Frame 187 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.17, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  61%|██████    | 183/300 [04:55<03:13,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 188 visualization saved to ./attention_test_results/frame_188_attention.png\n","\n","Frame 188 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.17, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  61%|██████▏   | 184/300 [04:57<03:09,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 189 visualization saved to ./attention_test_results/frame_189_attention.png\n","\n","Frame 189 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.18, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  62%|██████▏   | 185/300 [04:58<03:05,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 190 visualization saved to ./attention_test_results/frame_190_attention.png\n","\n","Frame 190 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.18, Looking at: Monitor/Screen\n","  Person 6: Score=0.30, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  62%|██████▏   | 186/300 [05:00<03:04,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 191 visualization saved to ./attention_test_results/frame_191_attention.png\n","\n","Frame 191 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.27, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.18, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  62%|██████▏   | 187/300 [05:02<03:04,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 192 visualization saved to ./attention_test_results/frame_192_attention.png\n","\n","Frame 192 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.18, Looking at: Monitor/Screen\n","  Person 6: Score=0.20, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  63%|██████▎   | 188/300 [05:03<03:00,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 193 visualization saved to ./attention_test_results/frame_193_attention.png\n","\n","Frame 193 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.27, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.33, Looking at: Monitor/Screen\n","  Person 5: Score=0.18, Looking at: Monitor/Screen\n","  Person 6: Score=0.19, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.07, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  63%|██████▎   | 189/300 [05:05<03:01,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 194 visualization saved to ./attention_test_results/frame_194_attention.png\n","\n","Frame 194 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.34, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.18, Looking at: Monitor/Screen\n","  Person 7: Score=0.36, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  63%|██████▎   | 190/300 [05:06<02:56,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 195 visualization saved to ./attention_test_results/frame_195_attention.png\n","\n","Frame 195 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.35, Looking at: Monitor/Screen\n","  Person 5: Score=0.19, Looking at: Monitor/Screen\n","  Person 6: Score=0.16, Looking at: Monitor/Screen\n","  Person 7: Score=0.36, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.09, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  64%|██████▎   | 191/300 [05:08<02:53,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 196 visualization saved to ./attention_test_results/frame_196_attention.png\n","\n","Frame 196 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.35, Looking at: Monitor/Screen\n","  Person 5: Score=0.20, Looking at: Monitor/Screen\n","  Person 6: Score=0.12, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.11, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.10, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  64%|██████▍   | 192/300 [05:10<03:20,  1.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 197 visualization saved to ./attention_test_results/frame_197_attention.png\n","\n","Frame 197 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.35, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.11, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  64%|██████▍   | 193/300 [05:12<03:11,  1.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 198 visualization saved to ./attention_test_results/frame_198_attention.png\n","\n","Frame 198 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.35, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.10, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  65%|██████▍   | 194/300 [05:14<03:00,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 199 visualization saved to ./attention_test_results/frame_199_attention.png\n","\n","Frame 199 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.35, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.09, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  65%|██████▌   | 195/300 [05:15<02:54,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 200 visualization saved to ./attention_test_results/frame_200_attention.png\n","\n","Frame 200 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.34, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.09, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.15, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  65%|██████▌   | 196/300 [05:17<02:52,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 201 visualization saved to ./attention_test_results/frame_201_attention.png\n","\n","Frame 201 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.35, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.09, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  66%|██████▌   | 197/300 [05:18<02:48,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 202 visualization saved to ./attention_test_results/frame_202_attention.png\n","\n","Frame 202 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.28, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.27, Looking at: Monitor/Screen\n","  Person 3: Score=0.28, Looking at: Monitor/Screen\n","  Person 4: Score=0.34, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  66%|██████▌   | 198/300 [05:20<02:45,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 203 visualization saved to ./attention_test_results/frame_203_attention.png\n","\n","Frame 203 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.34, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  66%|██████▋   | 199/300 [05:22<02:43,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 204 visualization saved to ./attention_test_results/frame_204_attention.png\n","\n","Frame 204 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.33, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.19, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  67%|██████▋   | 200/300 [05:23<02:41,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 205 visualization saved to ./attention_test_results/frame_205_attention.png\n","\n","Frame 205 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.33, Looking at: Monitor/Screen\n","  Person 5: Score=0.21, Looking at: Monitor/Screen\n","  Person 6: Score=0.05, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.20, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  67%|██████▋   | 201/300 [05:25<02:39,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 206 visualization saved to ./attention_test_results/frame_206_attention.png\n","\n","Frame 206 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.33, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.05, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.20, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  67%|██████▋   | 202/300 [05:26<02:37,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 207 visualization saved to ./attention_test_results/frame_207_attention.png\n","\n","Frame 207 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.05, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  68%|██████▊   | 203/300 [05:28<02:36,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 208 visualization saved to ./attention_test_results/frame_208_attention.png\n","\n","Frame 208 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.24, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  68%|██████▊   | 204/300 [05:30<02:34,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 209 visualization saved to ./attention_test_results/frame_209_attention.png\n","\n","Frame 209 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.24, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  68%|██████▊   | 205/300 [05:31<02:33,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 210 visualization saved to ./attention_test_results/frame_210_attention.png\n","\n","Frame 210 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.23, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.25, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  69%|██████▊   | 206/300 [05:33<02:31,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 211 visualization saved to ./attention_test_results/frame_211_attention.png\n","\n","Frame 211 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.26, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.08, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  69%|██████▉   | 207/300 [05:34<02:28,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 212 visualization saved to ./attention_test_results/frame_212_attention.png\n","\n","Frame 212 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.30, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  69%|██████▉   | 208/300 [05:36<02:25,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 213 visualization saved to ./attention_test_results/frame_213_attention.png\n","\n","Frame 213 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.24, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.31, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  70%|██████▉   | 209/300 [05:38<02:22,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 214 visualization saved to ./attention_test_results/frame_214_attention.png\n","\n","Frame 214 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.25, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.28, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  70%|███████   | 210/300 [05:39<02:20,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 215 visualization saved to ./attention_test_results/frame_215_attention.png\n","\n","Frame 215 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.26, Looking at: Monitor/Screen\n","  Person 6: Score=0.09, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.22, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  70%|███████   | 211/300 [05:41<02:18,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 216 visualization saved to ./attention_test_results/frame_216_attention.png\n","\n","Frame 216 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.26, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.26, Looking at: Monitor/Screen\n","  Person 6: Score=0.09, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.23, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  71%|███████   | 212/300 [05:42<02:15,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 217 visualization saved to ./attention_test_results/frame_217_attention.png\n","\n","Frame 217 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.27, Looking at: Monitor/Screen\n","  Person 6: Score=0.10, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  71%|███████   | 213/300 [05:44<02:12,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 218 visualization saved to ./attention_test_results/frame_218_attention.png\n","\n","Frame 218 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.27, Looking at: Monitor/Screen\n","  Person 6: Score=0.10, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Monitor/Screen\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  71%|███████▏  | 214/300 [05:45<02:12,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 219 visualization saved to ./attention_test_results/frame_219_attention.png\n","\n","Frame 219 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.27, Looking at: Monitor/Screen\n","  Person 6: Score=0.11, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  72%|███████▏  | 215/300 [05:47<02:10,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 220 visualization saved to ./attention_test_results/frame_220_attention.png\n","\n","Frame 220 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.27, Looking at: Monitor/Screen\n","  Person 6: Score=0.14, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.19, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Person/Teacher\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  72%|███████▏  | 216/300 [05:48<02:11,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 221 visualization saved to ./attention_test_results/frame_221_attention.png\n","\n","Frame 221 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.27, Looking at: Monitor/Screen\n","  Person 6: Score=0.14, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.19, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Person/Teacher\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  72%|███████▏  | 217/300 [05:50<02:09,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 222 visualization saved to ./attention_test_results/frame_222_attention.png\n","\n","Frame 222 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.28, Looking at: Monitor/Screen\n","  Person 6: Score=0.16, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.20, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.10, Looking at: Person/Teacher\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  73%|███████▎  | 218/300 [05:51<02:09,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 223 visualization saved to ./attention_test_results/frame_223_attention.png\n","\n","Frame 223 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.28, Looking at: Monitor/Screen\n","  Person 6: Score=0.16, Looking at: Water Dispenser\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.20, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.10, Looking at: Person/Teacher\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  73%|███████▎  | 219/300 [05:53<02:09,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 224 visualization saved to ./attention_test_results/frame_224_attention.png\n","\n","Frame 224 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.28, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Person/Teacher\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.22, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Person/Teacher\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  73%|███████▎  | 220/300 [05:55<02:05,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 225 visualization saved to ./attention_test_results/frame_225_attention.png\n","\n","Frame 225 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.28, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Person/Teacher\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.22, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.09, Looking at: Person/Teacher\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  74%|███████▎  | 221/300 [05:56<02:02,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 226 visualization saved to ./attention_test_results/frame_226_attention.png\n","\n","Frame 226 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.29, Looking at: Monitor/Screen\n","  Person 6: Score=0.18, Looking at: Person/Teacher\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.23, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.08, Looking at: Person/Teacher\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  74%|███████▍  | 222/300 [05:58<02:00,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 227 visualization saved to ./attention_test_results/frame_227_attention.png\n","\n","Frame 227 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.29, Looking at: Monitor/Screen\n","  Person 6: Score=0.18, Looking at: Person/Teacher\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.23, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.08, Looking at: Person/Teacher\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  74%|███████▍  | 223/300 [05:59<01:59,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 228 visualization saved to ./attention_test_results/frame_228_attention.png\n","\n","Frame 228 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.29, Looking at: Monitor/Screen\n","  Person 6: Score=0.18, Looking at: Person/Teacher\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.23, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.08, Looking at: Person/Teacher\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  75%|███████▍  | 224/300 [06:01<01:59,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 229 visualization saved to ./attention_test_results/frame_229_attention.png\n","\n","Frame 229 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.30, Looking at: Monitor/Screen\n","  Person 6: Score=0.18, Looking at: Person/Teacher\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.22, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.07, Looking at: Person/Teacher\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  75%|███████▌  | 225/300 [06:02<01:57,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 230 visualization saved to ./attention_test_results/frame_230_attention.png\n","\n","Frame 230 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.29, Looking at: Monitor/Screen\n","  Person 6: Score=0.18, Looking at: Person/Teacher\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.30, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.06, Looking at: Person/Teacher\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  75%|███████▌  | 226/300 [06:04<01:55,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 231 visualization saved to ./attention_test_results/frame_231_attention.png\n","\n","Frame 231 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.30, Looking at: Monitor/Screen\n","  Person 6: Score=0.18, Looking at: Person/Teacher\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.07, Looking at: Person/Teacher\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  76%|███████▌  | 227/300 [06:06<01:54,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 232 visualization saved to ./attention_test_results/frame_232_attention.png\n","\n","Frame 232 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.30, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Person/Teacher\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.07, Looking at: Person/Teacher\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  76%|███████▌  | 228/300 [06:07<01:53,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 233 visualization saved to ./attention_test_results/frame_233_attention.png\n","\n","Frame 233 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (9 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.31, Looking at: Monitor/Screen\n","  Person 6: Score=0.17, Looking at: Person/Teacher\n","  Person 7: Score=0.34, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.07, Looking at: Person/Teacher\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  76%|███████▋  | 229/300 [06:09<01:51,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 234 visualization saved to ./attention_test_results/frame_234_attention.png\n","\n","Frame 234 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.14, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.10, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  77%|███████▋  | 230/300 [06:10<01:50,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 235 visualization saved to ./attention_test_results/frame_235_attention.png\n","\n","Frame 235 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.13, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.32, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.10, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  77%|███████▋  | 231/300 [06:12<01:50,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 236 visualization saved to ./attention_test_results/frame_236_attention.png\n","\n","Frame 236 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.33, Looking at: Monitor/Screen\n","  Person 6: Score=0.12, Looking at: Water Dispenser\n","  Person 7: Score=0.35, Looking at: Monitor/Screen\n","  Person 8: Score=0.31, Looking at: Monitor/Screen\n","  Person 9: Score=0.16, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.11, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  77%|███████▋  | 232/300 [06:14<01:48,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 237 visualization saved to ./attention_test_results/frame_237_attention.png\n","\n","Frame 237 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.09, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.29, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.12, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  78%|███████▊  | 233/300 [06:15<01:47,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 238 visualization saved to ./attention_test_results/frame_238_attention.png\n","\n","Frame 238 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.27, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  78%|███████▊  | 234/300 [06:17<01:46,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 239 visualization saved to ./attention_test_results/frame_239_attention.png\n","\n","Frame 239 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.27, Looking at: Monitor/Screen\n","  Person 9: Score=0.13, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.13, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  78%|███████▊  | 235/300 [06:18<01:46,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 240 visualization saved to ./attention_test_results/frame_240_attention.png\n","\n","Frame 240 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.25, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.27, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  79%|███████▊  | 236/300 [06:20<01:44,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 241 visualization saved to ./attention_test_results/frame_241_attention.png\n","\n","Frame 241 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.28, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.12, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  79%|███████▉  | 237/300 [06:22<01:42,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 242 visualization saved to ./attention_test_results/frame_242_attention.png\n","\n","Frame 242 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.11, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  79%|███████▉  | 238/300 [06:23<01:42,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 243 visualization saved to ./attention_test_results/frame_243_attention.png\n","\n","Frame 243 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  80%|███████▉  | 239/300 [06:25<01:41,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 244 visualization saved to ./attention_test_results/frame_244_attention.png\n","\n","Frame 244 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.24, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.06, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  80%|████████  | 240/300 [06:27<01:38,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 245 visualization saved to ./attention_test_results/frame_245_attention.png\n","\n","Frame 245 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.32, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.05, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  80%|████████  | 241/300 [06:28<01:36,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 246 visualization saved to ./attention_test_results/frame_246_attention.png\n","\n","Frame 246 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.31, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.05, Looking at: Person/Teacher\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  81%|████████  | 242/300 [06:30<01:33,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 247 visualization saved to ./attention_test_results/frame_247_attention.png\n","\n","Frame 247 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.31, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.06, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.17, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  81%|████████  | 243/300 [06:31<01:31,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 248 visualization saved to ./attention_test_results/frame_248_attention.png\n","\n","Frame 248 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.31, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.17, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  81%|████████▏ | 244/300 [06:33<01:30,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 249 visualization saved to ./attention_test_results/frame_249_attention.png\n","\n","Frame 249 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.30, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.11, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.17, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  82%|████████▏ | 245/300 [06:35<01:30,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 250 visualization saved to ./attention_test_results/frame_250_attention.png\n","\n","Frame 250 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.30, Looking at: Monitor/Screen\n","  Person 6: Score=0.05, Looking at: Water Dispenser\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.17, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  82%|████████▏ | 246/300 [06:36<01:27,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 251 visualization saved to ./attention_test_results/frame_251_attention.png\n","\n","Frame 251 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.33, Looking at: Monitor/Screen\n","  Person 5: Score=0.30, Looking at: Monitor/Screen\n","  Person 6: Score=0.05, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.17, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  82%|████████▏ | 247/300 [06:38<01:25,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 252 visualization saved to ./attention_test_results/frame_252_attention.png\n","\n","Frame 252 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.33, Looking at: Monitor/Screen\n","  Person 5: Score=0.29, Looking at: Monitor/Screen\n","  Person 6: Score=0.05, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.21, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.17, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  83%|████████▎ | 248/300 [06:40<01:23,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 253 visualization saved to ./attention_test_results/frame_253_attention.png\n","\n","Frame 253 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.33, Looking at: Monitor/Screen\n","  Person 5: Score=0.28, Looking at: Monitor/Screen\n","  Person 6: Score=0.05, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.23, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.17, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  83%|████████▎ | 249/300 [06:41<01:20,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 254 visualization saved to ./attention_test_results/frame_254_attention.png\n","\n","Frame 254 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.33, Looking at: Monitor/Screen\n","  Person 5: Score=0.28, Looking at: Monitor/Screen\n","  Person 6: Score=0.05, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.24, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.17, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  83%|████████▎ | 250/300 [06:43<01:18,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 255 visualization saved to ./attention_test_results/frame_255_attention.png\n","\n","Frame 255 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.27, Looking at: Monitor/Screen\n","  Person 6: Score=0.05, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.26, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  84%|████████▎ | 251/300 [06:44<01:17,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 256 visualization saved to ./attention_test_results/frame_256_attention.png\n","\n","Frame 256 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.25, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.26, Looking at: Monitor/Screen\n","  Person 6: Score=0.05, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.27, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  84%|████████▍ | 252/300 [06:46<01:17,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 257 visualization saved to ./attention_test_results/frame_257_attention.png\n","\n","Frame 257 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.25, Looking at: Monitor/Screen\n","  Person 6: Score=0.06, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.28, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.16, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  84%|████████▍ | 253/300 [06:49<01:32,  1.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 258 visualization saved to ./attention_test_results/frame_258_attention.png\n","\n","Frame 258 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.25, Looking at: Monitor/Screen\n","  Person 6: Score=0.07, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.28, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  85%|████████▍ | 254/300 [06:50<01:25,  1.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 259 visualization saved to ./attention_test_results/frame_259_attention.png\n","\n","Frame 259 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.24, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.08, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.28, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  85%|████████▌ | 255/300 [06:52<01:20,  1.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 260 visualization saved to ./attention_test_results/frame_260_attention.png\n","\n","Frame 260 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.09, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.27, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  85%|████████▌ | 256/300 [06:54<01:15,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 261 visualization saved to ./attention_test_results/frame_261_attention.png\n","\n","Frame 261 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.10, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.26, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.02, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  86%|████████▌ | 257/300 [06:55<01:12,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 262 visualization saved to ./attention_test_results/frame_262_attention.png\n","\n","Frame 262 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.23, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.10, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.26, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  86%|████████▌ | 258/300 [06:57<01:10,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 263 visualization saved to ./attention_test_results/frame_263_attention.png\n","\n","Frame 263 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.22, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.11, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.25, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  86%|████████▋ | 259/300 [06:58<01:07,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 264 visualization saved to ./attention_test_results/frame_264_attention.png\n","\n","Frame 264 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.21, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.14, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.24, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.14, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  87%|████████▋ | 260/300 [07:00<01:05,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 265 visualization saved to ./attention_test_results/frame_265_attention.png\n","\n","Frame 265 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.19, Looking at: Monitor/Screen\n","  Person 2: Score=0.21, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.20, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.24, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  87%|████████▋ | 261/300 [07:02<01:04,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 266 visualization saved to ./attention_test_results/frame_266_attention.png\n","\n","Frame 266 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.18, Looking at: Monitor/Screen\n","  Person 2: Score=0.20, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.23, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.23, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  87%|████████▋ | 262/300 [07:03<01:02,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 267 visualization saved to ./attention_test_results/frame_267_attention.png\n","\n","Frame 267 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.21, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.25, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.23, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  88%|████████▊ | 263/300 [07:05<01:00,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 268 visualization saved to ./attention_test_results/frame_268_attention.png\n","\n","Frame 268 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.21, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.23, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  88%|████████▊ | 264/300 [07:06<00:58,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 269 visualization saved to ./attention_test_results/frame_269_attention.png\n","\n","Frame 269 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.21, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.26, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.23, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  88%|████████▊ | 265/300 [07:08<00:57,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 270 visualization saved to ./attention_test_results/frame_270_attention.png\n","\n","Frame 270 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.20, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.20, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  89%|████████▊ | 266/300 [07:10<00:54,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 271 visualization saved to ./attention_test_results/frame_271_attention.png\n","\n","Frame 271 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.21\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.20, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.20, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  89%|████████▉ | 267/300 [07:11<00:53,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 272 visualization saved to ./attention_test_results/frame_272_attention.png\n","\n","Frame 272 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  89%|████████▉ | 268/300 [07:13<00:51,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 273 visualization saved to ./attention_test_results/frame_273_attention.png\n","\n","Frame 273 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  90%|████████▉ | 269/300 [07:15<00:50,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 274 visualization saved to ./attention_test_results/frame_274_attention.png\n","\n","Frame 274 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.20, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  90%|█████████ | 270/300 [07:16<00:48,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 275 visualization saved to ./attention_test_results/frame_275_attention.png\n","\n","Frame 275 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  90%|█████████ | 271/300 [07:18<00:46,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 276 visualization saved to ./attention_test_results/frame_276_attention.png\n","\n","Frame 276 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.21, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  91%|█████████ | 272/300 [07:19<00:44,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 277 visualization saved to ./attention_test_results/frame_277_attention.png\n","\n","Frame 277 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  91%|█████████ | 273/300 [07:21<00:44,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 278 visualization saved to ./attention_test_results/frame_278_attention.png\n","\n","Frame 278 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  91%|█████████▏| 274/300 [07:23<00:42,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 279 visualization saved to ./attention_test_results/frame_279_attention.png\n","\n","Frame 279 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.22, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.33, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  92%|█████████▏| 275/300 [07:24<00:41,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 280 visualization saved to ./attention_test_results/frame_280_attention.png\n","\n","Frame 280 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.27, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  92%|█████████▏| 276/300 [07:26<00:39,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 281 visualization saved to ./attention_test_results/frame_281_attention.png\n","\n","Frame 281 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.27, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.18, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.01, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  92%|█████████▏| 277/300 [07:28<00:37,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 282 visualization saved to ./attention_test_results/frame_282_attention.png\n","\n","Frame 282 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  93%|█████████▎| 278/300 [07:29<00:36,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 283 visualization saved to ./attention_test_results/frame_283_attention.png\n","\n","Frame 283 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.32, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  93%|█████████▎| 279/300 [07:31<00:35,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 284 visualization saved to ./attention_test_results/frame_284_attention.png\n","\n","Frame 284 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.23, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.31, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.17, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  93%|█████████▎| 280/300 [07:33<00:33,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 285 visualization saved to ./attention_test_results/frame_285_attention.png\n","\n","Frame 285 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.30, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.16, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  94%|█████████▎| 281/300 [07:34<00:31,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 286 visualization saved to ./attention_test_results/frame_286_attention.png\n","\n","Frame 286 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.30, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.16, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  94%|█████████▍| 282/300 [07:36<00:29,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 287 visualization saved to ./attention_test_results/frame_287_attention.png\n","\n","Frame 287 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.27, Looking at: Monitor/Screen\n","  Person 7: Score=0.29, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.15, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  94%|█████████▍| 283/300 [07:37<00:27,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 288 visualization saved to ./attention_test_results/frame_288_attention.png\n","\n","Frame 288 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.27, Looking at: Monitor/Screen\n","  Person 7: Score=0.29, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.15, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  95%|█████████▍| 284/300 [07:39<00:25,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 289 visualization saved to ./attention_test_results/frame_289_attention.png\n","\n","Frame 289 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.27, Looking at: Monitor/Screen\n","  Person 7: Score=0.28, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Monitor/Screen\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  95%|█████████▌| 285/300 [07:41<00:24,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 290 visualization saved to ./attention_test_results/frame_290_attention.png\n","\n","Frame 290 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.27, Looking at: Monitor/Screen\n","  Person 7: Score=0.27, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Monitor/Screen\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  95%|█████████▌| 286/300 [07:42<00:22,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 291 visualization saved to ./attention_test_results/frame_291_attention.png\n","\n","Frame 291 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.26, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Monitor/Screen\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  96%|█████████▌| 287/300 [07:44<00:21,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 292 visualization saved to ./attention_test_results/frame_292_attention.png\n","\n","Frame 292 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.30, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.28, Looking at: Monitor/Screen\n","  Person 7: Score=0.24, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.07, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Monitor/Screen\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  96%|█████████▌| 288/300 [07:45<00:19,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 293 visualization saved to ./attention_test_results/frame_293_attention.png\n","\n","Frame 293 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.25, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.25, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Monitor/Screen\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  96%|█████████▋| 289/300 [07:47<00:17,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 294 visualization saved to ./attention_test_results/frame_294_attention.png\n","\n","Frame 294 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.22, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.14, Looking at: Monitor/Screen\n","  Person 10: Score=0.16, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Monitor/Screen\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  97%|█████████▋| 290/300 [07:49<00:15,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 295 visualization saved to ./attention_test_results/frame_295_attention.png\n","\n","Frame 295 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.29, Looking at: Monitor/Screen\n","  Person 5: Score=0.22, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.21, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.15, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Monitor/Screen\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  97%|█████████▋| 291/300 [07:50<00:14,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 296 visualization saved to ./attention_test_results/frame_296_attention.png\n","\n","Frame 296 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.20\n","- Most attended object: Monitor/Screen (13 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.23, Looking at: Monitor/Screen\n","  Person 4: Score=0.31, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.21, Looking at: Monitor/Screen\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.15, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Monitor/Screen\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  97%|█████████▋| 292/300 [07:52<00:12,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 297 visualization saved to ./attention_test_results/frame_297_attention.png\n","\n","Frame 297 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.17, Looking at: Person/Teacher\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.11, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Monitor/Screen\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  98%|█████████▊| 293/300 [07:53<00:11,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 298 visualization saved to ./attention_test_results/frame_298_attention.png\n","\n","Frame 298 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (12 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.15, Looking at: Person/Teacher\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.09, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Monitor/Screen\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  98%|█████████▊| 294/300 [07:55<00:09,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 299 visualization saved to ./attention_test_results/frame_299_attention.png\n","\n","Frame 299 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.19\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.13, Looking at: Person/Teacher\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.08, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.06, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  98%|█████████▊| 295/300 [07:57<00:07,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 300 visualization saved to ./attention_test_results/frame_300_attention.png\n","\n","Frame 300 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.10, Looking at: Person/Teacher\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.05, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  99%|█████████▊| 296/300 [07:58<00:06,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 301 visualization saved to ./attention_test_results/frame_301_attention.png\n","\n","Frame 301 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (11 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.09, Looking at: Person/Teacher\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Monitor/Screen\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  99%|█████████▉| 297/300 [08:00<00:04,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 302 visualization saved to ./attention_test_results/frame_302_attention.png\n","\n","Frame 302 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.19, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.24, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.08, Looking at: Person/Teacher\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.02, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames:  99%|█████████▉| 298/300 [08:02<00:03,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 303 visualization saved to ./attention_test_results/frame_303_attention.png\n","\n","Frame 303 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.25, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.08, Looking at: Person/Teacher\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing frames: 100%|█████████▉| 299/300 [08:03<00:01,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 304 visualization saved to ./attention_test_results/frame_304_attention.png\n","\n","Frame 304 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.25, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.08, Looking at: Person/Teacher\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.04, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.05, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["Analyzing frames: 100%|██████████| 300/300 [08:05<00:00,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Frame 305 visualization saved to ./attention_test_results/frame_305_attention.png\n","\n","Frame 305 Analysis Summary:\n","- Total people: 14\n","- Mean attention score: 0.18\n","- Most attended object: Monitor/Screen (10 people)\n","- Individual scores:\n","  Person 0: Score=0.26, Looking at: Monitor/Screen\n","  Person 1: Score=0.24, Looking at: Monitor/Screen\n","  Person 2: Score=0.18, Looking at: Monitor/Screen\n","  Person 3: Score=0.22, Looking at: Monitor/Screen\n","  Person 4: Score=0.32, Looking at: Monitor/Screen\n","  Person 5: Score=0.25, Looking at: Monitor/Screen\n","  Person 6: Score=0.29, Looking at: Monitor/Screen\n","  Person 7: Score=0.08, Looking at: Person/Teacher\n","  Person 8: Score=0.26, Looking at: Monitor/Screen\n","  Person 9: Score=0.05, Looking at: Person/Teacher\n","  Person 10: Score=0.17, Looking at: Monitor/Screen\n","  Person 11: Score=0.15, Looking at: Monitor/Screen\n","  Person 12: Score=0.04, Looking at: Person/Teacher\n","  Person 13: Score=0.00, Looking at: Person/Teacher\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Temporal analysis visualization saved to ./attention_test_results/attention_over_time.png\n","Attention stability: 0.008\n","Detected 0 major attention shifts\n","\n","Attention analysis testing complete!\n"]}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"id":"t1TrsDH--X2V"},"outputs":[],"execution_count":18}]}