{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11040930,
          "sourceType": "datasetVersion",
          "datasetId": 6877343
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "thebrokenvessel_gescam_partial_path = kagglehub.dataset_download('thebrokenvessel/gescam-partial')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "YX64n2c1eLK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9edeb2d-2021-4a32-d9ec-f89e1672ba05"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/thebrokenvessel/gescam-partial?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7.75G/7.75G [06:01<00:00, 23.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTS"
      ],
      "metadata": {
        "id": "ahkCTbQdeLK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import random\n",
        "import torchvision.models as models\n",
        "import math\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import bisect\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
        "print(\"CUDA device count:\", torch.cuda.device_count())\n",
        "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T09:29:54.936454Z",
          "iopub.execute_input": "2025-04-17T09:29:54.936812Z",
          "iopub.status.idle": "2025-04-17T09:29:54.941659Z",
          "shell.execute_reply.started": "2025-04-17T09:29:54.936792Z",
          "shell.execute_reply": "2025-04-17T09:29:54.940922Z"
        },
        "id": "kQJVbCRreLK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e531f2b-0469-477c-939d-7602fa59d764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available? True\n",
            "CUDA device count: 1\n",
            "CUDA device name: Tesla T4\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir('.'))\n",
        "print(thebrokenvessel_gescam_partial_path)\n",
        "\n",
        "import os\n",
        "\n",
        "# List top-level files and directories\n",
        "for root, dirs, files in os.walk(thebrokenvessel_gescam_partial_path):\n",
        "    print(\"Directory:\", root)\n",
        "    for name in files:\n",
        "        print(\"  File:\", name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p739Lb2gNaP",
        "outputId": "d029e752-0043-46da-91c8-9f55e803e8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'sample_data']\n",
            "/root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset/task_classroom_11_video-01_final\n",
            "  File: annotations.xml\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset/task_classroom_11_video-01_final/images\n",
            "  File: frame_000317.PNG\n",
            "  File: frame_000387.PNG\n",
            "  File: frame_000059.PNG\n",
            "  File: frame_000350.PNG\n",
            "  File: frame_000031.PNG\n",
            "  File: frame_000127.PNG\n",
            "  File: frame_000165.PNG\n",
            "  File: frame_000357.PNG\n",
            "  File: frame_000562.PNG\n",
            "  File: frame_000007.PNG\n",
            "  File: frame_000186.PNG\n",
            "  File: frame_000437.PNG\n",
            "  File: frame_000166.PNG\n",
            "  File: frame_000220.PNG\n",
            "  File: frame_000414.PNG\n",
            "  File: frame_000235.PNG\n",
            "  File: frame_000441.PNG\n",
            "  File: frame_000229.PNG\n",
            "  File: frame_000384.PNG\n",
            "  File: frame_000051.PNG\n",
            "  File: frame_000183.PNG\n",
            "  File: frame_000107.PNG\n",
            "  File: frame_000307.PNG\n",
            "  File: frame_000056.PNG\n",
            "  File: frame_000019.PNG\n",
            "  File: frame_000394.PNG\n",
            "  File: frame_000482.PNG\n",
            "  File: frame_000295.PNG\n",
            "  File: frame_000557.PNG\n",
            "  File: frame_000190.PNG\n",
            "  File: frame_000481.PNG\n",
            "  File: frame_000428.PNG\n",
            "  File: frame_000503.PNG\n",
            "  File: frame_000013.PNG\n",
            "  File: frame_000368.PNG\n",
            "  File: frame_000164.PNG\n",
            "  File: frame_000161.PNG\n",
            "  File: frame_000270.PNG\n",
            "  File: frame_000155.PNG\n",
            "  File: frame_000195.PNG\n",
            "  File: frame_000513.PNG\n",
            "  File: frame_000462.PNG\n",
            "  File: frame_000554.PNG\n",
            "  File: frame_000388.PNG\n",
            "  File: frame_000010.PNG\n",
            "  File: frame_000537.PNG\n",
            "  File: frame_000016.PNG\n",
            "  File: frame_000365.PNG\n",
            "  File: frame_000158.PNG\n",
            "  File: frame_000457.PNG\n",
            "  File: frame_000442.PNG\n",
            "  File: frame_000540.PNG\n",
            "  File: frame_000201.PNG\n",
            "  File: frame_000312.PNG\n",
            "  File: frame_000204.PNG\n",
            "  File: frame_000321.PNG\n",
            "  File: frame_000264.PNG\n",
            "  File: frame_000042.PNG\n",
            "  File: frame_000163.PNG\n",
            "  File: frame_000492.PNG\n",
            "  File: frame_000345.PNG\n",
            "  File: frame_000097.PNG\n",
            "  File: frame_000342.PNG\n",
            "  File: frame_000262.PNG\n",
            "  File: frame_000449.PNG\n",
            "  File: frame_000242.PNG\n",
            "  File: frame_000523.PNG\n",
            "  File: frame_000292.PNG\n",
            "  File: frame_000580.PNG\n",
            "  File: frame_000030.PNG\n",
            "  File: frame_000398.PNG\n",
            "  File: frame_000445.PNG\n",
            "  File: frame_000291.PNG\n",
            "  File: frame_000459.PNG\n",
            "  File: frame_000566.PNG\n",
            "  File: frame_000392.PNG\n",
            "  File: frame_000372.PNG\n",
            "  File: frame_000249.PNG\n",
            "  File: frame_000531.PNG\n",
            "  File: frame_000478.PNG\n",
            "  File: frame_000570.PNG\n",
            "  File: frame_000569.PNG\n",
            "  File: frame_000208.PNG\n",
            "  File: frame_000025.PNG\n",
            "  File: frame_000397.PNG\n",
            "  File: frame_000101.PNG\n",
            "  File: frame_000058.PNG\n",
            "  File: frame_000451.PNG\n",
            "  File: frame_000174.PNG\n",
            "  File: frame_000055.PNG\n",
            "  File: frame_000578.PNG\n",
            "  File: frame_000062.PNG\n",
            "  File: frame_000257.PNG\n",
            "  File: frame_000474.PNG\n",
            "  File: frame_000416.PNG\n",
            "  File: frame_000447.PNG\n",
            "  File: frame_000584.PNG\n",
            "  File: frame_000339.PNG\n",
            "  File: frame_000093.PNG\n",
            "  File: frame_000200.PNG\n",
            "  File: frame_000373.PNG\n",
            "  File: frame_000021.PNG\n",
            "  File: frame_000433.PNG\n",
            "  File: frame_000422.PNG\n",
            "  File: frame_000571.PNG\n",
            "  File: frame_000468.PNG\n",
            "  File: frame_000098.PNG\n",
            "  File: frame_000329.PNG\n",
            "  File: frame_000121.PNG\n",
            "  File: frame_000549.PNG\n",
            "  File: frame_000194.PNG\n",
            "  File: frame_000133.PNG\n",
            "  File: frame_000532.PNG\n",
            "  File: frame_000594.PNG\n",
            "  File: frame_000486.PNG\n",
            "  File: frame_000128.PNG\n",
            "  File: frame_000325.PNG\n",
            "  File: frame_000559.PNG\n",
            "  File: frame_000499.PNG\n",
            "  File: frame_000111.PNG\n",
            "  File: frame_000152.PNG\n",
            "  File: frame_000579.PNG\n",
            "  File: frame_000389.PNG\n",
            "  File: frame_000348.PNG\n",
            "  File: frame_000074.PNG\n",
            "  File: frame_000341.PNG\n",
            "  File: frame_000575.PNG\n",
            "  File: frame_000284.PNG\n",
            "  File: frame_000300.PNG\n",
            "  File: frame_000147.PNG\n",
            "  File: frame_000224.PNG\n",
            "  File: frame_000581.PNG\n",
            "  File: frame_000246.PNG\n",
            "  File: frame_000091.PNG\n",
            "  File: frame_000006.PNG\n",
            "  File: frame_000071.PNG\n",
            "  File: frame_000489.PNG\n",
            "  File: frame_000497.PNG\n",
            "  File: frame_000484.PNG\n",
            "  File: frame_000167.PNG\n",
            "  File: frame_000000.PNG\n",
            "  File: frame_000146.PNG\n",
            "  File: frame_000079.PNG\n",
            "  File: frame_000227.PNG\n",
            "  File: frame_000027.PNG\n",
            "  File: frame_000278.PNG\n",
            "  File: frame_000214.PNG\n",
            "  File: frame_000400.PNG\n",
            "  File: frame_000407.PNG\n",
            "  File: frame_000483.PNG\n",
            "  File: frame_000525.PNG\n",
            "  File: frame_000272.PNG\n",
            "  File: frame_000145.PNG\n",
            "  File: frame_000142.PNG\n",
            "  File: frame_000598.PNG\n",
            "  File: frame_000057.PNG\n",
            "  File: frame_000077.PNG\n",
            "  File: frame_000126.PNG\n",
            "  File: frame_000287.PNG\n",
            "  File: frame_000223.PNG\n",
            "  File: frame_000241.PNG\n",
            "  File: frame_000383.PNG\n",
            "  File: frame_000544.PNG\n",
            "  File: frame_000411.PNG\n",
            "  File: frame_000120.PNG\n",
            "  File: frame_000360.PNG\n",
            "  File: frame_000363.PNG\n",
            "  File: frame_000547.PNG\n",
            "  File: frame_000305.PNG\n",
            "  File: frame_000269.PNG\n",
            "  File: frame_000218.PNG\n",
            "  File: frame_000353.PNG\n",
            "  File: frame_000413.PNG\n",
            "  File: frame_000524.PNG\n",
            "  File: frame_000216.PNG\n",
            "  File: frame_000546.PNG\n",
            "  File: frame_000364.PNG\n",
            "  File: frame_000084.PNG\n",
            "  File: frame_000382.PNG\n",
            "  File: frame_000427.PNG\n",
            "  File: frame_000541.PNG\n",
            "  File: frame_000015.PNG\n",
            "  File: frame_000265.PNG\n",
            "  File: frame_000426.PNG\n",
            "  File: frame_000247.PNG\n",
            "  File: frame_000170.PNG\n",
            "  File: frame_000401.PNG\n",
            "  File: frame_000066.PNG\n",
            "  File: frame_000334.PNG\n",
            "  File: frame_000294.PNG\n",
            "  File: frame_000005.PNG\n",
            "  File: frame_000424.PNG\n",
            "  File: frame_000410.PNG\n",
            "  File: frame_000327.PNG\n",
            "  File: frame_000168.PNG\n",
            "  File: frame_000036.PNG\n",
            "  File: frame_000518.PNG\n",
            "  File: frame_000333.PNG\n",
            "  File: frame_000054.PNG\n",
            "  File: frame_000205.PNG\n",
            "  File: frame_000012.PNG\n",
            "  File: frame_000385.PNG\n",
            "  File: frame_000543.PNG\n",
            "  File: frame_000316.PNG\n",
            "  File: frame_000276.PNG\n",
            "  File: frame_000162.PNG\n",
            "  File: frame_000040.PNG\n",
            "  File: frame_000153.PNG\n",
            "  File: frame_000336.PNG\n",
            "  File: frame_000435.PNG\n",
            "  File: frame_000337.PNG\n",
            "  File: frame_000490.PNG\n",
            "  File: frame_000349.PNG\n",
            "  File: frame_000215.PNG\n",
            "  File: frame_000421.PNG\n",
            "  File: frame_000225.PNG\n",
            "  File: frame_000534.PNG\n",
            "  File: frame_000453.PNG\n",
            "  File: frame_000100.PNG\n",
            "  File: frame_000347.PNG\n",
            "  File: frame_000359.PNG\n",
            "  File: frame_000473.PNG\n",
            "  File: frame_000043.PNG\n",
            "  File: frame_000245.PNG\n",
            "  File: frame_000574.PNG\n",
            "  File: frame_000178.PNG\n",
            "  File: frame_000436.PNG\n",
            "  File: frame_000495.PNG\n",
            "  File: frame_000045.PNG\n",
            "  File: frame_000326.PNG\n",
            "  File: frame_000099.PNG\n",
            "  File: frame_000285.PNG\n",
            "  File: frame_000230.PNG\n",
            "  File: frame_000289.PNG\n",
            "  File: frame_000252.PNG\n",
            "  File: frame_000548.PNG\n",
            "  File: frame_000500.PNG\n",
            "  File: frame_000419.PNG\n",
            "  File: frame_000470.PNG\n",
            "  File: frame_000431.PNG\n",
            "  File: frame_000582.PNG\n",
            "  File: frame_000446.PNG\n",
            "  File: frame_000564.PNG\n",
            "  File: frame_000355.PNG\n",
            "  File: frame_000124.PNG\n",
            "  File: frame_000085.PNG\n",
            "  File: frame_000240.PNG\n",
            "  File: frame_000048.PNG\n",
            "  File: frame_000366.PNG\n",
            "  File: frame_000092.PNG\n",
            "  File: frame_000314.PNG\n",
            "  File: frame_000530.PNG\n",
            "  File: frame_000035.PNG\n",
            "  File: frame_000504.PNG\n",
            "  File: frame_000297.PNG\n",
            "  File: frame_000271.PNG\n",
            "  File: frame_000263.PNG\n",
            "  File: frame_000460.PNG\n",
            "  File: frame_000452.PNG\n",
            "  File: frame_000237.PNG\n",
            "  File: frame_000197.PNG\n",
            "  File: frame_000439.PNG\n",
            "  File: frame_000583.PNG\n",
            "  File: frame_000563.PNG\n",
            "  File: frame_000313.PNG\n",
            "  File: frame_000420.PNG\n",
            "  File: frame_000207.PNG\n",
            "  File: frame_000209.PNG\n",
            "  File: frame_000448.PNG\n",
            "  File: frame_000374.PNG\n",
            "  File: frame_000494.PNG\n",
            "  File: frame_000080.PNG\n",
            "  File: frame_000279.PNG\n",
            "  File: frame_000050.PNG\n",
            "  File: frame_000381.PNG\n",
            "  File: frame_000182.PNG\n",
            "  File: frame_000180.PNG\n",
            "  File: frame_000156.PNG\n",
            "  File: frame_000096.PNG\n",
            "  File: frame_000467.PNG\n",
            "  File: frame_000123.PNG\n",
            "  File: frame_000075.PNG\n",
            "  File: frame_000461.PNG\n",
            "  File: frame_000356.PNG\n",
            "  File: frame_000432.PNG\n",
            "  File: frame_000296.PNG\n",
            "  File: frame_000138.PNG\n",
            "  File: frame_000501.PNG\n",
            "  File: frame_000267.PNG\n",
            "  File: frame_000553.PNG\n",
            "  File: frame_000538.PNG\n",
            "  File: frame_000106.PNG\n",
            "  File: frame_000496.PNG\n",
            "  File: frame_000371.PNG\n",
            "  File: frame_000135.PNG\n",
            "  File: frame_000324.PNG\n",
            "  File: frame_000037.PNG\n",
            "  File: frame_000202.PNG\n",
            "  File: frame_000067.PNG\n",
            "  File: frame_000136.PNG\n",
            "  File: frame_000192.PNG\n",
            "  File: frame_000568.PNG\n",
            "  File: frame_000094.PNG\n",
            "  File: frame_000014.PNG\n",
            "  File: frame_000226.PNG\n",
            "  File: frame_000132.PNG\n",
            "  File: frame_000185.PNG\n",
            "  File: frame_000274.PNG\n",
            "  File: frame_000118.PNG\n",
            "  File: frame_000331.PNG\n",
            "  File: frame_000256.PNG\n",
            "  File: frame_000177.PNG\n",
            "  File: frame_000102.PNG\n",
            "  File: frame_000196.PNG\n",
            "  File: frame_000510.PNG\n",
            "  File: frame_000376.PNG\n",
            "  File: frame_000286.PNG\n",
            "  File: frame_000114.PNG\n",
            "  File: frame_000032.PNG\n",
            "  File: frame_000469.PNG\n",
            "  File: frame_000064.PNG\n",
            "  File: frame_000551.PNG\n",
            "  File: frame_000151.PNG\n",
            "  File: frame_000089.PNG\n",
            "  File: frame_000299.PNG\n",
            "  File: frame_000144.PNG\n",
            "  File: frame_000068.PNG\n",
            "  File: frame_000176.PNG\n",
            "  File: frame_000399.PNG\n",
            "  File: frame_000560.PNG\n",
            "  File: frame_000328.PNG\n",
            "  File: frame_000443.PNG\n",
            "  File: frame_000017.PNG\n",
            "  File: frame_000450.PNG\n",
            "  File: frame_000358.PNG\n",
            "  File: frame_000028.PNG\n",
            "  File: frame_000298.PNG\n",
            "  File: frame_000576.PNG\n",
            "  File: frame_000029.PNG\n",
            "  File: frame_000522.PNG\n",
            "  File: frame_000592.PNG\n",
            "  File: frame_000018.PNG\n",
            "  File: frame_000515.PNG\n",
            "  File: frame_000184.PNG\n",
            "  File: frame_000046.PNG\n",
            "  File: frame_000310.PNG\n",
            "  File: frame_000507.PNG\n",
            "  File: frame_000228.PNG\n",
            "  File: frame_000595.PNG\n",
            "  File: frame_000456.PNG\n",
            "  File: frame_000217.PNG\n",
            "  File: frame_000423.PNG\n",
            "  File: frame_000108.PNG\n",
            "  File: frame_000527.PNG\n",
            "  File: frame_000408.PNG\n",
            "  File: frame_000517.PNG\n",
            "  File: frame_000160.PNG\n",
            "  File: frame_000505.PNG\n",
            "  File: frame_000198.PNG\n",
            "  File: frame_000082.PNG\n",
            "  File: frame_000567.PNG\n",
            "  File: frame_000390.PNG\n",
            "  File: frame_000268.PNG\n",
            "  File: frame_000370.PNG\n",
            "  File: frame_000378.PNG\n",
            "  File: frame_000319.PNG\n",
            "  File: frame_000233.PNG\n",
            "  File: frame_000509.PNG\n",
            "  File: frame_000219.PNG\n",
            "  File: frame_000585.PNG\n",
            "  File: frame_000206.PNG\n",
            "  File: frame_000116.PNG\n",
            "  File: frame_000589.PNG\n",
            "  File: frame_000231.PNG\n",
            "  File: frame_000070.PNG\n",
            "  File: frame_000072.PNG\n",
            "  File: frame_000260.PNG\n",
            "  File: frame_000379.PNG\n",
            "  File: frame_000137.PNG\n",
            "  File: frame_000266.PNG\n",
            "  File: frame_000154.PNG\n",
            "  File: frame_000377.PNG\n",
            "  File: frame_000105.PNG\n",
            "  File: frame_000596.PNG\n",
            "  File: frame_000288.PNG\n",
            "  File: frame_000514.PNG\n",
            "  File: frame_000588.PNG\n",
            "  File: frame_000253.PNG\n",
            "  File: frame_000303.PNG\n",
            "  File: frame_000526.PNG\n",
            "  File: frame_000022.PNG\n",
            "  File: frame_000332.PNG\n",
            "  File: frame_000572.PNG\n",
            "  File: frame_000181.PNG\n",
            "  File: frame_000311.PNG\n",
            "  File: frame_000076.PNG\n",
            "  File: frame_000239.PNG\n",
            "  File: frame_000191.PNG\n",
            "  File: frame_000533.PNG\n",
            "  File: frame_000148.PNG\n",
            "  File: frame_000140.PNG\n",
            "  File: frame_000409.PNG\n",
            "  File: frame_000211.PNG\n",
            "  File: frame_000344.PNG\n",
            "  File: frame_000238.PNG\n",
            "  File: frame_000404.PNG\n",
            "  File: frame_000315.PNG\n",
            "  File: frame_000512.PNG\n",
            "  File: frame_000060.PNG\n",
            "  File: frame_000222.PNG\n",
            "  File: frame_000425.PNG\n",
            "  File: frame_000047.PNG\n",
            "  File: frame_000487.PNG\n",
            "  File: frame_000258.PNG\n",
            "  File: frame_000369.PNG\n",
            "  File: frame_000234.PNG\n",
            "  File: frame_000438.PNG\n",
            "  File: frame_000304.PNG\n",
            "  File: frame_000440.PNG\n",
            "  File: frame_000110.PNG\n",
            "  File: frame_000081.PNG\n",
            "  File: frame_000386.PNG\n",
            "  File: frame_000415.PNG\n",
            "  File: frame_000485.PNG\n",
            "  File: frame_000122.PNG\n",
            "  File: frame_000212.PNG\n",
            "  File: frame_000213.PNG\n",
            "  File: frame_000466.PNG\n",
            "  File: frame_000412.PNG\n",
            "  File: frame_000558.PNG\n",
            "  File: frame_000171.PNG\n",
            "  File: frame_000088.PNG\n",
            "  File: frame_000125.PNG\n",
            "  File: frame_000573.PNG\n",
            "  File: frame_000506.PNG\n",
            "  File: frame_000033.PNG\n",
            "  File: frame_000539.PNG\n",
            "  File: frame_000024.PNG\n",
            "  File: frame_000362.PNG\n",
            "  File: frame_000508.PNG\n",
            "  File: frame_000354.PNG\n",
            "  File: frame_000464.PNG\n",
            "  File: frame_000323.PNG\n",
            "  File: frame_000254.PNG\n",
            "  File: frame_000023.PNG\n",
            "  File: frame_000472.PNG\n",
            "  File: frame_000463.PNG\n",
            "  File: frame_000293.PNG\n",
            "  File: frame_000479.PNG\n",
            "  File: frame_000248.PNG\n",
            "  File: frame_000444.PNG\n",
            "  File: frame_000519.PNG\n",
            "  File: frame_000396.PNG\n",
            "  File: frame_000521.PNG\n",
            "  File: frame_000338.PNG\n",
            "  File: frame_000049.PNG\n",
            "  File: frame_000073.PNG\n",
            "  File: frame_000346.PNG\n",
            "  File: frame_000038.PNG\n",
            "  File: frame_000112.PNG\n",
            "  File: frame_000134.PNG\n",
            "  File: frame_000275.PNG\n",
            "  File: frame_000095.PNG\n",
            "  File: frame_000020.PNG\n",
            "  File: frame_000078.PNG\n",
            "  File: frame_000301.PNG\n",
            "  File: frame_000117.PNG\n",
            "  File: frame_000380.PNG\n",
            "  File: frame_000302.PNG\n",
            "  File: frame_000555.PNG\n",
            "  File: frame_000157.PNG\n",
            "  File: frame_000277.PNG\n",
            "  File: frame_000173.PNG\n",
            "  File: frame_000041.PNG\n",
            "  File: frame_000493.PNG\n",
            "  File: frame_000221.PNG\n",
            "  File: frame_000393.PNG\n",
            "  File: frame_000113.PNG\n",
            "  File: frame_000290.PNG\n",
            "  File: frame_000535.PNG\n",
            "  File: frame_000003.PNG\n",
            "  File: frame_000283.PNG\n",
            "  File: frame_000261.PNG\n",
            "  File: frame_000520.PNG\n",
            "  File: frame_000034.PNG\n",
            "  File: frame_000434.PNG\n",
            "  File: frame_000232.PNG\n",
            "  File: frame_000069.PNG\n",
            "  File: frame_000542.PNG\n",
            "  File: frame_000511.PNG\n",
            "  File: frame_000193.PNG\n",
            "  File: frame_000087.PNG\n",
            "  File: frame_000402.PNG\n",
            "  File: frame_000139.PNG\n",
            "  File: frame_000280.PNG\n",
            "  File: frame_000143.PNG\n",
            "  File: frame_000063.PNG\n",
            "  File: frame_000405.PNG\n",
            "  File: frame_000597.PNG\n",
            "  File: frame_000250.PNG\n",
            "  File: frame_000361.PNG\n",
            "  File: frame_000545.PNG\n",
            "  File: frame_000476.PNG\n",
            "  File: frame_000340.PNG\n",
            "  File: frame_000281.PNG\n",
            "  File: frame_000150.PNG\n",
            "  File: frame_000308.PNG\n",
            "  File: frame_000528.PNG\n",
            "  File: frame_000053.PNG\n",
            "  File: frame_000172.PNG\n",
            "  File: frame_000430.PNG\n",
            "  File: frame_000236.PNG\n",
            "  File: frame_000011.PNG\n",
            "  File: frame_000417.PNG\n",
            "  File: frame_000282.PNG\n",
            "  File: frame_000090.PNG\n",
            "  File: frame_000586.PNG\n",
            "  File: frame_000488.PNG\n",
            "  File: frame_000318.PNG\n",
            "  File: frame_000471.PNG\n",
            "  File: frame_000395.PNG\n",
            "  File: frame_000009.PNG\n",
            "  File: frame_000335.PNG\n",
            "  File: frame_000577.PNG\n",
            "  File: frame_000454.PNG\n",
            "  File: frame_000086.PNG\n",
            "  File: frame_000343.PNG\n",
            "  File: frame_000129.PNG\n",
            "  File: frame_000491.PNG\n",
            "  File: frame_000465.PNG\n",
            "  File: frame_000429.PNG\n",
            "  File: frame_000187.PNG\n",
            "  File: frame_000403.PNG\n",
            "  File: frame_000131.PNG\n",
            "  File: frame_000169.PNG\n",
            "  File: frame_000199.PNG\n",
            "  File: frame_000044.PNG\n",
            "  File: frame_000061.PNG\n",
            "  File: frame_000587.PNG\n",
            "  File: frame_000406.PNG\n",
            "  File: frame_000498.PNG\n",
            "  File: frame_000026.PNG\n",
            "  File: frame_000367.PNG\n",
            "  File: frame_000001.PNG\n",
            "  File: frame_000004.PNG\n",
            "  File: frame_000352.PNG\n",
            "  File: frame_000109.PNG\n",
            "  File: frame_000251.PNG\n",
            "  File: frame_000455.PNG\n",
            "  File: frame_000593.PNG\n",
            "  File: frame_000259.PNG\n",
            "  File: frame_000320.PNG\n",
            "  File: frame_000203.PNG\n",
            "  File: frame_000243.PNG\n",
            "  File: frame_000309.PNG\n",
            "  File: frame_000175.PNG\n",
            "  File: frame_000052.PNG\n",
            "  File: frame_000475.PNG\n",
            "  File: frame_000159.PNG\n",
            "  File: frame_000590.PNG\n",
            "  File: frame_000002.PNG\n",
            "  File: frame_000188.PNG\n",
            "  File: frame_000550.PNG\n",
            "  File: frame_000552.PNG\n",
            "  File: frame_000179.PNG\n",
            "  File: frame_000210.PNG\n",
            "  File: frame_000149.PNG\n",
            "  File: frame_000561.PNG\n",
            "  File: frame_000458.PNG\n",
            "  File: frame_000556.PNG\n",
            "  File: frame_000115.PNG\n",
            "  File: frame_000502.PNG\n",
            "  File: frame_000119.PNG\n",
            "  File: frame_000565.PNG\n",
            "  File: frame_000330.PNG\n",
            "  File: frame_000480.PNG\n",
            "  File: frame_000418.PNG\n",
            "  File: frame_000306.PNG\n",
            "  File: frame_000130.PNG\n",
            "  File: frame_000477.PNG\n",
            "  File: frame_000189.PNG\n",
            "  File: frame_000591.PNG\n",
            "  File: frame_000065.PNG\n",
            "  File: frame_000273.PNG\n",
            "  File: frame_000375.PNG\n",
            "  File: frame_000391.PNG\n",
            "  File: frame_000083.PNG\n",
            "  File: frame_000351.PNG\n",
            "  File: frame_000103.PNG\n",
            "  File: frame_000529.PNG\n",
            "  File: frame_000536.PNG\n",
            "  File: frame_000104.PNG\n",
            "  File: frame_000255.PNG\n",
            "  File: frame_000244.PNG\n",
            "  File: frame_000322.PNG\n",
            "  File: frame_000008.PNG\n",
            "  File: frame_000141.PNG\n",
            "  File: frame_000516.PNG\n",
            "  File: frame_000039.PNG\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video03_final\n",
            "  File: annotations.xml\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video03_final/images\n",
            "  File: frame_000317.PNG\n",
            "  File: frame_000387.PNG\n",
            "  File: frame_000059.PNG\n",
            "  File: frame_000350.PNG\n",
            "  File: frame_000031.PNG\n",
            "  File: frame_000127.PNG\n",
            "  File: frame_000165.PNG\n",
            "  File: frame_000357.PNG\n",
            "  File: frame_000562.PNG\n",
            "  File: frame_000007.PNG\n",
            "  File: frame_000186.PNG\n",
            "  File: frame_000437.PNG\n",
            "  File: frame_000166.PNG\n",
            "  File: frame_000220.PNG\n",
            "  File: frame_000414.PNG\n",
            "  File: frame_000235.PNG\n",
            "  File: frame_000441.PNG\n",
            "  File: frame_000229.PNG\n",
            "  File: frame_000384.PNG\n",
            "  File: frame_000051.PNG\n",
            "  File: frame_000183.PNG\n",
            "  File: frame_000107.PNG\n",
            "  File: frame_000307.PNG\n",
            "  File: frame_000056.PNG\n",
            "  File: frame_000019.PNG\n",
            "  File: frame_000394.PNG\n",
            "  File: frame_000482.PNG\n",
            "  File: frame_000295.PNG\n",
            "  File: frame_000557.PNG\n",
            "  File: frame_000190.PNG\n",
            "  File: frame_000481.PNG\n",
            "  File: frame_000428.PNG\n",
            "  File: frame_000503.PNG\n",
            "  File: frame_000013.PNG\n",
            "  File: frame_000368.PNG\n",
            "  File: frame_000164.PNG\n",
            "  File: frame_000161.PNG\n",
            "  File: frame_000270.PNG\n",
            "  File: frame_000155.PNG\n",
            "  File: frame_000195.PNG\n",
            "  File: frame_000513.PNG\n",
            "  File: frame_000462.PNG\n",
            "  File: frame_000554.PNG\n",
            "  File: frame_000388.PNG\n",
            "  File: frame_000010.PNG\n",
            "  File: frame_000537.PNG\n",
            "  File: frame_000016.PNG\n",
            "  File: frame_000365.PNG\n",
            "  File: frame_000158.PNG\n",
            "  File: frame_000457.PNG\n",
            "  File: frame_000442.PNG\n",
            "  File: frame_000540.PNG\n",
            "  File: frame_000201.PNG\n",
            "  File: frame_000312.PNG\n",
            "  File: frame_000204.PNG\n",
            "  File: frame_000321.PNG\n",
            "  File: frame_000264.PNG\n",
            "  File: frame_000042.PNG\n",
            "  File: frame_000163.PNG\n",
            "  File: frame_000492.PNG\n",
            "  File: frame_000345.PNG\n",
            "  File: frame_000097.PNG\n",
            "  File: frame_000342.PNG\n",
            "  File: frame_000262.PNG\n",
            "  File: frame_000449.PNG\n",
            "  File: frame_000242.PNG\n",
            "  File: frame_000523.PNG\n",
            "  File: frame_000292.PNG\n",
            "  File: frame_000580.PNG\n",
            "  File: frame_000030.PNG\n",
            "  File: frame_000398.PNG\n",
            "  File: frame_000445.PNG\n",
            "  File: frame_000291.PNG\n",
            "  File: frame_000459.PNG\n",
            "  File: frame_000566.PNG\n",
            "  File: frame_000392.PNG\n",
            "  File: frame_000372.PNG\n",
            "  File: frame_000249.PNG\n",
            "  File: frame_000531.PNG\n",
            "  File: frame_000478.PNG\n",
            "  File: frame_000570.PNG\n",
            "  File: frame_000569.PNG\n",
            "  File: frame_000208.PNG\n",
            "  File: frame_000025.PNG\n",
            "  File: frame_000397.PNG\n",
            "  File: frame_000101.PNG\n",
            "  File: frame_000058.PNG\n",
            "  File: frame_000451.PNG\n",
            "  File: frame_000174.PNG\n",
            "  File: frame_000055.PNG\n",
            "  File: frame_000578.PNG\n",
            "  File: frame_000062.PNG\n",
            "  File: frame_000257.PNG\n",
            "  File: frame_000474.PNG\n",
            "  File: frame_000416.PNG\n",
            "  File: frame_000447.PNG\n",
            "  File: frame_000584.PNG\n",
            "  File: frame_000339.PNG\n",
            "  File: frame_000093.PNG\n",
            "  File: frame_000200.PNG\n",
            "  File: frame_000373.PNG\n",
            "  File: frame_000021.PNG\n",
            "  File: frame_000433.PNG\n",
            "  File: frame_000422.PNG\n",
            "  File: frame_000571.PNG\n",
            "  File: frame_000468.PNG\n",
            "  File: frame_000098.PNG\n",
            "  File: frame_000329.PNG\n",
            "  File: frame_000121.PNG\n",
            "  File: frame_000549.PNG\n",
            "  File: frame_000194.PNG\n",
            "  File: frame_000133.PNG\n",
            "  File: frame_000532.PNG\n",
            "  File: frame_000594.PNG\n",
            "  File: frame_000486.PNG\n",
            "  File: frame_000128.PNG\n",
            "  File: frame_000325.PNG\n",
            "  File: frame_000559.PNG\n",
            "  File: frame_000499.PNG\n",
            "  File: frame_000111.PNG\n",
            "  File: frame_000152.PNG\n",
            "  File: frame_000579.PNG\n",
            "  File: frame_000389.PNG\n",
            "  File: frame_000348.PNG\n",
            "  File: frame_000074.PNG\n",
            "  File: frame_000341.PNG\n",
            "  File: frame_000575.PNG\n",
            "  File: frame_000284.PNG\n",
            "  File: frame_000300.PNG\n",
            "  File: frame_000147.PNG\n",
            "  File: frame_000224.PNG\n",
            "  File: frame_000581.PNG\n",
            "  File: frame_000246.PNG\n",
            "  File: frame_000091.PNG\n",
            "  File: frame_000006.PNG\n",
            "  File: frame_000071.PNG\n",
            "  File: frame_000489.PNG\n",
            "  File: frame_000497.PNG\n",
            "  File: frame_000484.PNG\n",
            "  File: frame_000167.PNG\n",
            "  File: frame_000000.PNG\n",
            "  File: frame_000146.PNG\n",
            "  File: frame_000079.PNG\n",
            "  File: frame_000227.PNG\n",
            "  File: frame_000027.PNG\n",
            "  File: frame_000278.PNG\n",
            "  File: frame_000214.PNG\n",
            "  File: frame_000400.PNG\n",
            "  File: frame_000407.PNG\n",
            "  File: frame_000483.PNG\n",
            "  File: frame_000525.PNG\n",
            "  File: frame_000272.PNG\n",
            "  File: frame_000145.PNG\n",
            "  File: frame_000142.PNG\n",
            "  File: frame_000598.PNG\n",
            "  File: frame_000057.PNG\n",
            "  File: frame_000077.PNG\n",
            "  File: frame_000126.PNG\n",
            "  File: frame_000287.PNG\n",
            "  File: frame_000223.PNG\n",
            "  File: frame_000241.PNG\n",
            "  File: frame_000383.PNG\n",
            "  File: frame_000544.PNG\n",
            "  File: frame_000411.PNG\n",
            "  File: frame_000120.PNG\n",
            "  File: frame_000360.PNG\n",
            "  File: frame_000363.PNG\n",
            "  File: frame_000547.PNG\n",
            "  File: frame_000305.PNG\n",
            "  File: frame_000269.PNG\n",
            "  File: frame_000218.PNG\n",
            "  File: frame_000353.PNG\n",
            "  File: frame_000413.PNG\n",
            "  File: frame_000524.PNG\n",
            "  File: frame_000216.PNG\n",
            "  File: frame_000546.PNG\n",
            "  File: frame_000364.PNG\n",
            "  File: frame_000084.PNG\n",
            "  File: frame_000382.PNG\n",
            "  File: frame_000427.PNG\n",
            "  File: frame_000541.PNG\n",
            "  File: frame_000015.PNG\n",
            "  File: frame_000265.PNG\n",
            "  File: frame_000426.PNG\n",
            "  File: frame_000247.PNG\n",
            "  File: frame_000170.PNG\n",
            "  File: frame_000401.PNG\n",
            "  File: frame_000066.PNG\n",
            "  File: frame_000334.PNG\n",
            "  File: frame_000294.PNG\n",
            "  File: frame_000005.PNG\n",
            "  File: frame_000424.PNG\n",
            "  File: frame_000410.PNG\n",
            "  File: frame_000327.PNG\n",
            "  File: frame_000168.PNG\n",
            "  File: frame_000036.PNG\n",
            "  File: frame_000518.PNG\n",
            "  File: frame_000333.PNG\n",
            "  File: frame_000054.PNG\n",
            "  File: frame_000205.PNG\n",
            "  File: frame_000012.PNG\n",
            "  File: frame_000385.PNG\n",
            "  File: frame_000543.PNG\n",
            "  File: frame_000316.PNG\n",
            "  File: frame_000276.PNG\n",
            "  File: frame_000162.PNG\n",
            "  File: frame_000040.PNG\n",
            "  File: frame_000153.PNG\n",
            "  File: frame_000336.PNG\n",
            "  File: frame_000435.PNG\n",
            "  File: frame_000337.PNG\n",
            "  File: frame_000490.PNG\n",
            "  File: frame_000349.PNG\n",
            "  File: frame_000215.PNG\n",
            "  File: frame_000421.PNG\n",
            "  File: frame_000225.PNG\n",
            "  File: frame_000534.PNG\n",
            "  File: frame_000453.PNG\n",
            "  File: frame_000100.PNG\n",
            "  File: frame_000347.PNG\n",
            "  File: frame_000359.PNG\n",
            "  File: frame_000473.PNG\n",
            "  File: frame_000043.PNG\n",
            "  File: frame_000245.PNG\n",
            "  File: frame_000574.PNG\n",
            "  File: frame_000178.PNG\n",
            "  File: frame_000436.PNG\n",
            "  File: frame_000495.PNG\n",
            "  File: frame_000045.PNG\n",
            "  File: frame_000326.PNG\n",
            "  File: frame_000099.PNG\n",
            "  File: frame_000285.PNG\n",
            "  File: frame_000230.PNG\n",
            "  File: frame_000289.PNG\n",
            "  File: frame_000252.PNG\n",
            "  File: frame_000548.PNG\n",
            "  File: frame_000500.PNG\n",
            "  File: frame_000419.PNG\n",
            "  File: frame_000470.PNG\n",
            "  File: frame_000431.PNG\n",
            "  File: frame_000582.PNG\n",
            "  File: frame_000446.PNG\n",
            "  File: frame_000564.PNG\n",
            "  File: frame_000355.PNG\n",
            "  File: frame_000124.PNG\n",
            "  File: frame_000085.PNG\n",
            "  File: frame_000240.PNG\n",
            "  File: frame_000048.PNG\n",
            "  File: frame_000366.PNG\n",
            "  File: frame_000092.PNG\n",
            "  File: frame_000314.PNG\n",
            "  File: frame_000530.PNG\n",
            "  File: frame_000035.PNG\n",
            "  File: frame_000504.PNG\n",
            "  File: frame_000297.PNG\n",
            "  File: frame_000271.PNG\n",
            "  File: frame_000263.PNG\n",
            "  File: frame_000460.PNG\n",
            "  File: frame_000452.PNG\n",
            "  File: frame_000237.PNG\n",
            "  File: frame_000600.PNG\n",
            "  File: frame_000197.PNG\n",
            "  File: frame_000439.PNG\n",
            "  File: frame_000583.PNG\n",
            "  File: frame_000563.PNG\n",
            "  File: frame_000313.PNG\n",
            "  File: frame_000420.PNG\n",
            "  File: frame_000207.PNG\n",
            "  File: frame_000209.PNG\n",
            "  File: frame_000448.PNG\n",
            "  File: frame_000374.PNG\n",
            "  File: frame_000494.PNG\n",
            "  File: frame_000080.PNG\n",
            "  File: frame_000279.PNG\n",
            "  File: frame_000050.PNG\n",
            "  File: frame_000381.PNG\n",
            "  File: frame_000182.PNG\n",
            "  File: frame_000180.PNG\n",
            "  File: frame_000156.PNG\n",
            "  File: frame_000096.PNG\n",
            "  File: frame_000467.PNG\n",
            "  File: frame_000123.PNG\n",
            "  File: frame_000075.PNG\n",
            "  File: frame_000461.PNG\n",
            "  File: frame_000356.PNG\n",
            "  File: frame_000432.PNG\n",
            "  File: frame_000296.PNG\n",
            "  File: frame_000138.PNG\n",
            "  File: frame_000501.PNG\n",
            "  File: frame_000267.PNG\n",
            "  File: frame_000553.PNG\n",
            "  File: frame_000538.PNG\n",
            "  File: frame_000106.PNG\n",
            "  File: frame_000496.PNG\n",
            "  File: frame_000371.PNG\n",
            "  File: frame_000135.PNG\n",
            "  File: frame_000324.PNG\n",
            "  File: frame_000037.PNG\n",
            "  File: frame_000202.PNG\n",
            "  File: frame_000067.PNG\n",
            "  File: frame_000136.PNG\n",
            "  File: frame_000192.PNG\n",
            "  File: frame_000568.PNG\n",
            "  File: frame_000094.PNG\n",
            "  File: frame_000014.PNG\n",
            "  File: frame_000226.PNG\n",
            "  File: frame_000132.PNG\n",
            "  File: frame_000185.PNG\n",
            "  File: frame_000274.PNG\n",
            "  File: frame_000118.PNG\n",
            "  File: frame_000331.PNG\n",
            "  File: frame_000256.PNG\n",
            "  File: frame_000177.PNG\n",
            "  File: frame_000102.PNG\n",
            "  File: frame_000196.PNG\n",
            "  File: frame_000510.PNG\n",
            "  File: frame_000376.PNG\n",
            "  File: frame_000286.PNG\n",
            "  File: frame_000114.PNG\n",
            "  File: frame_000032.PNG\n",
            "  File: frame_000469.PNG\n",
            "  File: frame_000064.PNG\n",
            "  File: frame_000551.PNG\n",
            "  File: frame_000151.PNG\n",
            "  File: frame_000089.PNG\n",
            "  File: frame_000299.PNG\n",
            "  File: frame_000144.PNG\n",
            "  File: frame_000068.PNG\n",
            "  File: frame_000176.PNG\n",
            "  File: frame_000399.PNG\n",
            "  File: frame_000560.PNG\n",
            "  File: frame_000328.PNG\n",
            "  File: frame_000443.PNG\n",
            "  File: frame_000017.PNG\n",
            "  File: frame_000450.PNG\n",
            "  File: frame_000358.PNG\n",
            "  File: frame_000028.PNG\n",
            "  File: frame_000298.PNG\n",
            "  File: frame_000576.PNG\n",
            "  File: frame_000029.PNG\n",
            "  File: frame_000522.PNG\n",
            "  File: frame_000592.PNG\n",
            "  File: frame_000018.PNG\n",
            "  File: frame_000515.PNG\n",
            "  File: frame_000184.PNG\n",
            "  File: frame_000046.PNG\n",
            "  File: frame_000310.PNG\n",
            "  File: frame_000507.PNG\n",
            "  File: frame_000228.PNG\n",
            "  File: frame_000595.PNG\n",
            "  File: frame_000456.PNG\n",
            "  File: frame_000217.PNG\n",
            "  File: frame_000423.PNG\n",
            "  File: frame_000108.PNG\n",
            "  File: frame_000527.PNG\n",
            "  File: frame_000408.PNG\n",
            "  File: frame_000517.PNG\n",
            "  File: frame_000160.PNG\n",
            "  File: frame_000505.PNG\n",
            "  File: frame_000198.PNG\n",
            "  File: frame_000082.PNG\n",
            "  File: frame_000567.PNG\n",
            "  File: frame_000390.PNG\n",
            "  File: frame_000268.PNG\n",
            "  File: frame_000370.PNG\n",
            "  File: frame_000378.PNG\n",
            "  File: frame_000319.PNG\n",
            "  File: frame_000233.PNG\n",
            "  File: frame_000509.PNG\n",
            "  File: frame_000219.PNG\n",
            "  File: frame_000585.PNG\n",
            "  File: frame_000206.PNG\n",
            "  File: frame_000116.PNG\n",
            "  File: frame_000589.PNG\n",
            "  File: frame_000231.PNG\n",
            "  File: frame_000070.PNG\n",
            "  File: frame_000072.PNG\n",
            "  File: frame_000260.PNG\n",
            "  File: frame_000379.PNG\n",
            "  File: frame_000137.PNG\n",
            "  File: frame_000266.PNG\n",
            "  File: frame_000154.PNG\n",
            "  File: frame_000377.PNG\n",
            "  File: frame_000105.PNG\n",
            "  File: frame_000596.PNG\n",
            "  File: frame_000288.PNG\n",
            "  File: frame_000514.PNG\n",
            "  File: frame_000588.PNG\n",
            "  File: frame_000253.PNG\n",
            "  File: frame_000303.PNG\n",
            "  File: frame_000526.PNG\n",
            "  File: frame_000022.PNG\n",
            "  File: frame_000332.PNG\n",
            "  File: frame_000572.PNG\n",
            "  File: frame_000181.PNG\n",
            "  File: frame_000311.PNG\n",
            "  File: frame_000076.PNG\n",
            "  File: frame_000239.PNG\n",
            "  File: frame_000191.PNG\n",
            "  File: frame_000533.PNG\n",
            "  File: frame_000148.PNG\n",
            "  File: frame_000140.PNG\n",
            "  File: frame_000409.PNG\n",
            "  File: frame_000211.PNG\n",
            "  File: frame_000344.PNG\n",
            "  File: frame_000238.PNG\n",
            "  File: frame_000404.PNG\n",
            "  File: frame_000315.PNG\n",
            "  File: frame_000512.PNG\n",
            "  File: frame_000060.PNG\n",
            "  File: frame_000222.PNG\n",
            "  File: frame_000425.PNG\n",
            "  File: frame_000047.PNG\n",
            "  File: frame_000487.PNG\n",
            "  File: frame_000258.PNG\n",
            "  File: frame_000369.PNG\n",
            "  File: frame_000234.PNG\n",
            "  File: frame_000438.PNG\n",
            "  File: frame_000304.PNG\n",
            "  File: frame_000440.PNG\n",
            "  File: frame_000110.PNG\n",
            "  File: frame_000081.PNG\n",
            "  File: frame_000386.PNG\n",
            "  File: frame_000415.PNG\n",
            "  File: frame_000485.PNG\n",
            "  File: frame_000122.PNG\n",
            "  File: frame_000212.PNG\n",
            "  File: frame_000213.PNG\n",
            "  File: frame_000466.PNG\n",
            "  File: frame_000412.PNG\n",
            "  File: frame_000558.PNG\n",
            "  File: frame_000171.PNG\n",
            "  File: frame_000088.PNG\n",
            "  File: frame_000125.PNG\n",
            "  File: frame_000573.PNG\n",
            "  File: frame_000506.PNG\n",
            "  File: frame_000033.PNG\n",
            "  File: frame_000539.PNG\n",
            "  File: frame_000024.PNG\n",
            "  File: frame_000362.PNG\n",
            "  File: frame_000508.PNG\n",
            "  File: frame_000354.PNG\n",
            "  File: frame_000464.PNG\n",
            "  File: frame_000323.PNG\n",
            "  File: frame_000254.PNG\n",
            "  File: frame_000023.PNG\n",
            "  File: frame_000472.PNG\n",
            "  File: frame_000463.PNG\n",
            "  File: frame_000293.PNG\n",
            "  File: frame_000479.PNG\n",
            "  File: frame_000248.PNG\n",
            "  File: frame_000444.PNG\n",
            "  File: frame_000519.PNG\n",
            "  File: frame_000396.PNG\n",
            "  File: frame_000521.PNG\n",
            "  File: frame_000338.PNG\n",
            "  File: frame_000049.PNG\n",
            "  File: frame_000073.PNG\n",
            "  File: frame_000346.PNG\n",
            "  File: frame_000038.PNG\n",
            "  File: frame_000112.PNG\n",
            "  File: frame_000134.PNG\n",
            "  File: frame_000275.PNG\n",
            "  File: frame_000095.PNG\n",
            "  File: frame_000020.PNG\n",
            "  File: frame_000078.PNG\n",
            "  File: frame_000301.PNG\n",
            "  File: frame_000117.PNG\n",
            "  File: frame_000380.PNG\n",
            "  File: frame_000302.PNG\n",
            "  File: frame_000555.PNG\n",
            "  File: frame_000157.PNG\n",
            "  File: frame_000277.PNG\n",
            "  File: frame_000173.PNG\n",
            "  File: frame_000041.PNG\n",
            "  File: frame_000493.PNG\n",
            "  File: frame_000221.PNG\n",
            "  File: frame_000393.PNG\n",
            "  File: frame_000113.PNG\n",
            "  File: frame_000290.PNG\n",
            "  File: frame_000535.PNG\n",
            "  File: frame_000003.PNG\n",
            "  File: frame_000283.PNG\n",
            "  File: frame_000261.PNG\n",
            "  File: frame_000520.PNG\n",
            "  File: frame_000034.PNG\n",
            "  File: frame_000434.PNG\n",
            "  File: frame_000232.PNG\n",
            "  File: frame_000069.PNG\n",
            "  File: frame_000542.PNG\n",
            "  File: frame_000511.PNG\n",
            "  File: frame_000193.PNG\n",
            "  File: frame_000087.PNG\n",
            "  File: frame_000402.PNG\n",
            "  File: frame_000139.PNG\n",
            "  File: frame_000280.PNG\n",
            "  File: frame_000143.PNG\n",
            "  File: frame_000063.PNG\n",
            "  File: frame_000405.PNG\n",
            "  File: frame_000597.PNG\n",
            "  File: frame_000250.PNG\n",
            "  File: frame_000361.PNG\n",
            "  File: frame_000545.PNG\n",
            "  File: frame_000476.PNG\n",
            "  File: frame_000340.PNG\n",
            "  File: frame_000281.PNG\n",
            "  File: frame_000150.PNG\n",
            "  File: frame_000308.PNG\n",
            "  File: frame_000528.PNG\n",
            "  File: frame_000053.PNG\n",
            "  File: frame_000172.PNG\n",
            "  File: frame_000430.PNG\n",
            "  File: frame_000236.PNG\n",
            "  File: frame_000011.PNG\n",
            "  File: frame_000417.PNG\n",
            "  File: frame_000282.PNG\n",
            "  File: frame_000090.PNG\n",
            "  File: frame_000586.PNG\n",
            "  File: frame_000488.PNG\n",
            "  File: frame_000318.PNG\n",
            "  File: frame_000471.PNG\n",
            "  File: frame_000395.PNG\n",
            "  File: frame_000009.PNG\n",
            "  File: frame_000335.PNG\n",
            "  File: frame_000577.PNG\n",
            "  File: frame_000454.PNG\n",
            "  File: frame_000086.PNG\n",
            "  File: frame_000343.PNG\n",
            "  File: frame_000129.PNG\n",
            "  File: frame_000491.PNG\n",
            "  File: frame_000465.PNG\n",
            "  File: frame_000429.PNG\n",
            "  File: frame_000187.PNG\n",
            "  File: frame_000403.PNG\n",
            "  File: frame_000131.PNG\n",
            "  File: frame_000169.PNG\n",
            "  File: frame_000199.PNG\n",
            "  File: frame_000044.PNG\n",
            "  File: frame_000061.PNG\n",
            "  File: frame_000587.PNG\n",
            "  File: frame_000406.PNG\n",
            "  File: frame_000498.PNG\n",
            "  File: frame_000026.PNG\n",
            "  File: frame_000367.PNG\n",
            "  File: frame_000001.PNG\n",
            "  File: frame_000004.PNG\n",
            "  File: frame_000352.PNG\n",
            "  File: frame_000109.PNG\n",
            "  File: frame_000251.PNG\n",
            "  File: frame_000455.PNG\n",
            "  File: frame_000593.PNG\n",
            "  File: frame_000259.PNG\n",
            "  File: frame_000320.PNG\n",
            "  File: frame_000203.PNG\n",
            "  File: frame_000243.PNG\n",
            "  File: frame_000309.PNG\n",
            "  File: frame_000175.PNG\n",
            "  File: frame_000052.PNG\n",
            "  File: frame_000475.PNG\n",
            "  File: frame_000159.PNG\n",
            "  File: frame_000590.PNG\n",
            "  File: frame_000002.PNG\n",
            "  File: frame_000188.PNG\n",
            "  File: frame_000550.PNG\n",
            "  File: frame_000552.PNG\n",
            "  File: frame_000179.PNG\n",
            "  File: frame_000210.PNG\n",
            "  File: frame_000149.PNG\n",
            "  File: frame_000561.PNG\n",
            "  File: frame_000458.PNG\n",
            "  File: frame_000556.PNG\n",
            "  File: frame_000115.PNG\n",
            "  File: frame_000502.PNG\n",
            "  File: frame_000119.PNG\n",
            "  File: frame_000565.PNG\n",
            "  File: frame_000330.PNG\n",
            "  File: frame_000480.PNG\n",
            "  File: frame_000418.PNG\n",
            "  File: frame_000306.PNG\n",
            "  File: frame_000130.PNG\n",
            "  File: frame_000477.PNG\n",
            "  File: frame_000189.PNG\n",
            "  File: frame_000591.PNG\n",
            "  File: frame_000065.PNG\n",
            "  File: frame_000273.PNG\n",
            "  File: frame_000375.PNG\n",
            "  File: frame_000391.PNG\n",
            "  File: frame_000083.PNG\n",
            "  File: frame_000351.PNG\n",
            "  File: frame_000103.PNG\n",
            "  File: frame_000529.PNG\n",
            "  File: frame_000536.PNG\n",
            "  File: frame_000104.PNG\n",
            "  File: frame_000255.PNG\n",
            "  File: frame_000244.PNG\n",
            "  File: frame_000322.PNG\n",
            "  File: frame_000599.PNG\n",
            "  File: frame_000008.PNG\n",
            "  File: frame_000141.PNG\n",
            "  File: frame_000516.PNG\n",
            "  File: frame_000039.PNG\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video04_final\n",
            "  File: annotations.xml\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video04_final/images\n",
            "  File: frame_000317.PNG\n",
            "  File: frame_000387.PNG\n",
            "  File: frame_000059.PNG\n",
            "  File: frame_000350.PNG\n",
            "  File: frame_000031.PNG\n",
            "  File: frame_000127.PNG\n",
            "  File: frame_000165.PNG\n",
            "  File: frame_000357.PNG\n",
            "  File: frame_000562.PNG\n",
            "  File: frame_000007.PNG\n",
            "  File: frame_000186.PNG\n",
            "  File: frame_000437.PNG\n",
            "  File: frame_000166.PNG\n",
            "  File: frame_000220.PNG\n",
            "  File: frame_000414.PNG\n",
            "  File: frame_000235.PNG\n",
            "  File: frame_000441.PNG\n",
            "  File: frame_000229.PNG\n",
            "  File: frame_000384.PNG\n",
            "  File: frame_000051.PNG\n",
            "  File: frame_000183.PNG\n",
            "  File: frame_000107.PNG\n",
            "  File: frame_000307.PNG\n",
            "  File: frame_000056.PNG\n",
            "  File: frame_000019.PNG\n",
            "  File: frame_000394.PNG\n",
            "  File: frame_000482.PNG\n",
            "  File: frame_000295.PNG\n",
            "  File: frame_000557.PNG\n",
            "  File: frame_000190.PNG\n",
            "  File: frame_000481.PNG\n",
            "  File: frame_000428.PNG\n",
            "  File: frame_000503.PNG\n",
            "  File: frame_000013.PNG\n",
            "  File: frame_000368.PNG\n",
            "  File: frame_000164.PNG\n",
            "  File: frame_000161.PNG\n",
            "  File: frame_000270.PNG\n",
            "  File: frame_000155.PNG\n",
            "  File: frame_000195.PNG\n",
            "  File: frame_000513.PNG\n",
            "  File: frame_000462.PNG\n",
            "  File: frame_000554.PNG\n",
            "  File: frame_000388.PNG\n",
            "  File: frame_000010.PNG\n",
            "  File: frame_000537.PNG\n",
            "  File: frame_000016.PNG\n",
            "  File: frame_000365.PNG\n",
            "  File: frame_000158.PNG\n",
            "  File: frame_000457.PNG\n",
            "  File: frame_000442.PNG\n",
            "  File: frame_000540.PNG\n",
            "  File: frame_000201.PNG\n",
            "  File: frame_000312.PNG\n",
            "  File: frame_000204.PNG\n",
            "  File: frame_000321.PNG\n",
            "  File: frame_000264.PNG\n",
            "  File: frame_000042.PNG\n",
            "  File: frame_000163.PNG\n",
            "  File: frame_000492.PNG\n",
            "  File: frame_000345.PNG\n",
            "  File: frame_000097.PNG\n",
            "  File: frame_000342.PNG\n",
            "  File: frame_000262.PNG\n",
            "  File: frame_000449.PNG\n",
            "  File: frame_000242.PNG\n",
            "  File: frame_000523.PNG\n",
            "  File: frame_000292.PNG\n",
            "  File: frame_000580.PNG\n",
            "  File: frame_000030.PNG\n",
            "  File: frame_000398.PNG\n",
            "  File: frame_000445.PNG\n",
            "  File: frame_000291.PNG\n",
            "  File: frame_000459.PNG\n",
            "  File: frame_000566.PNG\n",
            "  File: frame_000392.PNG\n",
            "  File: frame_000372.PNG\n",
            "  File: frame_000249.PNG\n",
            "  File: frame_000531.PNG\n",
            "  File: frame_000478.PNG\n",
            "  File: frame_000570.PNG\n",
            "  File: frame_000569.PNG\n",
            "  File: frame_000208.PNG\n",
            "  File: frame_000025.PNG\n",
            "  File: frame_000397.PNG\n",
            "  File: frame_000101.PNG\n",
            "  File: frame_000058.PNG\n",
            "  File: frame_000451.PNG\n",
            "  File: frame_000174.PNG\n",
            "  File: frame_000055.PNG\n",
            "  File: frame_000578.PNG\n",
            "  File: frame_000062.PNG\n",
            "  File: frame_000257.PNG\n",
            "  File: frame_000474.PNG\n",
            "  File: frame_000416.PNG\n",
            "  File: frame_000447.PNG\n",
            "  File: frame_000584.PNG\n",
            "  File: frame_000339.PNG\n",
            "  File: frame_000093.PNG\n",
            "  File: frame_000200.PNG\n",
            "  File: frame_000373.PNG\n",
            "  File: frame_000021.PNG\n",
            "  File: frame_000433.PNG\n",
            "  File: frame_000422.PNG\n",
            "  File: frame_000571.PNG\n",
            "  File: frame_000468.PNG\n",
            "  File: frame_000098.PNG\n",
            "  File: frame_000329.PNG\n",
            "  File: frame_000121.PNG\n",
            "  File: frame_000549.PNG\n",
            "  File: frame_000194.PNG\n",
            "  File: frame_000133.PNG\n",
            "  File: frame_000532.PNG\n",
            "  File: frame_000594.PNG\n",
            "  File: frame_000486.PNG\n",
            "  File: frame_000128.PNG\n",
            "  File: frame_000325.PNG\n",
            "  File: frame_000559.PNG\n",
            "  File: frame_000499.PNG\n",
            "  File: frame_000111.PNG\n",
            "  File: frame_000152.PNG\n",
            "  File: frame_000579.PNG\n",
            "  File: frame_000389.PNG\n",
            "  File: frame_000348.PNG\n",
            "  File: frame_000074.PNG\n",
            "  File: frame_000341.PNG\n",
            "  File: frame_000575.PNG\n",
            "  File: frame_000284.PNG\n",
            "  File: frame_000300.PNG\n",
            "  File: frame_000147.PNG\n",
            "  File: frame_000224.PNG\n",
            "  File: frame_000581.PNG\n",
            "  File: frame_000246.PNG\n",
            "  File: frame_000091.PNG\n",
            "  File: frame_000006.PNG\n",
            "  File: frame_000071.PNG\n",
            "  File: frame_000489.PNG\n",
            "  File: frame_000497.PNG\n",
            "  File: frame_000484.PNG\n",
            "  File: frame_000167.PNG\n",
            "  File: frame_000000.PNG\n",
            "  File: frame_000146.PNG\n",
            "  File: frame_000079.PNG\n",
            "  File: frame_000227.PNG\n",
            "  File: frame_000027.PNG\n",
            "  File: frame_000278.PNG\n",
            "  File: frame_000214.PNG\n",
            "  File: frame_000400.PNG\n",
            "  File: frame_000407.PNG\n",
            "  File: frame_000483.PNG\n",
            "  File: frame_000525.PNG\n",
            "  File: frame_000272.PNG\n",
            "  File: frame_000145.PNG\n",
            "  File: frame_000142.PNG\n",
            "  File: frame_000598.PNG\n",
            "  File: frame_000057.PNG\n",
            "  File: frame_000077.PNG\n",
            "  File: frame_000126.PNG\n",
            "  File: frame_000287.PNG\n",
            "  File: frame_000223.PNG\n",
            "  File: frame_000241.PNG\n",
            "  File: frame_000383.PNG\n",
            "  File: frame_000544.PNG\n",
            "  File: frame_000411.PNG\n",
            "  File: frame_000120.PNG\n",
            "  File: frame_000360.PNG\n",
            "  File: frame_000363.PNG\n",
            "  File: frame_000547.PNG\n",
            "  File: frame_000305.PNG\n",
            "  File: frame_000269.PNG\n",
            "  File: frame_000218.PNG\n",
            "  File: frame_000353.PNG\n",
            "  File: frame_000413.PNG\n",
            "  File: frame_000524.PNG\n",
            "  File: frame_000216.PNG\n",
            "  File: frame_000546.PNG\n",
            "  File: frame_000364.PNG\n",
            "  File: frame_000084.PNG\n",
            "  File: frame_000382.PNG\n",
            "  File: frame_000427.PNG\n",
            "  File: frame_000541.PNG\n",
            "  File: frame_000015.PNG\n",
            "  File: frame_000265.PNG\n",
            "  File: frame_000426.PNG\n",
            "  File: frame_000247.PNG\n",
            "  File: frame_000170.PNG\n",
            "  File: frame_000401.PNG\n",
            "  File: frame_000066.PNG\n",
            "  File: frame_000334.PNG\n",
            "  File: frame_000294.PNG\n",
            "  File: frame_000005.PNG\n",
            "  File: frame_000424.PNG\n",
            "  File: frame_000410.PNG\n",
            "  File: frame_000327.PNG\n",
            "  File: frame_000168.PNG\n",
            "  File: frame_000036.PNG\n",
            "  File: frame_000518.PNG\n",
            "  File: frame_000333.PNG\n",
            "  File: frame_000054.PNG\n",
            "  File: frame_000205.PNG\n",
            "  File: frame_000012.PNG\n",
            "  File: frame_000385.PNG\n",
            "  File: frame_000543.PNG\n",
            "  File: frame_000316.PNG\n",
            "  File: frame_000276.PNG\n",
            "  File: frame_000162.PNG\n",
            "  File: frame_000040.PNG\n",
            "  File: frame_000153.PNG\n",
            "  File: frame_000336.PNG\n",
            "  File: frame_000435.PNG\n",
            "  File: frame_000337.PNG\n",
            "  File: frame_000490.PNG\n",
            "  File: frame_000349.PNG\n",
            "  File: frame_000215.PNG\n",
            "  File: frame_000421.PNG\n",
            "  File: frame_000225.PNG\n",
            "  File: frame_000534.PNG\n",
            "  File: frame_000453.PNG\n",
            "  File: frame_000100.PNG\n",
            "  File: frame_000347.PNG\n",
            "  File: frame_000359.PNG\n",
            "  File: frame_000473.PNG\n",
            "  File: frame_000043.PNG\n",
            "  File: frame_000245.PNG\n",
            "  File: frame_000574.PNG\n",
            "  File: frame_000178.PNG\n",
            "  File: frame_000436.PNG\n",
            "  File: frame_000495.PNG\n",
            "  File: frame_000045.PNG\n",
            "  File: frame_000326.PNG\n",
            "  File: frame_000099.PNG\n",
            "  File: frame_000285.PNG\n",
            "  File: frame_000230.PNG\n",
            "  File: frame_000289.PNG\n",
            "  File: frame_000252.PNG\n",
            "  File: frame_000548.PNG\n",
            "  File: frame_000500.PNG\n",
            "  File: frame_000419.PNG\n",
            "  File: frame_000470.PNG\n",
            "  File: frame_000431.PNG\n",
            "  File: frame_000582.PNG\n",
            "  File: frame_000446.PNG\n",
            "  File: frame_000564.PNG\n",
            "  File: frame_000355.PNG\n",
            "  File: frame_000124.PNG\n",
            "  File: frame_000085.PNG\n",
            "  File: frame_000240.PNG\n",
            "  File: frame_000048.PNG\n",
            "  File: frame_000366.PNG\n",
            "  File: frame_000092.PNG\n",
            "  File: frame_000314.PNG\n",
            "  File: frame_000530.PNG\n",
            "  File: frame_000035.PNG\n",
            "  File: frame_000504.PNG\n",
            "  File: frame_000297.PNG\n",
            "  File: frame_000271.PNG\n",
            "  File: frame_000263.PNG\n",
            "  File: frame_000460.PNG\n",
            "  File: frame_000452.PNG\n",
            "  File: frame_000237.PNG\n",
            "  File: frame_000197.PNG\n",
            "  File: frame_000439.PNG\n",
            "  File: frame_000583.PNG\n",
            "  File: frame_000563.PNG\n",
            "  File: frame_000313.PNG\n",
            "  File: frame_000420.PNG\n",
            "  File: frame_000207.PNG\n",
            "  File: frame_000209.PNG\n",
            "  File: frame_000448.PNG\n",
            "  File: frame_000374.PNG\n",
            "  File: frame_000494.PNG\n",
            "  File: frame_000080.PNG\n",
            "  File: frame_000279.PNG\n",
            "  File: frame_000050.PNG\n",
            "  File: frame_000381.PNG\n",
            "  File: frame_000182.PNG\n",
            "  File: frame_000180.PNG\n",
            "  File: frame_000156.PNG\n",
            "  File: frame_000096.PNG\n",
            "  File: frame_000467.PNG\n",
            "  File: frame_000123.PNG\n",
            "  File: frame_000075.PNG\n",
            "  File: frame_000461.PNG\n",
            "  File: frame_000356.PNG\n",
            "  File: frame_000432.PNG\n",
            "  File: frame_000296.PNG\n",
            "  File: frame_000138.PNG\n",
            "  File: frame_000501.PNG\n",
            "  File: frame_000267.PNG\n",
            "  File: frame_000553.PNG\n",
            "  File: frame_000538.PNG\n",
            "  File: frame_000106.PNG\n",
            "  File: frame_000496.PNG\n",
            "  File: frame_000371.PNG\n",
            "  File: frame_000135.PNG\n",
            "  File: frame_000324.PNG\n",
            "  File: frame_000037.PNG\n",
            "  File: frame_000202.PNG\n",
            "  File: frame_000067.PNG\n",
            "  File: frame_000136.PNG\n",
            "  File: frame_000192.PNG\n",
            "  File: frame_000568.PNG\n",
            "  File: frame_000094.PNG\n",
            "  File: frame_000014.PNG\n",
            "  File: frame_000226.PNG\n",
            "  File: frame_000132.PNG\n",
            "  File: frame_000185.PNG\n",
            "  File: frame_000274.PNG\n",
            "  File: frame_000118.PNG\n",
            "  File: frame_000331.PNG\n",
            "  File: frame_000256.PNG\n",
            "  File: frame_000177.PNG\n",
            "  File: frame_000102.PNG\n",
            "  File: frame_000196.PNG\n",
            "  File: frame_000510.PNG\n",
            "  File: frame_000376.PNG\n",
            "  File: frame_000286.PNG\n",
            "  File: frame_000114.PNG\n",
            "  File: frame_000032.PNG\n",
            "  File: frame_000469.PNG\n",
            "  File: frame_000064.PNG\n",
            "  File: frame_000551.PNG\n",
            "  File: frame_000151.PNG\n",
            "  File: frame_000089.PNG\n",
            "  File: frame_000299.PNG\n",
            "  File: frame_000144.PNG\n",
            "  File: frame_000068.PNG\n",
            "  File: frame_000176.PNG\n",
            "  File: frame_000399.PNG\n",
            "  File: frame_000560.PNG\n",
            "  File: frame_000328.PNG\n",
            "  File: frame_000443.PNG\n",
            "  File: frame_000017.PNG\n",
            "  File: frame_000450.PNG\n",
            "  File: frame_000358.PNG\n",
            "  File: frame_000028.PNG\n",
            "  File: frame_000298.PNG\n",
            "  File: frame_000576.PNG\n",
            "  File: frame_000029.PNG\n",
            "  File: frame_000522.PNG\n",
            "  File: frame_000592.PNG\n",
            "  File: frame_000018.PNG\n",
            "  File: frame_000515.PNG\n",
            "  File: frame_000184.PNG\n",
            "  File: frame_000046.PNG\n",
            "  File: frame_000310.PNG\n",
            "  File: frame_000507.PNG\n",
            "  File: frame_000228.PNG\n",
            "  File: frame_000595.PNG\n",
            "  File: frame_000456.PNG\n",
            "  File: frame_000217.PNG\n",
            "  File: frame_000423.PNG\n",
            "  File: frame_000108.PNG\n",
            "  File: frame_000527.PNG\n",
            "  File: frame_000408.PNG\n",
            "  File: frame_000517.PNG\n",
            "  File: frame_000160.PNG\n",
            "  File: frame_000505.PNG\n",
            "  File: frame_000198.PNG\n",
            "  File: frame_000082.PNG\n",
            "  File: frame_000567.PNG\n",
            "  File: frame_000390.PNG\n",
            "  File: frame_000268.PNG\n",
            "  File: frame_000370.PNG\n",
            "  File: frame_000378.PNG\n",
            "  File: frame_000319.PNG\n",
            "  File: frame_000233.PNG\n",
            "  File: frame_000509.PNG\n",
            "  File: frame_000219.PNG\n",
            "  File: frame_000585.PNG\n",
            "  File: frame_000206.PNG\n",
            "  File: frame_000116.PNG\n",
            "  File: frame_000589.PNG\n",
            "  File: frame_000231.PNG\n",
            "  File: frame_000070.PNG\n",
            "  File: frame_000072.PNG\n",
            "  File: frame_000260.PNG\n",
            "  File: frame_000379.PNG\n",
            "  File: frame_000137.PNG\n",
            "  File: frame_000266.PNG\n",
            "  File: frame_000154.PNG\n",
            "  File: frame_000377.PNG\n",
            "  File: frame_000105.PNG\n",
            "  File: frame_000596.PNG\n",
            "  File: frame_000288.PNG\n",
            "  File: frame_000514.PNG\n",
            "  File: frame_000588.PNG\n",
            "  File: frame_000253.PNG\n",
            "  File: frame_000303.PNG\n",
            "  File: frame_000526.PNG\n",
            "  File: frame_000022.PNG\n",
            "  File: frame_000332.PNG\n",
            "  File: frame_000572.PNG\n",
            "  File: frame_000181.PNG\n",
            "  File: frame_000311.PNG\n",
            "  File: frame_000076.PNG\n",
            "  File: frame_000239.PNG\n",
            "  File: frame_000191.PNG\n",
            "  File: frame_000533.PNG\n",
            "  File: frame_000148.PNG\n",
            "  File: frame_000140.PNG\n",
            "  File: frame_000409.PNG\n",
            "  File: frame_000211.PNG\n",
            "  File: frame_000344.PNG\n",
            "  File: frame_000238.PNG\n",
            "  File: frame_000404.PNG\n",
            "  File: frame_000315.PNG\n",
            "  File: frame_000512.PNG\n",
            "  File: frame_000060.PNG\n",
            "  File: frame_000222.PNG\n",
            "  File: frame_000425.PNG\n",
            "  File: frame_000047.PNG\n",
            "  File: frame_000487.PNG\n",
            "  File: frame_000258.PNG\n",
            "  File: frame_000369.PNG\n",
            "  File: frame_000234.PNG\n",
            "  File: frame_000438.PNG\n",
            "  File: frame_000304.PNG\n",
            "  File: frame_000440.PNG\n",
            "  File: frame_000110.PNG\n",
            "  File: frame_000081.PNG\n",
            "  File: frame_000386.PNG\n",
            "  File: frame_000415.PNG\n",
            "  File: frame_000485.PNG\n",
            "  File: frame_000122.PNG\n",
            "  File: frame_000212.PNG\n",
            "  File: frame_000213.PNG\n",
            "  File: frame_000466.PNG\n",
            "  File: frame_000412.PNG\n",
            "  File: frame_000558.PNG\n",
            "  File: frame_000171.PNG\n",
            "  File: frame_000088.PNG\n",
            "  File: frame_000125.PNG\n",
            "  File: frame_000573.PNG\n",
            "  File: frame_000506.PNG\n",
            "  File: frame_000033.PNG\n",
            "  File: frame_000539.PNG\n",
            "  File: frame_000024.PNG\n",
            "  File: frame_000362.PNG\n",
            "  File: frame_000508.PNG\n",
            "  File: frame_000354.PNG\n",
            "  File: frame_000464.PNG\n",
            "  File: frame_000323.PNG\n",
            "  File: frame_000254.PNG\n",
            "  File: frame_000023.PNG\n",
            "  File: frame_000472.PNG\n",
            "  File: frame_000463.PNG\n",
            "  File: frame_000293.PNG\n",
            "  File: frame_000479.PNG\n",
            "  File: frame_000248.PNG\n",
            "  File: frame_000444.PNG\n",
            "  File: frame_000519.PNG\n",
            "  File: frame_000396.PNG\n",
            "  File: frame_000521.PNG\n",
            "  File: frame_000338.PNG\n",
            "  File: frame_000049.PNG\n",
            "  File: frame_000073.PNG\n",
            "  File: frame_000346.PNG\n",
            "  File: frame_000038.PNG\n",
            "  File: frame_000112.PNG\n",
            "  File: frame_000134.PNG\n",
            "  File: frame_000275.PNG\n",
            "  File: frame_000095.PNG\n",
            "  File: frame_000020.PNG\n",
            "  File: frame_000078.PNG\n",
            "  File: frame_000301.PNG\n",
            "  File: frame_000117.PNG\n",
            "  File: frame_000380.PNG\n",
            "  File: frame_000302.PNG\n",
            "  File: frame_000555.PNG\n",
            "  File: frame_000157.PNG\n",
            "  File: frame_000277.PNG\n",
            "  File: frame_000173.PNG\n",
            "  File: frame_000041.PNG\n",
            "  File: frame_000493.PNG\n",
            "  File: frame_000221.PNG\n",
            "  File: frame_000393.PNG\n",
            "  File: frame_000113.PNG\n",
            "  File: frame_000290.PNG\n",
            "  File: frame_000535.PNG\n",
            "  File: frame_000003.PNG\n",
            "  File: frame_000283.PNG\n",
            "  File: frame_000261.PNG\n",
            "  File: frame_000520.PNG\n",
            "  File: frame_000034.PNG\n",
            "  File: frame_000434.PNG\n",
            "  File: frame_000232.PNG\n",
            "  File: frame_000069.PNG\n",
            "  File: frame_000542.PNG\n",
            "  File: frame_000511.PNG\n",
            "  File: frame_000193.PNG\n",
            "  File: frame_000087.PNG\n",
            "  File: frame_000402.PNG\n",
            "  File: frame_000139.PNG\n",
            "  File: frame_000280.PNG\n",
            "  File: frame_000143.PNG\n",
            "  File: frame_000063.PNG\n",
            "  File: frame_000405.PNG\n",
            "  File: frame_000597.PNG\n",
            "  File: frame_000250.PNG\n",
            "  File: frame_000361.PNG\n",
            "  File: frame_000545.PNG\n",
            "  File: frame_000476.PNG\n",
            "  File: frame_000340.PNG\n",
            "  File: frame_000281.PNG\n",
            "  File: frame_000150.PNG\n",
            "  File: frame_000308.PNG\n",
            "  File: frame_000528.PNG\n",
            "  File: frame_000053.PNG\n",
            "  File: frame_000172.PNG\n",
            "  File: frame_000430.PNG\n",
            "  File: frame_000236.PNG\n",
            "  File: frame_000011.PNG\n",
            "  File: frame_000417.PNG\n",
            "  File: frame_000282.PNG\n",
            "  File: frame_000090.PNG\n",
            "  File: frame_000586.PNG\n",
            "  File: frame_000488.PNG\n",
            "  File: frame_000318.PNG\n",
            "  File: frame_000471.PNG\n",
            "  File: frame_000395.PNG\n",
            "  File: frame_000009.PNG\n",
            "  File: frame_000335.PNG\n",
            "  File: frame_000577.PNG\n",
            "  File: frame_000454.PNG\n",
            "  File: frame_000086.PNG\n",
            "  File: frame_000343.PNG\n",
            "  File: frame_000129.PNG\n",
            "  File: frame_000491.PNG\n",
            "  File: frame_000465.PNG\n",
            "  File: frame_000429.PNG\n",
            "  File: frame_000187.PNG\n",
            "  File: frame_000403.PNG\n",
            "  File: frame_000131.PNG\n",
            "  File: frame_000169.PNG\n",
            "  File: frame_000199.PNG\n",
            "  File: frame_000044.PNG\n",
            "  File: frame_000061.PNG\n",
            "  File: frame_000587.PNG\n",
            "  File: frame_000406.PNG\n",
            "  File: frame_000498.PNG\n",
            "  File: frame_000026.PNG\n",
            "  File: frame_000367.PNG\n",
            "  File: frame_000001.PNG\n",
            "  File: frame_000004.PNG\n",
            "  File: frame_000352.PNG\n",
            "  File: frame_000109.PNG\n",
            "  File: frame_000251.PNG\n",
            "  File: frame_000455.PNG\n",
            "  File: frame_000593.PNG\n",
            "  File: frame_000259.PNG\n",
            "  File: frame_000320.PNG\n",
            "  File: frame_000203.PNG\n",
            "  File: frame_000243.PNG\n",
            "  File: frame_000309.PNG\n",
            "  File: frame_000175.PNG\n",
            "  File: frame_000052.PNG\n",
            "  File: frame_000475.PNG\n",
            "  File: frame_000159.PNG\n",
            "  File: frame_000590.PNG\n",
            "  File: frame_000002.PNG\n",
            "  File: frame_000188.PNG\n",
            "  File: frame_000550.PNG\n",
            "  File: frame_000552.PNG\n",
            "  File: frame_000179.PNG\n",
            "  File: frame_000210.PNG\n",
            "  File: frame_000149.PNG\n",
            "  File: frame_000561.PNG\n",
            "  File: frame_000458.PNG\n",
            "  File: frame_000556.PNG\n",
            "  File: frame_000115.PNG\n",
            "  File: frame_000502.PNG\n",
            "  File: frame_000119.PNG\n",
            "  File: frame_000565.PNG\n",
            "  File: frame_000330.PNG\n",
            "  File: frame_000480.PNG\n",
            "  File: frame_000418.PNG\n",
            "  File: frame_000306.PNG\n",
            "  File: frame_000130.PNG\n",
            "  File: frame_000477.PNG\n",
            "  File: frame_000189.PNG\n",
            "  File: frame_000591.PNG\n",
            "  File: frame_000065.PNG\n",
            "  File: frame_000273.PNG\n",
            "  File: frame_000375.PNG\n",
            "  File: frame_000391.PNG\n",
            "  File: frame_000083.PNG\n",
            "  File: frame_000351.PNG\n",
            "  File: frame_000103.PNG\n",
            "  File: frame_000529.PNG\n",
            "  File: frame_000536.PNG\n",
            "  File: frame_000104.PNG\n",
            "  File: frame_000255.PNG\n",
            "  File: frame_000244.PNG\n",
            "  File: frame_000322.PNG\n",
            "  File: frame_000599.PNG\n",
            "  File: frame_000008.PNG\n",
            "  File: frame_000141.PNG\n",
            "  File: frame_000516.PNG\n",
            "  File: frame_000039.PNG\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video05_final\n",
            "  File: annotations.xml\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video05_final/images\n",
            "  File: frame_000317.PNG\n",
            "  File: frame_000387.PNG\n",
            "  File: frame_000059.PNG\n",
            "  File: frame_000350.PNG\n",
            "  File: frame_000031.PNG\n",
            "  File: frame_000127.PNG\n",
            "  File: frame_000165.PNG\n",
            "  File: frame_000357.PNG\n",
            "  File: frame_000562.PNG\n",
            "  File: frame_000007.PNG\n",
            "  File: frame_000186.PNG\n",
            "  File: frame_000437.PNG\n",
            "  File: frame_000166.PNG\n",
            "  File: frame_000220.PNG\n",
            "  File: frame_000414.PNG\n",
            "  File: frame_000235.PNG\n",
            "  File: frame_000441.PNG\n",
            "  File: frame_000229.PNG\n",
            "  File: frame_000384.PNG\n",
            "  File: frame_000051.PNG\n",
            "  File: frame_000183.PNG\n",
            "  File: frame_000107.PNG\n",
            "  File: frame_000307.PNG\n",
            "  File: frame_000056.PNG\n",
            "  File: frame_000019.PNG\n",
            "  File: frame_000394.PNG\n",
            "  File: frame_000482.PNG\n",
            "  File: frame_000295.PNG\n",
            "  File: frame_000557.PNG\n",
            "  File: frame_000190.PNG\n",
            "  File: frame_000481.PNG\n",
            "  File: frame_000428.PNG\n",
            "  File: frame_000503.PNG\n",
            "  File: frame_000013.PNG\n",
            "  File: frame_000368.PNG\n",
            "  File: frame_000164.PNG\n",
            "  File: frame_000161.PNG\n",
            "  File: frame_000270.PNG\n",
            "  File: frame_000155.PNG\n",
            "  File: frame_000195.PNG\n",
            "  File: frame_000513.PNG\n",
            "  File: frame_000462.PNG\n",
            "  File: frame_000554.PNG\n",
            "  File: frame_000388.PNG\n",
            "  File: frame_000010.PNG\n",
            "  File: frame_000537.PNG\n",
            "  File: frame_000016.PNG\n",
            "  File: frame_000365.PNG\n",
            "  File: frame_000158.PNG\n",
            "  File: frame_000457.PNG\n",
            "  File: frame_000442.PNG\n",
            "  File: frame_000540.PNG\n",
            "  File: frame_000201.PNG\n",
            "  File: frame_000312.PNG\n",
            "  File: frame_000204.PNG\n",
            "  File: frame_000321.PNG\n",
            "  File: frame_000264.PNG\n",
            "  File: frame_000042.PNG\n",
            "  File: frame_000163.PNG\n",
            "  File: frame_000492.PNG\n",
            "  File: frame_000345.PNG\n",
            "  File: frame_000097.PNG\n",
            "  File: frame_000342.PNG\n",
            "  File: frame_000262.PNG\n",
            "  File: frame_000449.PNG\n",
            "  File: frame_000242.PNG\n",
            "  File: frame_000523.PNG\n",
            "  File: frame_000292.PNG\n",
            "  File: frame_000580.PNG\n",
            "  File: frame_000030.PNG\n",
            "  File: frame_000398.PNG\n",
            "  File: frame_000445.PNG\n",
            "  File: frame_000291.PNG\n",
            "  File: frame_000459.PNG\n",
            "  File: frame_000566.PNG\n",
            "  File: frame_000392.PNG\n",
            "  File: frame_000372.PNG\n",
            "  File: frame_000249.PNG\n",
            "  File: frame_000531.PNG\n",
            "  File: frame_000478.PNG\n",
            "  File: frame_000570.PNG\n",
            "  File: frame_000569.PNG\n",
            "  File: frame_000208.PNG\n",
            "  File: frame_000025.PNG\n",
            "  File: frame_000397.PNG\n",
            "  File: frame_000101.PNG\n",
            "  File: frame_000058.PNG\n",
            "  File: frame_000451.PNG\n",
            "  File: frame_000174.PNG\n",
            "  File: frame_000055.PNG\n",
            "  File: frame_000578.PNG\n",
            "  File: frame_000062.PNG\n",
            "  File: frame_000257.PNG\n",
            "  File: frame_000474.PNG\n",
            "  File: frame_000416.PNG\n",
            "  File: frame_000447.PNG\n",
            "  File: frame_000584.PNG\n",
            "  File: frame_000339.PNG\n",
            "  File: frame_000093.PNG\n",
            "  File: frame_000200.PNG\n",
            "  File: frame_000373.PNG\n",
            "  File: frame_000021.PNG\n",
            "  File: frame_000433.PNG\n",
            "  File: frame_000422.PNG\n",
            "  File: frame_000571.PNG\n",
            "  File: frame_000468.PNG\n",
            "  File: frame_000098.PNG\n",
            "  File: frame_000329.PNG\n",
            "  File: frame_000121.PNG\n",
            "  File: frame_000549.PNG\n",
            "  File: frame_000194.PNG\n",
            "  File: frame_000133.PNG\n",
            "  File: frame_000532.PNG\n",
            "  File: frame_000594.PNG\n",
            "  File: frame_000486.PNG\n",
            "  File: frame_000128.PNG\n",
            "  File: frame_000325.PNG\n",
            "  File: frame_000559.PNG\n",
            "  File: frame_000499.PNG\n",
            "  File: frame_000111.PNG\n",
            "  File: frame_000152.PNG\n",
            "  File: frame_000579.PNG\n",
            "  File: frame_000389.PNG\n",
            "  File: frame_000348.PNG\n",
            "  File: frame_000074.PNG\n",
            "  File: frame_000341.PNG\n",
            "  File: frame_000575.PNG\n",
            "  File: frame_000284.PNG\n",
            "  File: frame_000300.PNG\n",
            "  File: frame_000147.PNG\n",
            "  File: frame_000224.PNG\n",
            "  File: frame_000581.PNG\n",
            "  File: frame_000246.PNG\n",
            "  File: frame_000091.PNG\n",
            "  File: frame_000006.PNG\n",
            "  File: frame_000071.PNG\n",
            "  File: frame_000489.PNG\n",
            "  File: frame_000497.PNG\n",
            "  File: frame_000484.PNG\n",
            "  File: frame_000167.PNG\n",
            "  File: frame_000000.PNG\n",
            "  File: frame_000146.PNG\n",
            "  File: frame_000079.PNG\n",
            "  File: frame_000227.PNG\n",
            "  File: frame_000027.PNG\n",
            "  File: frame_000278.PNG\n",
            "  File: frame_000214.PNG\n",
            "  File: frame_000400.PNG\n",
            "  File: frame_000407.PNG\n",
            "  File: frame_000483.PNG\n",
            "  File: frame_000525.PNG\n",
            "  File: frame_000272.PNG\n",
            "  File: frame_000145.PNG\n",
            "  File: frame_000142.PNG\n",
            "  File: frame_000598.PNG\n",
            "  File: frame_000057.PNG\n",
            "  File: frame_000077.PNG\n",
            "  File: frame_000126.PNG\n",
            "  File: frame_000287.PNG\n",
            "  File: frame_000223.PNG\n",
            "  File: frame_000241.PNG\n",
            "  File: frame_000383.PNG\n",
            "  File: frame_000544.PNG\n",
            "  File: frame_000411.PNG\n",
            "  File: frame_000120.PNG\n",
            "  File: frame_000360.PNG\n",
            "  File: frame_000363.PNG\n",
            "  File: frame_000547.PNG\n",
            "  File: frame_000305.PNG\n",
            "  File: frame_000269.PNG\n",
            "  File: frame_000218.PNG\n",
            "  File: frame_000353.PNG\n",
            "  File: frame_000413.PNG\n",
            "  File: frame_000524.PNG\n",
            "  File: frame_000216.PNG\n",
            "  File: frame_000546.PNG\n",
            "  File: frame_000364.PNG\n",
            "  File: frame_000084.PNG\n",
            "  File: frame_000382.PNG\n",
            "  File: frame_000427.PNG\n",
            "  File: frame_000541.PNG\n",
            "  File: frame_000015.PNG\n",
            "  File: frame_000265.PNG\n",
            "  File: frame_000426.PNG\n",
            "  File: frame_000247.PNG\n",
            "  File: frame_000170.PNG\n",
            "  File: frame_000401.PNG\n",
            "  File: frame_000066.PNG\n",
            "  File: frame_000334.PNG\n",
            "  File: frame_000294.PNG\n",
            "  File: frame_000005.PNG\n",
            "  File: frame_000424.PNG\n",
            "  File: frame_000410.PNG\n",
            "  File: frame_000327.PNG\n",
            "  File: frame_000168.PNG\n",
            "  File: frame_000036.PNG\n",
            "  File: frame_000518.PNG\n",
            "  File: frame_000333.PNG\n",
            "  File: frame_000054.PNG\n",
            "  File: frame_000205.PNG\n",
            "  File: frame_000012.PNG\n",
            "  File: frame_000385.PNG\n",
            "  File: frame_000543.PNG\n",
            "  File: frame_000316.PNG\n",
            "  File: frame_000276.PNG\n",
            "  File: frame_000162.PNG\n",
            "  File: frame_000040.PNG\n",
            "  File: frame_000153.PNG\n",
            "  File: frame_000336.PNG\n",
            "  File: frame_000435.PNG\n",
            "  File: frame_000337.PNG\n",
            "  File: frame_000490.PNG\n",
            "  File: frame_000349.PNG\n",
            "  File: frame_000215.PNG\n",
            "  File: frame_000421.PNG\n",
            "  File: frame_000225.PNG\n",
            "  File: frame_000534.PNG\n",
            "  File: frame_000453.PNG\n",
            "  File: frame_000100.PNG\n",
            "  File: frame_000347.PNG\n",
            "  File: frame_000359.PNG\n",
            "  File: frame_000473.PNG\n",
            "  File: frame_000043.PNG\n",
            "  File: frame_000245.PNG\n",
            "  File: frame_000574.PNG\n",
            "  File: frame_000178.PNG\n",
            "  File: frame_000436.PNG\n",
            "  File: frame_000495.PNG\n",
            "  File: frame_000045.PNG\n",
            "  File: frame_000326.PNG\n",
            "  File: frame_000099.PNG\n",
            "  File: frame_000285.PNG\n",
            "  File: frame_000230.PNG\n",
            "  File: frame_000289.PNG\n",
            "  File: frame_000252.PNG\n",
            "  File: frame_000548.PNG\n",
            "  File: frame_000500.PNG\n",
            "  File: frame_000419.PNG\n",
            "  File: frame_000470.PNG\n",
            "  File: frame_000431.PNG\n",
            "  File: frame_000582.PNG\n",
            "  File: frame_000446.PNG\n",
            "  File: frame_000564.PNG\n",
            "  File: frame_000355.PNG\n",
            "  File: frame_000124.PNG\n",
            "  File: frame_000085.PNG\n",
            "  File: frame_000240.PNG\n",
            "  File: frame_000048.PNG\n",
            "  File: frame_000366.PNG\n",
            "  File: frame_000092.PNG\n",
            "  File: frame_000314.PNG\n",
            "  File: frame_000530.PNG\n",
            "  File: frame_000035.PNG\n",
            "  File: frame_000504.PNG\n",
            "  File: frame_000297.PNG\n",
            "  File: frame_000271.PNG\n",
            "  File: frame_000263.PNG\n",
            "  File: frame_000460.PNG\n",
            "  File: frame_000452.PNG\n",
            "  File: frame_000237.PNG\n",
            "  File: frame_000197.PNG\n",
            "  File: frame_000439.PNG\n",
            "  File: frame_000583.PNG\n",
            "  File: frame_000563.PNG\n",
            "  File: frame_000313.PNG\n",
            "  File: frame_000420.PNG\n",
            "  File: frame_000207.PNG\n",
            "  File: frame_000209.PNG\n",
            "  File: frame_000448.PNG\n",
            "  File: frame_000374.PNG\n",
            "  File: frame_000494.PNG\n",
            "  File: frame_000080.PNG\n",
            "  File: frame_000279.PNG\n",
            "  File: frame_000050.PNG\n",
            "  File: frame_000381.PNG\n",
            "  File: frame_000182.PNG\n",
            "  File: frame_000180.PNG\n",
            "  File: frame_000156.PNG\n",
            "  File: frame_000096.PNG\n",
            "  File: frame_000467.PNG\n",
            "  File: frame_000123.PNG\n",
            "  File: frame_000075.PNG\n",
            "  File: frame_000461.PNG\n",
            "  File: frame_000356.PNG\n",
            "  File: frame_000432.PNG\n",
            "  File: frame_000296.PNG\n",
            "  File: frame_000138.PNG\n",
            "  File: frame_000501.PNG\n",
            "  File: frame_000267.PNG\n",
            "  File: frame_000553.PNG\n",
            "  File: frame_000538.PNG\n",
            "  File: frame_000106.PNG\n",
            "  File: frame_000496.PNG\n",
            "  File: frame_000371.PNG\n",
            "  File: frame_000135.PNG\n",
            "  File: frame_000324.PNG\n",
            "  File: frame_000037.PNG\n",
            "  File: frame_000202.PNG\n",
            "  File: frame_000067.PNG\n",
            "  File: frame_000136.PNG\n",
            "  File: frame_000192.PNG\n",
            "  File: frame_000568.PNG\n",
            "  File: frame_000094.PNG\n",
            "  File: frame_000014.PNG\n",
            "  File: frame_000226.PNG\n",
            "  File: frame_000132.PNG\n",
            "  File: frame_000185.PNG\n",
            "  File: frame_000274.PNG\n",
            "  File: frame_000118.PNG\n",
            "  File: frame_000331.PNG\n",
            "  File: frame_000256.PNG\n",
            "  File: frame_000177.PNG\n",
            "  File: frame_000102.PNG\n",
            "  File: frame_000196.PNG\n",
            "  File: frame_000510.PNG\n",
            "  File: frame_000376.PNG\n",
            "  File: frame_000286.PNG\n",
            "  File: frame_000114.PNG\n",
            "  File: frame_000032.PNG\n",
            "  File: frame_000469.PNG\n",
            "  File: frame_000064.PNG\n",
            "  File: frame_000551.PNG\n",
            "  File: frame_000151.PNG\n",
            "  File: frame_000089.PNG\n",
            "  File: frame_000299.PNG\n",
            "  File: frame_000144.PNG\n",
            "  File: frame_000068.PNG\n",
            "  File: frame_000176.PNG\n",
            "  File: frame_000399.PNG\n",
            "  File: frame_000560.PNG\n",
            "  File: frame_000328.PNG\n",
            "  File: frame_000443.PNG\n",
            "  File: frame_000017.PNG\n",
            "  File: frame_000450.PNG\n",
            "  File: frame_000358.PNG\n",
            "  File: frame_000028.PNG\n",
            "  File: frame_000298.PNG\n",
            "  File: frame_000576.PNG\n",
            "  File: frame_000029.PNG\n",
            "  File: frame_000522.PNG\n",
            "  File: frame_000592.PNG\n",
            "  File: frame_000018.PNG\n",
            "  File: frame_000515.PNG\n",
            "  File: frame_000184.PNG\n",
            "  File: frame_000046.PNG\n",
            "  File: frame_000310.PNG\n",
            "  File: frame_000507.PNG\n",
            "  File: frame_000228.PNG\n",
            "  File: frame_000595.PNG\n",
            "  File: frame_000456.PNG\n",
            "  File: frame_000217.PNG\n",
            "  File: frame_000423.PNG\n",
            "  File: frame_000108.PNG\n",
            "  File: frame_000527.PNG\n",
            "  File: frame_000408.PNG\n",
            "  File: frame_000517.PNG\n",
            "  File: frame_000160.PNG\n",
            "  File: frame_000505.PNG\n",
            "  File: frame_000198.PNG\n",
            "  File: frame_000082.PNG\n",
            "  File: frame_000567.PNG\n",
            "  File: frame_000390.PNG\n",
            "  File: frame_000268.PNG\n",
            "  File: frame_000370.PNG\n",
            "  File: frame_000378.PNG\n",
            "  File: frame_000319.PNG\n",
            "  File: frame_000233.PNG\n",
            "  File: frame_000509.PNG\n",
            "  File: frame_000219.PNG\n",
            "  File: frame_000585.PNG\n",
            "  File: frame_000206.PNG\n",
            "  File: frame_000116.PNG\n",
            "  File: frame_000589.PNG\n",
            "  File: frame_000231.PNG\n",
            "  File: frame_000070.PNG\n",
            "  File: frame_000072.PNG\n",
            "  File: frame_000260.PNG\n",
            "  File: frame_000379.PNG\n",
            "  File: frame_000137.PNG\n",
            "  File: frame_000266.PNG\n",
            "  File: frame_000154.PNG\n",
            "  File: frame_000377.PNG\n",
            "  File: frame_000105.PNG\n",
            "  File: frame_000596.PNG\n",
            "  File: frame_000288.PNG\n",
            "  File: frame_000514.PNG\n",
            "  File: frame_000588.PNG\n",
            "  File: frame_000253.PNG\n",
            "  File: frame_000303.PNG\n",
            "  File: frame_000526.PNG\n",
            "  File: frame_000022.PNG\n",
            "  File: frame_000332.PNG\n",
            "  File: frame_000572.PNG\n",
            "  File: frame_000181.PNG\n",
            "  File: frame_000311.PNG\n",
            "  File: frame_000076.PNG\n",
            "  File: frame_000239.PNG\n",
            "  File: frame_000191.PNG\n",
            "  File: frame_000533.PNG\n",
            "  File: frame_000148.PNG\n",
            "  File: frame_000140.PNG\n",
            "  File: frame_000409.PNG\n",
            "  File: frame_000211.PNG\n",
            "  File: frame_000344.PNG\n",
            "  File: frame_000238.PNG\n",
            "  File: frame_000404.PNG\n",
            "  File: frame_000315.PNG\n",
            "  File: frame_000512.PNG\n",
            "  File: frame_000060.PNG\n",
            "  File: frame_000222.PNG\n",
            "  File: frame_000425.PNG\n",
            "  File: frame_000047.PNG\n",
            "  File: frame_000487.PNG\n",
            "  File: frame_000258.PNG\n",
            "  File: frame_000369.PNG\n",
            "  File: frame_000234.PNG\n",
            "  File: frame_000438.PNG\n",
            "  File: frame_000304.PNG\n",
            "  File: frame_000440.PNG\n",
            "  File: frame_000110.PNG\n",
            "  File: frame_000081.PNG\n",
            "  File: frame_000386.PNG\n",
            "  File: frame_000415.PNG\n",
            "  File: frame_000485.PNG\n",
            "  File: frame_000122.PNG\n",
            "  File: frame_000212.PNG\n",
            "  File: frame_000213.PNG\n",
            "  File: frame_000466.PNG\n",
            "  File: frame_000412.PNG\n",
            "  File: frame_000558.PNG\n",
            "  File: frame_000171.PNG\n",
            "  File: frame_000088.PNG\n",
            "  File: frame_000125.PNG\n",
            "  File: frame_000573.PNG\n",
            "  File: frame_000506.PNG\n",
            "  File: frame_000033.PNG\n",
            "  File: frame_000539.PNG\n",
            "  File: frame_000024.PNG\n",
            "  File: frame_000362.PNG\n",
            "  File: frame_000508.PNG\n",
            "  File: frame_000354.PNG\n",
            "  File: frame_000464.PNG\n",
            "  File: frame_000323.PNG\n",
            "  File: frame_000254.PNG\n",
            "  File: frame_000023.PNG\n",
            "  File: frame_000472.PNG\n",
            "  File: frame_000463.PNG\n",
            "  File: frame_000293.PNG\n",
            "  File: frame_000479.PNG\n",
            "  File: frame_000248.PNG\n",
            "  File: frame_000444.PNG\n",
            "  File: frame_000519.PNG\n",
            "  File: frame_000396.PNG\n",
            "  File: frame_000521.PNG\n",
            "  File: frame_000338.PNG\n",
            "  File: frame_000049.PNG\n",
            "  File: frame_000073.PNG\n",
            "  File: frame_000346.PNG\n",
            "  File: frame_000038.PNG\n",
            "  File: frame_000112.PNG\n",
            "  File: frame_000134.PNG\n",
            "  File: frame_000275.PNG\n",
            "  File: frame_000095.PNG\n",
            "  File: frame_000020.PNG\n",
            "  File: frame_000078.PNG\n",
            "  File: frame_000301.PNG\n",
            "  File: frame_000117.PNG\n",
            "  File: frame_000380.PNG\n",
            "  File: frame_000302.PNG\n",
            "  File: frame_000555.PNG\n",
            "  File: frame_000157.PNG\n",
            "  File: frame_000277.PNG\n",
            "  File: frame_000173.PNG\n",
            "  File: frame_000041.PNG\n",
            "  File: frame_000493.PNG\n",
            "  File: frame_000221.PNG\n",
            "  File: frame_000393.PNG\n",
            "  File: frame_000113.PNG\n",
            "  File: frame_000290.PNG\n",
            "  File: frame_000535.PNG\n",
            "  File: frame_000003.PNG\n",
            "  File: frame_000283.PNG\n",
            "  File: frame_000261.PNG\n",
            "  File: frame_000520.PNG\n",
            "  File: frame_000034.PNG\n",
            "  File: frame_000434.PNG\n",
            "  File: frame_000232.PNG\n",
            "  File: frame_000069.PNG\n",
            "  File: frame_000542.PNG\n",
            "  File: frame_000511.PNG\n",
            "  File: frame_000193.PNG\n",
            "  File: frame_000087.PNG\n",
            "  File: frame_000402.PNG\n",
            "  File: frame_000139.PNG\n",
            "  File: frame_000280.PNG\n",
            "  File: frame_000143.PNG\n",
            "  File: frame_000063.PNG\n",
            "  File: frame_000405.PNG\n",
            "  File: frame_000597.PNG\n",
            "  File: frame_000250.PNG\n",
            "  File: frame_000361.PNG\n",
            "  File: frame_000545.PNG\n",
            "  File: frame_000476.PNG\n",
            "  File: frame_000340.PNG\n",
            "  File: frame_000281.PNG\n",
            "  File: frame_000150.PNG\n",
            "  File: frame_000308.PNG\n",
            "  File: frame_000528.PNG\n",
            "  File: frame_000053.PNG\n",
            "  File: frame_000172.PNG\n",
            "  File: frame_000430.PNG\n",
            "  File: frame_000236.PNG\n",
            "  File: frame_000011.PNG\n",
            "  File: frame_000417.PNG\n",
            "  File: frame_000282.PNG\n",
            "  File: frame_000090.PNG\n",
            "  File: frame_000586.PNG\n",
            "  File: frame_000488.PNG\n",
            "  File: frame_000318.PNG\n",
            "  File: frame_000471.PNG\n",
            "  File: frame_000395.PNG\n",
            "  File: frame_000009.PNG\n",
            "  File: frame_000335.PNG\n",
            "  File: frame_000577.PNG\n",
            "  File: frame_000454.PNG\n",
            "  File: frame_000086.PNG\n",
            "  File: frame_000343.PNG\n",
            "  File: frame_000129.PNG\n",
            "  File: frame_000491.PNG\n",
            "  File: frame_000465.PNG\n",
            "  File: frame_000429.PNG\n",
            "  File: frame_000187.PNG\n",
            "  File: frame_000403.PNG\n",
            "  File: frame_000131.PNG\n",
            "  File: frame_000169.PNG\n",
            "  File: frame_000199.PNG\n",
            "  File: frame_000044.PNG\n",
            "  File: frame_000061.PNG\n",
            "  File: frame_000587.PNG\n",
            "  File: frame_000406.PNG\n",
            "  File: frame_000498.PNG\n",
            "  File: frame_000026.PNG\n",
            "  File: frame_000367.PNG\n",
            "  File: frame_000001.PNG\n",
            "  File: frame_000004.PNG\n",
            "  File: frame_000352.PNG\n",
            "  File: frame_000109.PNG\n",
            "  File: frame_000251.PNG\n",
            "  File: frame_000455.PNG\n",
            "  File: frame_000593.PNG\n",
            "  File: frame_000259.PNG\n",
            "  File: frame_000320.PNG\n",
            "  File: frame_000203.PNG\n",
            "  File: frame_000243.PNG\n",
            "  File: frame_000309.PNG\n",
            "  File: frame_000175.PNG\n",
            "  File: frame_000052.PNG\n",
            "  File: frame_000475.PNG\n",
            "  File: frame_000159.PNG\n",
            "  File: frame_000590.PNG\n",
            "  File: frame_000002.PNG\n",
            "  File: frame_000188.PNG\n",
            "  File: frame_000550.PNG\n",
            "  File: frame_000552.PNG\n",
            "  File: frame_000179.PNG\n",
            "  File: frame_000210.PNG\n",
            "  File: frame_000149.PNG\n",
            "  File: frame_000561.PNG\n",
            "  File: frame_000458.PNG\n",
            "  File: frame_000556.PNG\n",
            "  File: frame_000115.PNG\n",
            "  File: frame_000502.PNG\n",
            "  File: frame_000119.PNG\n",
            "  File: frame_000565.PNG\n",
            "  File: frame_000330.PNG\n",
            "  File: frame_000480.PNG\n",
            "  File: frame_000418.PNG\n",
            "  File: frame_000306.PNG\n",
            "  File: frame_000130.PNG\n",
            "  File: frame_000477.PNG\n",
            "  File: frame_000189.PNG\n",
            "  File: frame_000591.PNG\n",
            "  File: frame_000065.PNG\n",
            "  File: frame_000273.PNG\n",
            "  File: frame_000375.PNG\n",
            "  File: frame_000391.PNG\n",
            "  File: frame_000083.PNG\n",
            "  File: frame_000351.PNG\n",
            "  File: frame_000103.PNG\n",
            "  File: frame_000529.PNG\n",
            "  File: frame_000536.PNG\n",
            "  File: frame_000104.PNG\n",
            "  File: frame_000255.PNG\n",
            "  File: frame_000244.PNG\n",
            "  File: frame_000322.PNG\n",
            "  File: frame_000599.PNG\n",
            "  File: frame_000008.PNG\n",
            "  File: frame_000141.PNG\n",
            "  File: frame_000516.PNG\n",
            "  File: frame_000039.PNG\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)\n",
            "  File: annotations.xml\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\n",
            "  File: frame_000059.PNG\n",
            "  File: frame_000031.PNG\n",
            "  File: frame_000127.PNG\n",
            "  File: frame_000165.PNG\n",
            "  File: frame_000007.PNG\n",
            "  File: frame_000186.PNG\n",
            "  File: frame_000166.PNG\n",
            "  File: frame_000220.PNG\n",
            "  File: frame_000235.PNG\n",
            "  File: frame_000229.PNG\n",
            "  File: frame_000051.PNG\n",
            "  File: frame_000183.PNG\n",
            "  File: frame_000107.PNG\n",
            "  File: frame_000056.PNG\n",
            "  File: frame_000019.PNG\n",
            "  File: frame_000295.PNG\n",
            "  File: frame_000190.PNG\n",
            "  File: frame_000013.PNG\n",
            "  File: frame_000164.PNG\n",
            "  File: frame_000161.PNG\n",
            "  File: frame_000270.PNG\n",
            "  File: frame_000155.PNG\n",
            "  File: frame_000195.PNG\n",
            "  File: frame_000010.PNG\n",
            "  File: frame_000016.PNG\n",
            "  File: frame_000158.PNG\n",
            "  File: frame_000201.PNG\n",
            "  File: frame_000204.PNG\n",
            "  File: frame_000264.PNG\n",
            "  File: frame_000042.PNG\n",
            "  File: frame_000163.PNG\n",
            "  File: frame_000097.PNG\n",
            "  File: frame_000262.PNG\n",
            "  File: frame_000242.PNG\n",
            "  File: frame_000292.PNG\n",
            "  File: frame_000030.PNG\n",
            "  File: frame_000291.PNG\n",
            "  File: frame_000249.PNG\n",
            "  File: frame_000208.PNG\n",
            "  File: frame_000025.PNG\n",
            "  File: frame_000101.PNG\n",
            "  File: frame_000058.PNG\n",
            "  File: frame_000174.PNG\n",
            "  File: frame_000055.PNG\n",
            "  File: frame_000062.PNG\n",
            "  File: frame_000257.PNG\n",
            "  File: frame_000093.PNG\n",
            "  File: frame_000200.PNG\n",
            "  File: frame_000021.PNG\n",
            "  File: frame_000098.PNG\n",
            "  File: frame_000121.PNG\n",
            "  File: frame_000194.PNG\n",
            "  File: frame_000133.PNG\n",
            "  File: frame_000128.PNG\n",
            "  File: frame_000111.PNG\n",
            "  File: frame_000152.PNG\n",
            "  File: frame_000074.PNG\n",
            "  File: frame_000284.PNG\n",
            "  File: frame_000300.PNG\n",
            "  File: frame_000147.PNG\n",
            "  File: frame_000224.PNG\n",
            "  File: frame_000246.PNG\n",
            "  File: frame_000091.PNG\n",
            "  File: frame_000006.PNG\n",
            "  File: frame_000071.PNG\n",
            "  File: frame_000167.PNG\n",
            "  File: frame_000000.PNG\n",
            "  File: frame_000146.PNG\n",
            "  File: frame_000079.PNG\n",
            "  File: frame_000227.PNG\n",
            "  File: frame_000027.PNG\n",
            "  File: frame_000278.PNG\n",
            "  File: frame_000214.PNG\n",
            "  File: frame_000272.PNG\n",
            "  File: frame_000145.PNG\n",
            "  File: frame_000142.PNG\n",
            "  File: frame_000057.PNG\n",
            "  File: frame_000077.PNG\n",
            "  File: frame_000126.PNG\n",
            "  File: frame_000287.PNG\n",
            "  File: frame_000223.PNG\n",
            "  File: frame_000241.PNG\n",
            "  File: frame_000120.PNG\n",
            "  File: frame_000269.PNG\n",
            "  File: frame_000218.PNG\n",
            "  File: frame_000216.PNG\n",
            "  File: frame_000084.PNG\n",
            "  File: frame_000015.PNG\n",
            "  File: frame_000265.PNG\n",
            "  File: frame_000247.PNG\n",
            "  File: frame_000170.PNG\n",
            "  File: frame_000066.PNG\n",
            "  File: frame_000294.PNG\n",
            "  File: frame_000005.PNG\n",
            "  File: frame_000168.PNG\n",
            "  File: frame_000036.PNG\n",
            "  File: frame_000054.PNG\n",
            "  File: frame_000205.PNG\n",
            "  File: frame_000012.PNG\n",
            "  File: frame_000276.PNG\n",
            "  File: frame_000162.PNG\n",
            "  File: frame_000040.PNG\n",
            "  File: frame_000153.PNG\n",
            "  File: frame_000215.PNG\n",
            "  File: frame_000225.PNG\n",
            "  File: frame_000100.PNG\n",
            "  File: frame_000043.PNG\n",
            "  File: frame_000245.PNG\n",
            "  File: frame_000178.PNG\n",
            "  File: frame_000045.PNG\n",
            "  File: frame_000099.PNG\n",
            "  File: frame_000285.PNG\n",
            "  File: frame_000230.PNG\n",
            "  File: frame_000289.PNG\n",
            "  File: frame_000252.PNG\n",
            "  File: frame_000124.PNG\n",
            "  File: frame_000085.PNG\n",
            "  File: frame_000240.PNG\n",
            "  File: frame_000048.PNG\n",
            "  File: frame_000092.PNG\n",
            "  File: frame_000035.PNG\n",
            "  File: frame_000297.PNG\n",
            "  File: frame_000271.PNG\n",
            "  File: frame_000263.PNG\n",
            "  File: frame_000237.PNG\n",
            "  File: frame_000197.PNG\n",
            "  File: frame_000207.PNG\n",
            "  File: frame_000209.PNG\n",
            "  File: frame_000080.PNG\n",
            "  File: frame_000279.PNG\n",
            "  File: frame_000050.PNG\n",
            "  File: frame_000182.PNG\n",
            "  File: frame_000180.PNG\n",
            "  File: frame_000156.PNG\n",
            "  File: frame_000096.PNG\n",
            "  File: frame_000123.PNG\n",
            "  File: frame_000075.PNG\n",
            "  File: frame_000296.PNG\n",
            "  File: frame_000138.PNG\n",
            "  File: frame_000267.PNG\n",
            "  File: frame_000106.PNG\n",
            "  File: frame_000135.PNG\n",
            "  File: frame_000037.PNG\n",
            "  File: frame_000202.PNG\n",
            "  File: frame_000067.PNG\n",
            "  File: frame_000136.PNG\n",
            "  File: frame_000192.PNG\n",
            "  File: frame_000094.PNG\n",
            "  File: frame_000014.PNG\n",
            "  File: frame_000226.PNG\n",
            "  File: frame_000132.PNG\n",
            "  File: frame_000185.PNG\n",
            "  File: frame_000274.PNG\n",
            "  File: frame_000118.PNG\n",
            "  File: frame_000256.PNG\n",
            "  File: frame_000177.PNG\n",
            "  File: frame_000102.PNG\n",
            "  File: frame_000196.PNG\n",
            "  File: frame_000286.PNG\n",
            "  File: frame_000114.PNG\n",
            "  File: frame_000032.PNG\n",
            "  File: frame_000064.PNG\n",
            "  File: frame_000151.PNG\n",
            "  File: frame_000089.PNG\n",
            "  File: frame_000299.PNG\n",
            "  File: frame_000144.PNG\n",
            "  File: frame_000068.PNG\n",
            "  File: frame_000176.PNG\n",
            "  File: frame_000017.PNG\n",
            "  File: frame_000028.PNG\n",
            "  File: frame_000298.PNG\n",
            "  File: frame_000029.PNG\n",
            "  File: frame_000018.PNG\n",
            "  File: frame_000184.PNG\n",
            "  File: frame_000046.PNG\n",
            "  File: frame_000228.PNG\n",
            "  File: frame_000217.PNG\n",
            "  File: frame_000108.PNG\n",
            "  File: frame_000160.PNG\n",
            "  File: frame_000198.PNG\n",
            "  File: frame_000082.PNG\n",
            "  File: frame_000268.PNG\n",
            "  File: frame_000233.PNG\n",
            "  File: frame_000219.PNG\n",
            "  File: frame_000206.PNG\n",
            "  File: frame_000116.PNG\n",
            "  File: frame_000231.PNG\n",
            "  File: frame_000070.PNG\n",
            "  File: frame_000072.PNG\n",
            "  File: frame_000260.PNG\n",
            "  File: frame_000137.PNG\n",
            "  File: frame_000266.PNG\n",
            "  File: frame_000154.PNG\n",
            "  File: frame_000105.PNG\n",
            "  File: frame_000288.PNG\n",
            "  File: frame_000253.PNG\n",
            "  File: frame_000303.PNG\n",
            "  File: frame_000022.PNG\n",
            "  File: frame_000181.PNG\n",
            "  File: frame_000076.PNG\n",
            "  File: frame_000239.PNG\n",
            "  File: frame_000191.PNG\n",
            "  File: frame_000148.PNG\n",
            "  File: frame_000140.PNG\n",
            "  File: frame_000211.PNG\n",
            "  File: frame_000238.PNG\n",
            "  File: frame_000060.PNG\n",
            "  File: frame_000222.PNG\n",
            "  File: frame_000047.PNG\n",
            "  File: frame_000258.PNG\n",
            "  File: frame_000234.PNG\n",
            "  File: frame_000304.PNG\n",
            "  File: frame_000110.PNG\n",
            "  File: frame_000081.PNG\n",
            "  File: frame_000122.PNG\n",
            "  File: frame_000212.PNG\n",
            "  File: frame_000213.PNG\n",
            "  File: frame_000171.PNG\n",
            "  File: frame_000088.PNG\n",
            "  File: frame_000125.PNG\n",
            "  File: frame_000033.PNG\n",
            "  File: frame_000024.PNG\n",
            "  File: frame_000254.PNG\n",
            "  File: frame_000023.PNG\n",
            "  File: frame_000293.PNG\n",
            "  File: frame_000248.PNG\n",
            "  File: frame_000049.PNG\n",
            "  File: frame_000073.PNG\n",
            "  File: frame_000038.PNG\n",
            "  File: frame_000112.PNG\n",
            "  File: frame_000134.PNG\n",
            "  File: frame_000275.PNG\n",
            "  File: frame_000095.PNG\n",
            "  File: frame_000020.PNG\n",
            "  File: frame_000078.PNG\n",
            "  File: frame_000301.PNG\n",
            "  File: frame_000117.PNG\n",
            "  File: frame_000302.PNG\n",
            "  File: frame_000157.PNG\n",
            "  File: frame_000277.PNG\n",
            "  File: frame_000173.PNG\n",
            "  File: frame_000041.PNG\n",
            "  File: frame_000221.PNG\n",
            "  File: frame_000113.PNG\n",
            "  File: frame_000290.PNG\n",
            "  File: frame_000003.PNG\n",
            "  File: frame_000283.PNG\n",
            "  File: frame_000261.PNG\n",
            "  File: frame_000034.PNG\n",
            "  File: frame_000232.PNG\n",
            "  File: frame_000069.PNG\n",
            "  File: frame_000193.PNG\n",
            "  File: frame_000087.PNG\n",
            "  File: frame_000139.PNG\n",
            "  File: frame_000280.PNG\n",
            "  File: frame_000143.PNG\n",
            "  File: frame_000063.PNG\n",
            "  File: frame_000250.PNG\n",
            "  File: frame_000281.PNG\n",
            "  File: frame_000150.PNG\n",
            "  File: frame_000053.PNG\n",
            "  File: frame_000172.PNG\n",
            "  File: frame_000236.PNG\n",
            "  File: frame_000011.PNG\n",
            "  File: frame_000282.PNG\n",
            "  File: frame_000090.PNG\n",
            "  File: frame_000009.PNG\n",
            "  File: frame_000086.PNG\n",
            "  File: frame_000129.PNG\n",
            "  File: frame_000187.PNG\n",
            "  File: frame_000131.PNG\n",
            "  File: frame_000169.PNG\n",
            "  File: frame_000199.PNG\n",
            "  File: frame_000044.PNG\n",
            "  File: frame_000061.PNG\n",
            "  File: frame_000026.PNG\n",
            "  File: frame_000001.PNG\n",
            "  File: frame_000004.PNG\n",
            "  File: frame_000109.PNG\n",
            "  File: frame_000251.PNG\n",
            "  File: frame_000259.PNG\n",
            "  File: frame_000203.PNG\n",
            "  File: frame_000243.PNG\n",
            "  File: frame_000175.PNG\n",
            "  File: frame_000052.PNG\n",
            "  File: frame_000159.PNG\n",
            "  File: frame_000002.PNG\n",
            "  File: frame_000188.PNG\n",
            "  File: frame_000179.PNG\n",
            "  File: frame_000210.PNG\n",
            "  File: frame_000149.PNG\n",
            "  File: frame_000115.PNG\n",
            "  File: frame_000119.PNG\n",
            "  File: frame_000130.PNG\n",
            "  File: frame_000189.PNG\n",
            "  File: frame_000065.PNG\n",
            "  File: frame_000273.PNG\n",
            "  File: frame_000083.PNG\n",
            "  File: frame_000103.PNG\n",
            "  File: frame_000104.PNG\n",
            "  File: frame_000255.PNG\n",
            "  File: frame_000244.PNG\n",
            "  File: frame_000008.PNG\n",
            "  File: frame_000141.PNG\n",
            "  File: frame_000039.PNG\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(301-600)\n",
            "  File: annotations.xml\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(301-600)/images\n",
            "  File: frame_000059.PNG\n",
            "  File: frame_000031.PNG\n",
            "  File: frame_000127.PNG\n",
            "  File: frame_000165.PNG\n",
            "  File: frame_000007.PNG\n",
            "  File: frame_000186.PNG\n",
            "  File: frame_000166.PNG\n",
            "  File: frame_000220.PNG\n",
            "  File: frame_000235.PNG\n",
            "  File: frame_000229.PNG\n",
            "  File: frame_000051.PNG\n",
            "  File: frame_000183.PNG\n",
            "  File: frame_000107.PNG\n",
            "  File: frame_000056.PNG\n",
            "  File: frame_000019.PNG\n",
            "  File: frame_000295.PNG\n",
            "  File: frame_000190.PNG\n",
            "  File: frame_000013.PNG\n",
            "  File: frame_000164.PNG\n",
            "  File: frame_000161.PNG\n",
            "  File: frame_000270.PNG\n",
            "  File: frame_000155.PNG\n",
            "  File: frame_000195.PNG\n",
            "  File: frame_000010.PNG\n",
            "  File: frame_000016.PNG\n",
            "  File: frame_000158.PNG\n",
            "  File: frame_000201.PNG\n",
            "  File: frame_000204.PNG\n",
            "  File: frame_000264.PNG\n",
            "  File: frame_000042.PNG\n",
            "  File: frame_000163.PNG\n",
            "  File: frame_000097.PNG\n",
            "  File: frame_000262.PNG\n",
            "  File: frame_000242.PNG\n",
            "  File: frame_000292.PNG\n",
            "  File: frame_000030.PNG\n",
            "  File: frame_000291.PNG\n",
            "  File: frame_000249.PNG\n",
            "  File: frame_000208.PNG\n",
            "  File: frame_000025.PNG\n",
            "  File: frame_000101.PNG\n",
            "  File: frame_000058.PNG\n",
            "  File: frame_000174.PNG\n",
            "  File: frame_000055.PNG\n",
            "  File: frame_000062.PNG\n",
            "  File: frame_000257.PNG\n",
            "  File: frame_000093.PNG\n",
            "  File: frame_000200.PNG\n",
            "  File: frame_000021.PNG\n",
            "  File: frame_000098.PNG\n",
            "  File: frame_000121.PNG\n",
            "  File: frame_000194.PNG\n",
            "  File: frame_000133.PNG\n",
            "  File: frame_000128.PNG\n",
            "  File: frame_000111.PNG\n",
            "  File: frame_000152.PNG\n",
            "  File: frame_000074.PNG\n",
            "  File: frame_000284.PNG\n",
            "  File: frame_000300.PNG\n",
            "  File: frame_000147.PNG\n",
            "  File: frame_000224.PNG\n",
            "  File: frame_000246.PNG\n",
            "  File: frame_000091.PNG\n",
            "  File: frame_000006.PNG\n",
            "  File: frame_000071.PNG\n",
            "  File: frame_000167.PNG\n",
            "  File: frame_000000.PNG\n",
            "  File: frame_000146.PNG\n",
            "  File: frame_000079.PNG\n",
            "  File: frame_000227.PNG\n",
            "  File: frame_000027.PNG\n",
            "  File: frame_000278.PNG\n",
            "  File: frame_000214.PNG\n",
            "  File: frame_000272.PNG\n",
            "  File: frame_000145.PNG\n",
            "  File: frame_000142.PNG\n",
            "  File: frame_000057.PNG\n",
            "  File: frame_000077.PNG\n",
            "  File: frame_000126.PNG\n",
            "  File: frame_000287.PNG\n",
            "  File: frame_000223.PNG\n",
            "  File: frame_000241.PNG\n",
            "  File: frame_000120.PNG\n",
            "  File: frame_000269.PNG\n",
            "  File: frame_000218.PNG\n",
            "  File: frame_000216.PNG\n",
            "  File: frame_000084.PNG\n",
            "  File: frame_000015.PNG\n",
            "  File: frame_000265.PNG\n",
            "  File: frame_000247.PNG\n",
            "  File: frame_000170.PNG\n",
            "  File: frame_000066.PNG\n",
            "  File: frame_000294.PNG\n",
            "  File: frame_000005.PNG\n",
            "  File: frame_000168.PNG\n",
            "  File: frame_000036.PNG\n",
            "  File: frame_000054.PNG\n",
            "  File: frame_000205.PNG\n",
            "  File: frame_000012.PNG\n",
            "  File: frame_000276.PNG\n",
            "  File: frame_000162.PNG\n",
            "  File: frame_000040.PNG\n",
            "  File: frame_000153.PNG\n",
            "  File: frame_000215.PNG\n",
            "  File: frame_000225.PNG\n",
            "  File: frame_000100.PNG\n",
            "  File: frame_000043.PNG\n",
            "  File: frame_000245.PNG\n",
            "  File: frame_000178.PNG\n",
            "  File: frame_000045.PNG\n",
            "  File: frame_000099.PNG\n",
            "  File: frame_000285.PNG\n",
            "  File: frame_000230.PNG\n",
            "  File: frame_000289.PNG\n",
            "  File: frame_000252.PNG\n",
            "  File: frame_000124.PNG\n",
            "  File: frame_000085.PNG\n",
            "  File: frame_000240.PNG\n",
            "  File: frame_000048.PNG\n",
            "  File: frame_000092.PNG\n",
            "  File: frame_000035.PNG\n",
            "  File: frame_000297.PNG\n",
            "  File: frame_000271.PNG\n",
            "  File: frame_000263.PNG\n",
            "  File: frame_000237.PNG\n",
            "  File: frame_000197.PNG\n",
            "  File: frame_000207.PNG\n",
            "  File: frame_000209.PNG\n",
            "  File: frame_000080.PNG\n",
            "  File: frame_000279.PNG\n",
            "  File: frame_000050.PNG\n",
            "  File: frame_000182.PNG\n",
            "  File: frame_000180.PNG\n",
            "  File: frame_000156.PNG\n",
            "  File: frame_000096.PNG\n",
            "  File: frame_000123.PNG\n",
            "  File: frame_000075.PNG\n",
            "  File: frame_000296.PNG\n",
            "  File: frame_000138.PNG\n",
            "  File: frame_000267.PNG\n",
            "  File: frame_000106.PNG\n",
            "  File: frame_000135.PNG\n",
            "  File: frame_000037.PNG\n",
            "  File: frame_000202.PNG\n",
            "  File: frame_000067.PNG\n",
            "  File: frame_000136.PNG\n",
            "  File: frame_000192.PNG\n",
            "  File: frame_000094.PNG\n",
            "  File: frame_000014.PNG\n",
            "  File: frame_000226.PNG\n",
            "  File: frame_000132.PNG\n",
            "  File: frame_000185.PNG\n",
            "  File: frame_000274.PNG\n",
            "  File: frame_000118.PNG\n",
            "  File: frame_000256.PNG\n",
            "  File: frame_000177.PNG\n",
            "  File: frame_000102.PNG\n",
            "  File: frame_000196.PNG\n",
            "  File: frame_000286.PNG\n",
            "  File: frame_000114.PNG\n",
            "  File: frame_000032.PNG\n",
            "  File: frame_000064.PNG\n",
            "  File: frame_000151.PNG\n",
            "  File: frame_000089.PNG\n",
            "  File: frame_000299.PNG\n",
            "  File: frame_000144.PNG\n",
            "  File: frame_000068.PNG\n",
            "  File: frame_000176.PNG\n",
            "  File: frame_000017.PNG\n",
            "  File: frame_000028.PNG\n",
            "  File: frame_000298.PNG\n",
            "  File: frame_000029.PNG\n",
            "  File: frame_000018.PNG\n",
            "  File: frame_000184.PNG\n",
            "  File: frame_000046.PNG\n",
            "  File: frame_000228.PNG\n",
            "  File: frame_000217.PNG\n",
            "  File: frame_000108.PNG\n",
            "  File: frame_000160.PNG\n",
            "  File: frame_000198.PNG\n",
            "  File: frame_000082.PNG\n",
            "  File: frame_000268.PNG\n",
            "  File: frame_000233.PNG\n",
            "  File: frame_000219.PNG\n",
            "  File: frame_000206.PNG\n",
            "  File: frame_000116.PNG\n",
            "  File: frame_000231.PNG\n",
            "  File: frame_000070.PNG\n",
            "  File: frame_000072.PNG\n",
            "  File: frame_000260.PNG\n",
            "  File: frame_000137.PNG\n",
            "  File: frame_000266.PNG\n",
            "  File: frame_000154.PNG\n",
            "  File: frame_000105.PNG\n",
            "  File: frame_000288.PNG\n",
            "  File: frame_000253.PNG\n",
            "  File: frame_000303.PNG\n",
            "  File: frame_000022.PNG\n",
            "  File: frame_000181.PNG\n",
            "  File: frame_000076.PNG\n",
            "  File: frame_000239.PNG\n",
            "  File: frame_000191.PNG\n",
            "  File: frame_000148.PNG\n",
            "  File: frame_000140.PNG\n",
            "  File: frame_000211.PNG\n",
            "  File: frame_000238.PNG\n",
            "  File: frame_000060.PNG\n",
            "  File: frame_000222.PNG\n",
            "  File: frame_000047.PNG\n",
            "  File: frame_000258.PNG\n",
            "  File: frame_000234.PNG\n",
            "  File: frame_000304.PNG\n",
            "  File: frame_000110.PNG\n",
            "  File: frame_000081.PNG\n",
            "  File: frame_000122.PNG\n",
            "  File: frame_000212.PNG\n",
            "  File: frame_000213.PNG\n",
            "  File: frame_000171.PNG\n",
            "  File: frame_000088.PNG\n",
            "  File: frame_000125.PNG\n",
            "  File: frame_000033.PNG\n",
            "  File: frame_000024.PNG\n",
            "  File: frame_000254.PNG\n",
            "  File: frame_000023.PNG\n",
            "  File: frame_000293.PNG\n",
            "  File: frame_000248.PNG\n",
            "  File: frame_000049.PNG\n",
            "  File: frame_000073.PNG\n",
            "  File: frame_000038.PNG\n",
            "  File: frame_000112.PNG\n",
            "  File: frame_000134.PNG\n",
            "  File: frame_000275.PNG\n",
            "  File: frame_000095.PNG\n",
            "  File: frame_000020.PNG\n",
            "  File: frame_000078.PNG\n",
            "  File: frame_000301.PNG\n",
            "  File: frame_000117.PNG\n",
            "  File: frame_000302.PNG\n",
            "  File: frame_000157.PNG\n",
            "  File: frame_000277.PNG\n",
            "  File: frame_000173.PNG\n",
            "  File: frame_000041.PNG\n",
            "  File: frame_000221.PNG\n",
            "  File: frame_000113.PNG\n",
            "  File: frame_000290.PNG\n",
            "  File: frame_000003.PNG\n",
            "  File: frame_000283.PNG\n",
            "  File: frame_000261.PNG\n",
            "  File: frame_000034.PNG\n",
            "  File: frame_000232.PNG\n",
            "  File: frame_000069.PNG\n",
            "  File: frame_000193.PNG\n",
            "  File: frame_000087.PNG\n",
            "  File: frame_000139.PNG\n",
            "  File: frame_000280.PNG\n",
            "  File: frame_000143.PNG\n",
            "  File: frame_000063.PNG\n",
            "  File: frame_000250.PNG\n",
            "  File: frame_000281.PNG\n",
            "  File: frame_000150.PNG\n",
            "  File: frame_000053.PNG\n",
            "  File: frame_000172.PNG\n",
            "  File: frame_000236.PNG\n",
            "  File: frame_000011.PNG\n",
            "  File: frame_000282.PNG\n",
            "  File: frame_000090.PNG\n",
            "  File: frame_000009.PNG\n",
            "  File: frame_000086.PNG\n",
            "  File: frame_000129.PNG\n",
            "  File: frame_000187.PNG\n",
            "  File: frame_000131.PNG\n",
            "  File: frame_000169.PNG\n",
            "  File: frame_000199.PNG\n",
            "  File: frame_000044.PNG\n",
            "  File: frame_000061.PNG\n",
            "  File: frame_000026.PNG\n",
            "  File: frame_000001.PNG\n",
            "  File: frame_000004.PNG\n",
            "  File: frame_000109.PNG\n",
            "  File: frame_000251.PNG\n",
            "  File: frame_000259.PNG\n",
            "  File: frame_000203.PNG\n",
            "  File: frame_000243.PNG\n",
            "  File: frame_000175.PNG\n",
            "  File: frame_000052.PNG\n",
            "  File: frame_000159.PNG\n",
            "  File: frame_000002.PNG\n",
            "  File: frame_000188.PNG\n",
            "  File: frame_000179.PNG\n",
            "  File: frame_000210.PNG\n",
            "  File: frame_000149.PNG\n",
            "  File: frame_000115.PNG\n",
            "  File: frame_000119.PNG\n",
            "  File: frame_000130.PNG\n",
            "  File: frame_000189.PNG\n",
            "  File: frame_000065.PNG\n",
            "  File: frame_000273.PNG\n",
            "  File: frame_000083.PNG\n",
            "  File: frame_000103.PNG\n",
            "  File: frame_000104.PNG\n",
            "  File: frame_000255.PNG\n",
            "  File: frame_000244.PNG\n",
            "  File: frame_000008.PNG\n",
            "  File: frame_000141.PNG\n",
            "  File: frame_000039.PNG\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(0-300)\n",
            "  File: annotations.xml\n",
            "Directory: /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(0-300)/images\n",
            "  File: frame_000059.PNG\n",
            "  File: frame_000031.PNG\n",
            "  File: frame_000127.PNG\n",
            "  File: frame_000165.PNG\n",
            "  File: frame_000007.PNG\n",
            "  File: frame_000186.PNG\n",
            "  File: frame_000166.PNG\n",
            "  File: frame_000220.PNG\n",
            "  File: frame_000235.PNG\n",
            "  File: frame_000229.PNG\n",
            "  File: frame_000051.PNG\n",
            "  File: frame_000183.PNG\n",
            "  File: frame_000107.PNG\n",
            "  File: frame_000056.PNG\n",
            "  File: frame_000019.PNG\n",
            "  File: frame_000295.PNG\n",
            "  File: frame_000190.PNG\n",
            "  File: frame_000013.PNG\n",
            "  File: frame_000164.PNG\n",
            "  File: frame_000161.PNG\n",
            "  File: frame_000270.PNG\n",
            "  File: frame_000155.PNG\n",
            "  File: frame_000195.PNG\n",
            "  File: frame_000010.PNG\n",
            "  File: frame_000016.PNG\n",
            "  File: frame_000158.PNG\n",
            "  File: frame_000201.PNG\n",
            "  File: frame_000204.PNG\n",
            "  File: frame_000264.PNG\n",
            "  File: frame_000042.PNG\n",
            "  File: frame_000163.PNG\n",
            "  File: frame_000097.PNG\n",
            "  File: frame_000262.PNG\n",
            "  File: frame_000242.PNG\n",
            "  File: frame_000292.PNG\n",
            "  File: frame_000030.PNG\n",
            "  File: frame_000291.PNG\n",
            "  File: frame_000249.PNG\n",
            "  File: frame_000208.PNG\n",
            "  File: frame_000025.PNG\n",
            "  File: frame_000101.PNG\n",
            "  File: frame_000058.PNG\n",
            "  File: frame_000174.PNG\n",
            "  File: frame_000055.PNG\n",
            "  File: frame_000062.PNG\n",
            "  File: frame_000257.PNG\n",
            "  File: frame_000093.PNG\n",
            "  File: frame_000200.PNG\n",
            "  File: frame_000021.PNG\n",
            "  File: frame_000098.PNG\n",
            "  File: frame_000121.PNG\n",
            "  File: frame_000194.PNG\n",
            "  File: frame_000133.PNG\n",
            "  File: frame_000128.PNG\n",
            "  File: frame_000111.PNG\n",
            "  File: frame_000152.PNG\n",
            "  File: frame_000074.PNG\n",
            "  File: frame_000284.PNG\n",
            "  File: frame_000300.PNG\n",
            "  File: frame_000147.PNG\n",
            "  File: frame_000224.PNG\n",
            "  File: frame_000246.PNG\n",
            "  File: frame_000091.PNG\n",
            "  File: frame_000006.PNG\n",
            "  File: frame_000071.PNG\n",
            "  File: frame_000167.PNG\n",
            "  File: frame_000000.PNG\n",
            "  File: frame_000146.PNG\n",
            "  File: frame_000079.PNG\n",
            "  File: frame_000227.PNG\n",
            "  File: frame_000027.PNG\n",
            "  File: frame_000278.PNG\n",
            "  File: frame_000214.PNG\n",
            "  File: frame_000272.PNG\n",
            "  File: frame_000145.PNG\n",
            "  File: frame_000142.PNG\n",
            "  File: frame_000057.PNG\n",
            "  File: frame_000077.PNG\n",
            "  File: frame_000126.PNG\n",
            "  File: frame_000287.PNG\n",
            "  File: frame_000223.PNG\n",
            "  File: frame_000241.PNG\n",
            "  File: frame_000120.PNG\n",
            "  File: frame_000305.PNG\n",
            "  File: frame_000269.PNG\n",
            "  File: frame_000218.PNG\n",
            "  File: frame_000216.PNG\n",
            "  File: frame_000084.PNG\n",
            "  File: frame_000015.PNG\n",
            "  File: frame_000265.PNG\n",
            "  File: frame_000247.PNG\n",
            "  File: frame_000170.PNG\n",
            "  File: frame_000066.PNG\n",
            "  File: frame_000294.PNG\n",
            "  File: frame_000005.PNG\n",
            "  File: frame_000168.PNG\n",
            "  File: frame_000036.PNG\n",
            "  File: frame_000054.PNG\n",
            "  File: frame_000205.PNG\n",
            "  File: frame_000012.PNG\n",
            "  File: frame_000276.PNG\n",
            "  File: frame_000162.PNG\n",
            "  File: frame_000040.PNG\n",
            "  File: frame_000153.PNG\n",
            "  File: frame_000215.PNG\n",
            "  File: frame_000225.PNG\n",
            "  File: frame_000100.PNG\n",
            "  File: frame_000043.PNG\n",
            "  File: frame_000245.PNG\n",
            "  File: frame_000178.PNG\n",
            "  File: frame_000045.PNG\n",
            "  File: frame_000099.PNG\n",
            "  File: frame_000285.PNG\n",
            "  File: frame_000230.PNG\n",
            "  File: frame_000289.PNG\n",
            "  File: frame_000252.PNG\n",
            "  File: frame_000124.PNG\n",
            "  File: frame_000085.PNG\n",
            "  File: frame_000240.PNG\n",
            "  File: frame_000048.PNG\n",
            "  File: frame_000092.PNG\n",
            "  File: frame_000035.PNG\n",
            "  File: frame_000297.PNG\n",
            "  File: frame_000271.PNG\n",
            "  File: frame_000263.PNG\n",
            "  File: frame_000237.PNG\n",
            "  File: frame_000197.PNG\n",
            "  File: frame_000207.PNG\n",
            "  File: frame_000209.PNG\n",
            "  File: frame_000080.PNG\n",
            "  File: frame_000279.PNG\n",
            "  File: frame_000050.PNG\n",
            "  File: frame_000182.PNG\n",
            "  File: frame_000180.PNG\n",
            "  File: frame_000156.PNG\n",
            "  File: frame_000096.PNG\n",
            "  File: frame_000123.PNG\n",
            "  File: frame_000075.PNG\n",
            "  File: frame_000296.PNG\n",
            "  File: frame_000138.PNG\n",
            "  File: frame_000267.PNG\n",
            "  File: frame_000106.PNG\n",
            "  File: frame_000135.PNG\n",
            "  File: frame_000037.PNG\n",
            "  File: frame_000202.PNG\n",
            "  File: frame_000067.PNG\n",
            "  File: frame_000136.PNG\n",
            "  File: frame_000192.PNG\n",
            "  File: frame_000094.PNG\n",
            "  File: frame_000014.PNG\n",
            "  File: frame_000226.PNG\n",
            "  File: frame_000132.PNG\n",
            "  File: frame_000185.PNG\n",
            "  File: frame_000274.PNG\n",
            "  File: frame_000118.PNG\n",
            "  File: frame_000256.PNG\n",
            "  File: frame_000177.PNG\n",
            "  File: frame_000102.PNG\n",
            "  File: frame_000196.PNG\n",
            "  File: frame_000286.PNG\n",
            "  File: frame_000114.PNG\n",
            "  File: frame_000032.PNG\n",
            "  File: frame_000064.PNG\n",
            "  File: frame_000151.PNG\n",
            "  File: frame_000089.PNG\n",
            "  File: frame_000299.PNG\n",
            "  File: frame_000144.PNG\n",
            "  File: frame_000068.PNG\n",
            "  File: frame_000176.PNG\n",
            "  File: frame_000017.PNG\n",
            "  File: frame_000028.PNG\n",
            "  File: frame_000298.PNG\n",
            "  File: frame_000029.PNG\n",
            "  File: frame_000018.PNG\n",
            "  File: frame_000184.PNG\n",
            "  File: frame_000046.PNG\n",
            "  File: frame_000228.PNG\n",
            "  File: frame_000217.PNG\n",
            "  File: frame_000108.PNG\n",
            "  File: frame_000160.PNG\n",
            "  File: frame_000198.PNG\n",
            "  File: frame_000082.PNG\n",
            "  File: frame_000268.PNG\n",
            "  File: frame_000233.PNG\n",
            "  File: frame_000219.PNG\n",
            "  File: frame_000206.PNG\n",
            "  File: frame_000116.PNG\n",
            "  File: frame_000231.PNG\n",
            "  File: frame_000070.PNG\n",
            "  File: frame_000072.PNG\n",
            "  File: frame_000260.PNG\n",
            "  File: frame_000137.PNG\n",
            "  File: frame_000266.PNG\n",
            "  File: frame_000154.PNG\n",
            "  File: frame_000105.PNG\n",
            "  File: frame_000288.PNG\n",
            "  File: frame_000253.PNG\n",
            "  File: frame_000303.PNG\n",
            "  File: frame_000022.PNG\n",
            "  File: frame_000181.PNG\n",
            "  File: frame_000076.PNG\n",
            "  File: frame_000239.PNG\n",
            "  File: frame_000191.PNG\n",
            "  File: frame_000148.PNG\n",
            "  File: frame_000140.PNG\n",
            "  File: frame_000211.PNG\n",
            "  File: frame_000238.PNG\n",
            "  File: frame_000060.PNG\n",
            "  File: frame_000222.PNG\n",
            "  File: frame_000047.PNG\n",
            "  File: frame_000258.PNG\n",
            "  File: frame_000234.PNG\n",
            "  File: frame_000304.PNG\n",
            "  File: frame_000110.PNG\n",
            "  File: frame_000081.PNG\n",
            "  File: frame_000122.PNG\n",
            "  File: frame_000212.PNG\n",
            "  File: frame_000213.PNG\n",
            "  File: frame_000171.PNG\n",
            "  File: frame_000088.PNG\n",
            "  File: frame_000125.PNG\n",
            "  File: frame_000033.PNG\n",
            "  File: frame_000024.PNG\n",
            "  File: frame_000254.PNG\n",
            "  File: frame_000023.PNG\n",
            "  File: frame_000293.PNG\n",
            "  File: frame_000248.PNG\n",
            "  File: frame_000049.PNG\n",
            "  File: frame_000073.PNG\n",
            "  File: frame_000038.PNG\n",
            "  File: frame_000112.PNG\n",
            "  File: frame_000134.PNG\n",
            "  File: frame_000275.PNG\n",
            "  File: frame_000095.PNG\n",
            "  File: frame_000020.PNG\n",
            "  File: frame_000078.PNG\n",
            "  File: frame_000301.PNG\n",
            "  File: frame_000117.PNG\n",
            "  File: frame_000302.PNG\n",
            "  File: frame_000157.PNG\n",
            "  File: frame_000277.PNG\n",
            "  File: frame_000173.PNG\n",
            "  File: frame_000041.PNG\n",
            "  File: frame_000221.PNG\n",
            "  File: frame_000113.PNG\n",
            "  File: frame_000290.PNG\n",
            "  File: frame_000003.PNG\n",
            "  File: frame_000283.PNG\n",
            "  File: frame_000261.PNG\n",
            "  File: frame_000034.PNG\n",
            "  File: frame_000232.PNG\n",
            "  File: frame_000069.PNG\n",
            "  File: frame_000193.PNG\n",
            "  File: frame_000087.PNG\n",
            "  File: frame_000139.PNG\n",
            "  File: frame_000280.PNG\n",
            "  File: frame_000143.PNG\n",
            "  File: frame_000063.PNG\n",
            "  File: frame_000250.PNG\n",
            "  File: frame_000281.PNG\n",
            "  File: frame_000150.PNG\n",
            "  File: frame_000053.PNG\n",
            "  File: frame_000172.PNG\n",
            "  File: frame_000236.PNG\n",
            "  File: frame_000011.PNG\n",
            "  File: frame_000282.PNG\n",
            "  File: frame_000090.PNG\n",
            "  File: frame_000009.PNG\n",
            "  File: frame_000086.PNG\n",
            "  File: frame_000129.PNG\n",
            "  File: frame_000187.PNG\n",
            "  File: frame_000131.PNG\n",
            "  File: frame_000169.PNG\n",
            "  File: frame_000199.PNG\n",
            "  File: frame_000044.PNG\n",
            "  File: frame_000061.PNG\n",
            "  File: frame_000026.PNG\n",
            "  File: frame_000001.PNG\n",
            "  File: frame_000004.PNG\n",
            "  File: frame_000109.PNG\n",
            "  File: frame_000251.PNG\n",
            "  File: frame_000259.PNG\n",
            "  File: frame_000203.PNG\n",
            "  File: frame_000243.PNG\n",
            "  File: frame_000175.PNG\n",
            "  File: frame_000052.PNG\n",
            "  File: frame_000159.PNG\n",
            "  File: frame_000002.PNG\n",
            "  File: frame_000188.PNG\n",
            "  File: frame_000179.PNG\n",
            "  File: frame_000210.PNG\n",
            "  File: frame_000149.PNG\n",
            "  File: frame_000115.PNG\n",
            "  File: frame_000119.PNG\n",
            "  File: frame_000130.PNG\n",
            "  File: frame_000189.PNG\n",
            "  File: frame_000065.PNG\n",
            "  File: frame_000273.PNG\n",
            "  File: frame_000083.PNG\n",
            "  File: frame_000103.PNG\n",
            "  File: frame_000104.PNG\n",
            "  File: frame_000255.PNG\n",
            "  File: frame_000244.PNG\n",
            "  File: frame_000008.PNG\n",
            "  File: frame_000141.PNG\n",
            "  File: frame_000039.PNG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATASET"
      ],
      "metadata": {
        "id": "pV4zpDHKeLK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle = False\n",
        "base_dir = ''\n",
        "output_dir = ''\n",
        "if kaggle:\n",
        "  base_dir = \"/kaggle/input/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement\"\n",
        "  output_dir = \"/kaggle/working\"\n",
        "else:\n",
        "  base_dir = f'{thebrokenvessel_gescam_partial_path}/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement'\n",
        "  output_dir = '.'\n",
        "xml_path = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\"\n",
        "image_folder = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T09:34:15.483122Z",
          "iopub.execute_input": "2025-04-17T09:34:15.483486Z",
          "iopub.status.idle": "2025-04-17T09:34:15.487338Z",
          "shell.execute_reply.started": "2025-04-17T09:34:15.483463Z",
          "shell.execute_reply": "2025-04-17T09:34:15.486692Z"
        },
        "id": "CUGmKvsOeLK9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OBJECT MASK PIPELINE\n",
        "\n",
        "We are introducing an object mask detection pipeline to detect objects in a classroom scene, this will help us map the gaze heatmaps to detected objects so that we can tell where the students are looking, it could be:\n",
        "\n",
        "1. Books/notebooks\n",
        "2. screens/monitors\n",
        "3. whiteboards/blackboards\n",
        "4. tables/desks\n",
        "5. people (teachers/students"
      ],
      "metadata": {
        "id": "yGx2HA9geLK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_object_masks_from_annotations(frame_data, width, height, num_categories=11):\n",
        "    \"\"\"\n",
        "    Extract object masks from frame annotations\n",
        "\n",
        "    Args:\n",
        "        frame_data: Dictionary containing frame annotations\n",
        "        width: Image width\n",
        "        height: Image height\n",
        "        num_categories: Number of object categories\n",
        "\n",
        "    Returns:\n",
        "        object_masks: Array of shape [num_categories, height, width]\n",
        "    \"\"\"\n",
        "    # Initialize masks\n",
        "    object_masks = np.zeros((num_categories, height, width), dtype=np.float32)\n",
        "\n",
        "    # Map of object labels to category indices\n",
        "    category_map = {\n",
        "        'person': 0,\n",
        "        'teacher': 0,  # Group all people in category 0\n",
        "        'blackboard': 1,\n",
        "        'whiteboard': 1,  # Group all boards in category 1\n",
        "        'notebook': 2,\n",
        "        'book': 2,  # Group all reading materials in category 2\n",
        "        'monitor': 3,\n",
        "        'screen': 3,  # Group all displays in category 3\n",
        "        'mobile': 4,\n",
        "        'phone': 4,  # Group all phones in category 4\n",
        "        'table': 5,\n",
        "        'desk': 5,  # Group all tables in category 5\n",
        "        'water dispenser': 6,\n",
        "        'mug': 7,\n",
        "        'table lamp': 8,\n",
        "        # Add more mappings as needed\n",
        "    }\n",
        "\n",
        "    # Process each box\n",
        "    for box in frame_data[\"boxes\"]:\n",
        "        label = box[\"label\"].lower()\n",
        "\n",
        "        # Extract category\n",
        "        category = -1\n",
        "        for key, idx in category_map.items():\n",
        "            if key in label:\n",
        "                category = idx\n",
        "                break\n",
        "\n",
        "        if category >= 0 and category < num_categories:\n",
        "            # Extract coordinates\n",
        "            x1, y1 = int(box[\"xtl\"]), int(box[\"ytl\"])\n",
        "            x2, y2 = int(box[\"xbr\"]), int(box[\"ybr\"])\n",
        "\n",
        "            # Ensure within bounds\n",
        "            x1 = max(0, min(width-1, x1))\n",
        "            y1 = max(0, min(height-1, y1))\n",
        "            x2 = max(x1+1, min(width, x2))\n",
        "            y2 = max(y1+1, min(height, y2))\n",
        "\n",
        "            # Set mask\n",
        "            object_masks[category, y1:y2, x1:x2] = 1.0\n",
        "\n",
        "    return object_masks"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T09:37:00.43166Z",
          "iopub.execute_input": "2025-04-17T09:37:00.432279Z",
          "iopub.status.idle": "2025-04-17T09:37:00.439001Z",
          "shell.execute_reply.started": "2025-04-17T09:37:00.432225Z",
          "shell.execute_reply": "2025-04-17T09:37:00.438284Z"
        },
        "id": "QjOrQhIEeLK-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_xml(xml_path):\n",
        "    \"\"\"\n",
        "    Parse the XML file and extract bounding boxes and polylines for each frame.\n",
        "\n",
        "    Args:\n",
        "        xml_path (str): Path to the XML file.\n",
        "\n",
        "    Returns:\n",
        "        frames (dict): Dictionary containing frame data (bounding boxes and polylines).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "        frames = {}\n",
        "\n",
        "        print(f\"Parsing XML annotations from {xml_path}\")\n",
        "        print(f\"Root tag: {root.tag}, with {len(root)} child elements\")\n",
        "\n",
        "        frame_count = 0\n",
        "        for image in tqdm(root.findall(\"image\"), desc=\"Parsing frames\"):\n",
        "            try:\n",
        "                frame_id = int(image.attrib[\"id\"])  # Extract frame ID\n",
        "                frame_name = image.attrib[\"name\"]  # Extract frame name\n",
        "                width = int(image.attrib[\"width\"])  # Image width\n",
        "                height = int(image.attrib[\"height\"])  # Image height\n",
        "\n",
        "                # Initialize lists for bounding boxes and polylines\n",
        "                frame_boxes = []\n",
        "                frame_polylines = []\n",
        "\n",
        "                # Extract bounding boxes\n",
        "                for box in image.findall(\"box\"):\n",
        "                    try:\n",
        "                        # Extract all box attributes\n",
        "                        box_info = {\n",
        "                            \"label\": box.attrib.get(\"label\", \"unknown\"),\n",
        "                            \"xtl\": float(box.attrib.get(\"xtl\", 0)),\n",
        "                            \"ytl\": float(box.attrib.get(\"ytl\", 0)),\n",
        "                            \"xbr\": float(box.attrib.get(\"xbr\", 0)),\n",
        "                            \"ybr\": float(box.attrib.get(\"ybr\", 0)),\n",
        "                        }\n",
        "                        frame_boxes.append(box_info)\n",
        "                    except Exception as box_err:\n",
        "                        print(f\"Error parsing box in frame {frame_id}: {box_err}\")\n",
        "\n",
        "                # Extract polylines\n",
        "                for polyline in image.findall(\"polyline\"):\n",
        "                    try:\n",
        "                        polyline_info = {\n",
        "                            \"label\": polyline.attrib.get(\"label\", \"unknown\"),\n",
        "                            \"points\": polyline.attrib.get(\"points\", \"\")\n",
        "                        }\n",
        "                        frame_polylines.append(polyline_info)\n",
        "                    except Exception as polyline_err:\n",
        "                        print(f\"Error parsing polyline in frame {frame_id}: {polyline_err}\")\n",
        "\n",
        "                # Store frame information\n",
        "                frames[frame_id] = {\n",
        "                    \"name\": frame_name,\n",
        "                    \"width\": width,\n",
        "                    \"height\": height,\n",
        "                    \"boxes\": frame_boxes,\n",
        "                    \"polylines\": frame_polylines\n",
        "                }\n",
        "                frame_count += 1\n",
        "\n",
        "                # Debug first frame\n",
        "                if frame_count == 1:\n",
        "                    print(f\"Sample frame: ID={frame_id}, Name={frame_name}, Size={width}x{height}\")\n",
        "                    print(f\"Found {len(frame_boxes)} boxes and {len(frame_polylines)} polylines in first frame\")\n",
        "                    if frame_boxes:\n",
        "                        print(f\"Sample box labels: {[box['label'] for box in frame_boxes[:5]]}\")\n",
        "                    if frame_polylines:\n",
        "                        print(f\"Sample polyline labels: {[p['label'] for p in frame_polylines[:5]]}\")\n",
        "            except Exception as frame_err:\n",
        "                print(f\"Error parsing frame: {frame_err}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Successfully parsed {len(frames)} frames\")\n",
        "        return frames\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing XML file: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {}\n",
        "\n",
        "def create_frame_to_image_mapping(image_folder):\n",
        "    \"\"\"\n",
        "    Create a mapping from frame IDs to image paths.\n",
        "\n",
        "    Args:\n",
        "        image_folder (str): Path to the folder containing images.\n",
        "\n",
        "    Returns:\n",
        "        frame_to_image (dict): Mapping from frame IDs to image paths.\n",
        "    \"\"\"\n",
        "    frame_to_image = {}\n",
        "\n",
        "    if not os.path.exists(image_folder):\n",
        "        print(f\"Warning: Image folder {image_folder} does not exist\")\n",
        "        return frame_to_image\n",
        "\n",
        "    for image_name in os.listdir(image_folder):\n",
        "        if not any(image_name.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.bmp']):\n",
        "            continue\n",
        "\n",
        "        # Try various patterns to extract frame ID\n",
        "        # Pattern 1: frame_000123.jpg/png\n",
        "        match = re.search(r'frame_0*(\\d+)', image_name.lower())\n",
        "        if match:\n",
        "            frame_id = int(match.group(1))\n",
        "            frame_to_image[frame_id] = os.path.join(image_folder, image_name)\n",
        "            continue\n",
        "\n",
        "        # Pattern 2: 000123.jpg\n",
        "        match = re.search(r'^0*(\\d+)', image_name)\n",
        "        if match:\n",
        "            frame_id = int(match.group(1))\n",
        "            frame_to_image[frame_id] = os.path.join(image_folder, image_name)\n",
        "            continue\n",
        "\n",
        "        # Pattern 3: Any number in the filename\n",
        "        match = re.search(r'(\\d+)', image_name)\n",
        "        if match:\n",
        "            frame_id = int(match.group(1))\n",
        "            frame_to_image[frame_id] = os.path.join(image_folder, image_name)\n",
        "\n",
        "    print(f\"Found {len(frame_to_image)} images with extractable frame IDs\")\n",
        "    return frame_to_image\n",
        "\n",
        "class GESCAMCustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for GESCAM (Gaze Estimation based Synthetic Classroom Attention Measurement)\n",
        "    Customized for the specific annotation format\n",
        "    \"\"\"\n",
        "    def __init__(self, xml_path, image_folder, transform=None, head_transform=None,\n",
        "                 input_size=224, output_size=64, test=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            xml_path (str): Path to the XML annotation file\n",
        "            image_folder (str): Path to the folder containing images\n",
        "            transform: Transformations to apply to the scene image\n",
        "            head_transform: Transformations to apply to the head crop\n",
        "            input_size: Input image size for the model\n",
        "            output_size: Output heatmap size\n",
        "            test: Whether this is a test dataset\n",
        "        \"\"\"\n",
        "        super(GESCAMCustomDataset, self).__init__()\n",
        "\n",
        "        self.xml_path = xml_path\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform\n",
        "        self.head_transform = head_transform if head_transform else transform\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.test = test\n",
        "\n",
        "        # Parse annotations and create image mapping\n",
        "        self.frames = parse_xml(xml_path)\n",
        "        self.frame_to_image = create_frame_to_image_mapping(image_folder)\n",
        "\n",
        "        # Create samples\n",
        "        self.samples = self._create_samples()\n",
        "        print(f\"Created dataset with {len(self.samples)} samples\")\n",
        "\n",
        "    def _match_person_to_sight_line(self, person_box, polylines):\n",
        "        \"\"\"\n",
        "        Match a person bounding box to the corresponding line of sight polyline\n",
        "\n",
        "        Args:\n",
        "            person_box: Dictionary containing person bounding box\n",
        "            polylines: List of polyline dictionaries for the frame\n",
        "\n",
        "        Returns:\n",
        "            target_point: (x,y) tuple of gaze target or None if no match\n",
        "            has_target: Boolean indicating if a match was found\n",
        "        \"\"\"\n",
        "        # Find polylines labeled as \"line of sight\"\n",
        "        sight_lines = [p for p in polylines if p[\"label\"].lower() == \"line of sight\"]\n",
        "\n",
        "        if not sight_lines:\n",
        "            return None, False\n",
        "\n",
        "        # Calculate person box center\n",
        "        person_center_x = (person_box[\"xtl\"] + person_box[\"xbr\"]) / 2\n",
        "        person_center_y = (person_box[\"ytl\"] + person_box[\"ybr\"]) / 2\n",
        "        person_width = person_box[\"xbr\"] - person_box[\"xtl\"]\n",
        "\n",
        "        # Find closest matching sight line\n",
        "        best_match = None\n",
        "        best_distance = float('inf')\n",
        "\n",
        "        for polyline in sight_lines:\n",
        "            points_str = polyline[\"points\"]\n",
        "            try:\n",
        "                # Parse points from string format \"x1,y1;x2,y2;...\"\n",
        "                points = [tuple(map(float, point.split(\",\"))) for point in points_str.split(\";\")]\n",
        "\n",
        "                if len(points) >= 2:  # Need at least start and end point\n",
        "                    start_x, start_y = points[0]\n",
        "                    end_x, end_y = points[-1]\n",
        "\n",
        "                    # Calculate distance from polyline start to person center\n",
        "                    distance = np.sqrt((start_x - person_center_x)**2 + (start_y - person_center_y)**2)\n",
        "\n",
        "                    # Check if this is a good match (close to person center)\n",
        "                    if distance < best_distance and distance < person_width * 1.5:\n",
        "                        best_distance = distance\n",
        "                        best_match = (end_x, end_y)  # Use end point as gaze target\n",
        "            except Exception as e:\n",
        "                # Print details for debugging\n",
        "                print(f\"Error parsing polyline points: {e}, points_str: {points_str}\")\n",
        "                continue\n",
        "\n",
        "        return best_match, best_match is not None\n",
        "\n",
        "    def _create_samples(self):\n",
        "        \"\"\"\n",
        "        Create dataset samples from parsed frames\n",
        "\n",
        "        Returns:\n",
        "            samples: List of sample dictionaries\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        frames_with_persons = 0\n",
        "        frames_with_sight_lines = 0\n",
        "\n",
        "        for frame_id, frame_data in self.frames.items():\n",
        "            # Skip frames without matching images\n",
        "            if frame_id not in self.frame_to_image:\n",
        "                continue\n",
        "\n",
        "            image_path = self.frame_to_image[frame_id]\n",
        "            width, height = frame_data[\"width\"], frame_data[\"height\"]\n",
        "\n",
        "            # Extract object masks for this frame\n",
        "            object_masks = extract_object_masks_from_annotations(frame_data, width, height)\n",
        "\n",
        "            # Check if there are person boxes in this frame\n",
        "            person_boxes = [box for box in frame_data[\"boxes\"] if \"person\" in box[\"label\"].lower()]\n",
        "            if person_boxes:\n",
        "                frames_with_persons += 1\n",
        "\n",
        "            # Check if there are line of sight polylines\n",
        "            sight_lines = [p for p in frame_data[\"polylines\"] if p[\"label\"].lower() == \"line of sight\"]\n",
        "            if sight_lines:\n",
        "                frames_with_sight_lines += 1\n",
        "\n",
        "            # Process each person box\n",
        "            for person_box in person_boxes:\n",
        "                # Find matching sight line\n",
        "                gaze_target, has_target = self._match_person_to_sight_line(person_box, frame_data[\"polylines\"])\n",
        "\n",
        "                # Create sample\n",
        "                sample = {\n",
        "                    \"frame_id\": frame_id,\n",
        "                    \"image_path\": image_path,\n",
        "                    \"width\": width,\n",
        "                    \"height\": height,\n",
        "                    \"head_bbox\": [person_box[\"xtl\"], person_box[\"ytl\"], person_box[\"xbr\"], person_box[\"ybr\"]],\n",
        "                    \"gaze_target\": gaze_target,\n",
        "                    \"in_frame\": has_target,\n",
        "                    \"object_masks\": object_masks  # Add object masks\n",
        "                }\n",
        "\n",
        "                samples.append(sample)\n",
        "\n",
        "        print(f\"Statistics: {frames_with_persons} frames with person boxes, {frames_with_sight_lines} frames with sight lines\")\n",
        "        return samples\n",
        "\n",
        "    def _create_head_position_channel(self, head_bbox, width, height):\n",
        "        \"\"\"\n",
        "        Create a binary mask for head position\n",
        "        \"\"\"\n",
        "        x1, y1, x2, y2 = head_bbox\n",
        "        head_mask = torch.zeros(height, width)\n",
        "        x1, y1, x2, y2 = int(max(0, x1)), int(max(0, y1)), int(min(width, x2)), int(min(height, y2))\n",
        "        head_mask[y1:y2, x1:x2] = 1.0\n",
        "        return head_mask\n",
        "\n",
        "    def _create_gaze_heatmap(self, gaze_target, width, height):\n",
        "        \"\"\"\n",
        "        Create a Gaussian heatmap at the gaze point\n",
        "        \"\"\"\n",
        "        if not gaze_target:\n",
        "            return torch.zeros(self.output_size, self.output_size)\n",
        "\n",
        "        x, y = gaze_target\n",
        "\n",
        "        # Scale coordinates to output size\n",
        "        x = x * self.output_size / width\n",
        "        y = y * self.output_size / height\n",
        "\n",
        "        # Create meshgrid\n",
        "        Y, X = torch.meshgrid(torch.arange(self.output_size), torch.arange(self.output_size), indexing='ij')\n",
        "\n",
        "        # Create Gaussian heatmap\n",
        "        sigma = 3.0\n",
        "        heatmap = torch.exp(-((X - x) ** 2 + (Y - y) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "        return heatmap\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            img = Image.open(sample[\"image_path\"]).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {sample['image_path']}: {e}\")\n",
        "            # Return a placeholder if image can't be loaded\n",
        "            img = Image.new('RGB', (self.input_size, self.input_size), color='gray')\n",
        "\n",
        "        width, height = sample[\"width\"], sample[\"height\"]\n",
        "\n",
        "        # Extract head crop\n",
        "        head_bbox = sample[\"head_bbox\"]\n",
        "        x1, y1, x2, y2 = head_bbox\n",
        "\n",
        "        # Ensure bbox is within image bounds\n",
        "        x1 = max(0, min(width-1, x1))\n",
        "        y1 = max(0, min(height-1, y1))\n",
        "        x2 = max(x1+1, min(width, x2))\n",
        "        y2 = max(y1+1, min(height, y2))\n",
        "\n",
        "        try:\n",
        "            head_img = img.crop((int(x1), int(y1), int(x2), int(y2)))\n",
        "        except Exception as e:\n",
        "            print(f\"Error cropping head: {e}, bbox: {head_bbox}, image size: {img.size}\")\n",
        "            head_img = Image.new('RGB', (100, 100), color='gray')\n",
        "\n",
        "        # Create head position channel\n",
        "        head_pos = self._create_head_position_channel(head_bbox, width, height)\n",
        "\n",
        "        # Get object masks\n",
        "        object_masks = torch.from_numpy(sample[\"object_masks\"])\n",
        "\n",
        "        # Create gaze heatmap\n",
        "        if sample[\"in_frame\"] and sample[\"gaze_target\"]:\n",
        "            gaze_target = sample[\"gaze_target\"]\n",
        "            gaze_heatmap = self._create_gaze_heatmap(gaze_target, width, height)\n",
        "\n",
        "            # Calculate gaze vector (from head center to gaze point)\n",
        "            head_center_x = (x1 + x2) / 2 / width\n",
        "            head_center_y = (y1 + y2) / 2 / height\n",
        "            gaze_x = gaze_target[0] / width\n",
        "            gaze_y = gaze_target[1] / height\n",
        "            gaze_vector = torch.tensor([gaze_x - head_center_x, gaze_y - head_center_y])\n",
        "        else:\n",
        "            gaze_heatmap = torch.zeros(self.output_size, self.output_size)\n",
        "            gaze_vector = torch.tensor([0.0, 0.0])  # Default for out-of-frame\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.head_transform:\n",
        "            head_img = self.head_transform(head_img)\n",
        "\n",
        "        # Resize head position to match input size\n",
        "        head_pos = head_pos.unsqueeze(0)\n",
        "        head_pos = F.interpolate(head_pos.unsqueeze(0), size=(self.input_size, self.input_size),\n",
        "                                 mode='nearest').squeeze(0)\n",
        "\n",
        "        # Resize object masks to match input size\n",
        "        object_masks = F.interpolate(object_masks.unsqueeze(0),\n",
        "                                     size=(self.input_size, self.input_size),\n",
        "                                     mode='nearest').squeeze(0)\n",
        "\n",
        "        in_frame = torch.tensor([float(sample[\"in_frame\"])])\n",
        "\n",
        "        # For compatibility with existing code\n",
        "        object_label = torch.tensor([0])  # placeholder\n",
        "\n",
        "        # Instead of returning frame_id as last element (which might cause issues with batching),\n",
        "        # return a metadata dictionary alongside the tensors\n",
        "        metadata = {\n",
        "            \"frame_id\": sample[\"frame_id\"],\n",
        "            \"image_path\": sample[\"image_path\"],\n",
        "            \"head_bbox\": sample[\"head_bbox\"],\n",
        "            \"original_size\": (width, height)\n",
        "        }\n",
        "\n",
        "        return img, head_img, head_pos, gaze_heatmap, in_frame, object_label, gaze_vector, object_masks, metadata\n",
        "\n",
        "def get_transforms(input_size=224, augment=True):\n",
        "    \"\"\"\n",
        "    Get data transformations for training and validation\n",
        "    \"\"\"\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "\n",
        "    if augment:\n",
        "        # Training transforms with augmentation\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    else:\n",
        "        # Validation/test transforms without augmentation\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "\n",
        "    return transform\n",
        "\n",
        "\n",
        "def visualize_sample_with_objects(sample, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize a dataset sample with object masks\n",
        "\n",
        "    Args:\n",
        "        sample: Tuple of tensors from dataset __getitem__\n",
        "        save_path: Path to save visualization (if None, displays inline)\n",
        "    \"\"\"\n",
        "    img, head_img, head_pos, gaze_heatmap, in_frame, object_label, gaze_vector, object_masks, metadata = sample\n",
        "\n",
        "    # Denormalize image\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    img_vis = img.clone()\n",
        "    img_vis = img_vis * std + mean\n",
        "    img_vis = img_vis.permute(1, 2, 0).numpy()\n",
        "    img_vis = np.clip(img_vis, 0, 1)\n",
        "\n",
        "    head_img_vis = head_img.clone()\n",
        "    head_img_vis = head_img_vis * std + mean\n",
        "    head_img_vis = head_img_vis.permute(1, 2, 0).numpy()\n",
        "    head_img_vis = np.clip(head_img_vis, 0, 1)\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(15, 12))\n",
        "\n",
        "    # Create a 3x3 grid\n",
        "    plt.subplot(3, 3, 1)\n",
        "    plt.imshow(img_vis)\n",
        "    plt.title(f\"Frame ID: {metadata['frame_id']}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(3, 3, 2)\n",
        "    plt.imshow(head_img_vis)\n",
        "    plt.title(\"Head/Person Crop\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(3, 3, 3)\n",
        "    plt.imshow(head_pos.squeeze().numpy(), cmap='gray')\n",
        "    plt.title(\"Head Position Channel\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display select object mask channels\n",
        "    category_names = [\"People\", \"Boards\", \"Books\", \"Monitors\", \"Phones\",\n",
        "                     \"Tables\", \"Water Disp.\", \"Mugs\", \"Lamps\", \"Other1\", \"Other2\"]\n",
        "\n",
        "    # Display first few object masks\n",
        "    for i in range(min(4, object_masks.shape[0])):\n",
        "        plt.subplot(3, 3, 4 + i)\n",
        "        plt.imshow(object_masks[i].numpy(), cmap='viridis')\n",
        "        plt.title(f\"Object: {category_names[i]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Gaze heatmap and visualizations\n",
        "    plt.subplot(3, 3, 8)\n",
        "    plt.imshow(gaze_heatmap.numpy(), cmap='jet')\n",
        "    plt.title(f\"Gaze Heatmap (In-frame: {bool(in_frame.item())})\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(3, 3, 9)\n",
        "    # Original image with heatmap overlay\n",
        "    plt.imshow(img_vis)\n",
        "\n",
        "    # Resize heatmap to match image size for overlay\n",
        "    heatmap_vis = gaze_heatmap.numpy()\n",
        "    heatmap_vis = cv2.resize(heatmap_vis, (img_vis.shape[1], img_vis.shape[0]))\n",
        "\n",
        "    # Only show heatmap if gaze is in frame\n",
        "    if in_frame.item():\n",
        "        plt.imshow(heatmap_vis, cmap='jet', alpha=0.5)\n",
        "\n",
        "    plt.title(\"Heatmap Overlay\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def test_dataset(xml_path, image_folder):\n",
        "    \"\"\"\n",
        "    Test the dataset with visualization\n",
        "\n",
        "    Args:\n",
        "        xml_path: Path to the XML annotation file\n",
        "        image_folder: Path to the folder with images\n",
        "    \"\"\"\n",
        "    # Create transforms\n",
        "    transform = get_transforms(augment=False)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GESCAMCustomDataset(\n",
        "        xml_path=xml_path,\n",
        "        image_folder=image_folder,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Check dataset size\n",
        "    print(f\"\\nDataset contains {len(dataset)} samples\")\n",
        "\n",
        "    # If dataset has samples, visualize some\n",
        "    if len(dataset) > 0:\n",
        "        print(\"\\nVisualizing samples:\")\n",
        "        num_samples = min(3, len(dataset))\n",
        "        for i in range(num_samples):\n",
        "            # Get a sample\n",
        "            sample_idx = i\n",
        "            sample = dataset[sample_idx]\n",
        "\n",
        "            # Visualize\n",
        "            save_path = f\"sample_with_objects_{i}.png\"\n",
        "            visualize_sample_with_objects(sample, save_path)\n",
        "            print(f\"Sample {i} visualization saved to {save_path}\")\n",
        "    else:\n",
        "        print(\"No samples to visualize!\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example paths (replace with actual paths)\n",
        "    print(\"hey\")\n",
        "    print(xml_path)\n",
        "    xml_path = xml_path\n",
        "    image_folder = image_folder\n",
        "\n",
        "    # Test the dataset\n",
        "    dataset = test_dataset(xml_path, image_folder)\n",
        "\n",
        "    # Create DataLoader if we have samples\n",
        "    if len(dataset) > 0:\n",
        "        batch_size = 4\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Test the DataLoader by fetching a batch\n",
        "        for batch in dataloader:\n",
        "            print(f\"Successfully loaded a batch of size {len(batch[0])}\")\n",
        "            break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T09:40:33.086824Z",
          "iopub.execute_input": "2025-04-17T09:40:33.08711Z",
          "iopub.status.idle": "2025-04-17T09:40:38.710623Z",
          "shell.execute_reply.started": "2025-04-17T09:40:33.087088Z",
          "shell.execute_reply": "2025-04-17T09:40:38.70988Z"
        },
        "id": "HXcwFlyIeLK_",
        "outputId": "b45cfa32-16d6-4e51-f438-954a8f2bae59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey\n",
            "/root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n",
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n",
            "Root tag: annotations, with 307 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 13816.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 56 boxes and 14 polylines in first frame\n",
            "Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 305 frames\n",
            "Found 305 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 305 frames with person boxes, 305 frames with sight lines\n",
            "Created dataset with 4575 samples\n",
            "\n",
            "Dataset contains 4575 samples\n",
            "\n",
            "Visualizing samples:\n",
            "Sample 0 visualization saved to sample_with_objects_0.png\n",
            "Sample 1 visualization saved to sample_with_objects_1.png\n",
            "Sample 2 visualization saved to sample_with_objects_2.png\n",
            "Successfully loaded a batch of size 4\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Set paths to your data\n",
        "    xml_path = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\"\n",
        "    image_folder = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\"\n",
        "\n",
        "\n",
        "    # Create transforms\n",
        "    transform = get_transforms(augment=False)\n",
        "\n",
        "    print(\"Creating dataset...\")\n",
        "    # Create dataset with the customized class\n",
        "    dataset = GESCAMCustomDataset(\n",
        "        xml_path=xml_path,\n",
        "        image_folder=image_folder,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # If we have samples, create a DataLoader and visualize\n",
        "    if len(dataset) > 0:\n",
        "        # Create DataLoader\n",
        "        batch_size = 4\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        print(f\"Created DataLoader with batch size {batch_size}\")\n",
        "\n",
        "        # Visualize some samples\n",
        "        print(\"Visualizing samples...\")\n",
        "        num_samples = min(5, len(dataset))\n",
        "        for i in range(num_samples):\n",
        "            # Choose random sample for variety\n",
        "            sample_idx = np.random.randint(0, len(dataset))\n",
        "            sample = dataset[sample_idx]\n",
        "\n",
        "            # Visualize\n",
        "            save_path = os.path.join(output_dir, f\"sample_{i}.png\")\n",
        "            # Use the new visualization function that handles object masks\n",
        "            visualize_sample_with_objects(sample, save_path)\n",
        "            print(f\"Sample {i} visualization saved to {save_path}\")\n",
        "\n",
        "        # Create a video visualization\n",
        "        create_visualization_video(dataset, os.path.join(output_dir, \"visualization.mp4\"),\n",
        "                                  num_samples=min(30, len(dataset)), fps=2)\n",
        "    else:\n",
        "        print(\"No samples found in the dataset!\")\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "def create_visualization_video(dataset, output_video_path, num_samples=20, fps=5):\n",
        "    \"\"\"\n",
        "    Create a video visualizing dataset samples\n",
        "\n",
        "    Args:\n",
        "        dataset: Dataset instance\n",
        "        output_video_path: Path to save the video\n",
        "        num_samples: Number of samples to include\n",
        "        fps: Frames per second\n",
        "    \"\"\"\n",
        "    if len(dataset) == 0:\n",
        "        print(\"Cannot create video with empty dataset\")\n",
        "        return\n",
        "\n",
        "    print(f\"Creating visualization video with {num_samples} samples...\")\n",
        "\n",
        "    # Create a temporary directory for frames\n",
        "    temp_dir = \"temp_viz_frames\"\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Get evenly distributed sample indices\n",
        "    indices = np.linspace(0, len(dataset)-1, num_samples).astype(int)\n",
        "\n",
        "    # Visualize each sample\n",
        "    for i, idx in enumerate(tqdm(indices, desc=\"Generating frames\")):\n",
        "        sample = dataset[idx]\n",
        "\n",
        "        # Save visualization to temp file\n",
        "        temp_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n",
        "        # Use the new visualization function that handles object masks\n",
        "        visualize_sample_with_objects(sample, temp_path)\n",
        "\n",
        "    # Get size of the first frame to set video dimensions\n",
        "    first_frame = cv2.imread(os.path.join(temp_dir, \"frame_0000.png\"))\n",
        "    height, width, _ = first_frame.shape  # Fixed the syntax error here\n",
        "\n",
        "    # Create video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Add frames to video\n",
        "    for i in range(len(indices)):\n",
        "        frame_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n",
        "        frame = cv2.imread(frame_path)\n",
        "        video_writer.write(frame)\n",
        "\n",
        "    # Release video writer\n",
        "    video_writer.release()\n",
        "\n",
        "    # Clean up temporary files\n",
        "    for i in range(len(indices)):\n",
        "        frame_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n",
        "        if os.path.exists(frame_path):\n",
        "            os.remove(frame_path)\n",
        "    if os.path.exists(temp_dir):\n",
        "        os.rmdir(temp_dir)\n",
        "\n",
        "    print(f\"Visualization video saved to {output_video_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T09:44:14.943305Z",
          "iopub.execute_input": "2025-04-17T09:44:14.943978Z",
          "iopub.status.idle": "2025-04-17T09:44:57.715947Z",
          "shell.execute_reply.started": "2025-04-17T09:44:14.943953Z",
          "shell.execute_reply": "2025-04-17T09:44:57.715292Z"
        },
        "id": "Utr8fSG9eLLB",
        "outputId": "b1ef7274-c7f2-4929-d90e-f98e6e19d21f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset...\n",
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n",
            "Root tag: annotations, with 307 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 12403.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 56 boxes and 14 polylines in first frame\n",
            "Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 305 frames\n",
            "Found 305 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 305 frames with person boxes, 305 frames with sight lines\n",
            "Created dataset with 4575 samples\n",
            "Created DataLoader with batch size 4\n",
            "Visualizing samples...\n",
            "Sample 0 visualization saved to ./sample_0.png\n",
            "Sample 1 visualization saved to ./sample_1.png\n",
            "Sample 2 visualization saved to ./sample_2.png\n",
            "Sample 3 visualization saved to ./sample_3.png\n",
            "Sample 4 visualization saved to ./sample_4.png\n",
            "Creating visualization video with 30 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating frames: 100%|██████████| 30/30 [00:27<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualization video saved to ./visualization.mp4\n",
            "Done!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMBINING MULTIPLE DATA FOLDERS"
      ],
      "metadata": {
        "id": "ZAuIZNyLeLLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_datasets(xml_paths, image_folders, transform):\n",
        "    \"\"\"\n",
        "    Combine multiple datasets into one\n",
        "\n",
        "    Args:\n",
        "        xml_paths: List of paths to XML annotation files\n",
        "        image_folders: List of paths to image folders\n",
        "        transform: Transformations to apply\n",
        "\n",
        "    Returns:\n",
        "        combined_dataset: Combined dataset\n",
        "    \"\"\"\n",
        "    all_datasets = []\n",
        "\n",
        "    for xml_path, image_folder in zip(xml_paths, image_folders):\n",
        "        dataset = GESCAMCustomDataset(\n",
        "            xml_path=xml_path,\n",
        "            image_folder=image_folder,\n",
        "            transform=transform\n",
        "        )\n",
        "        all_datasets.append(dataset)\n",
        "\n",
        "    # Create a simple wrapper dataset class\n",
        "    class CombinedDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, datasets):\n",
        "            self.datasets = datasets\n",
        "            self.lengths = [len(d) for d in datasets]\n",
        "            self.cumulative_lengths = [0]\n",
        "\n",
        "            for length in self.lengths:\n",
        "                self.cumulative_lengths.append(self.cumulative_lengths[-1] + length)\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.cumulative_lengths[-1]\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            # Find which dataset this index belongs to\n",
        "            dataset_idx = bisect.bisect_right(self.cumulative_lengths, idx) - 1\n",
        "            sample_idx = idx - self.cumulative_lengths[dataset_idx]\n",
        "            return self.datasets[dataset_idx][sample_idx]\n",
        "\n",
        "    return CombinedDataset(all_datasets)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T09:53:46.147576Z",
          "iopub.execute_input": "2025-04-17T09:53:46.14787Z",
          "iopub.status.idle": "2025-04-17T09:53:46.154116Z",
          "shell.execute_reply.started": "2025-04-17T09:53:46.147848Z",
          "shell.execute_reply": "2025-04-17T09:53:46.153491Z"
        },
        "id": "0Y8ju0JZeLLC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all your data paths\n",
        "xml_paths = [\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\",\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video02(0-300)/annotations.xml\",\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video02(301-600)/annotations.xml\",\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video03_final/annotations.xml\",\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video04_final/annotations.xml\",\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video05_final/annotations.xml\"\n",
        "\n",
        "]\n",
        "\n",
        "image_folders = [\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\",\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video02(0-300)/images\",\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video02(301-600)/images\",\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video03_final/images\",\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video04_final/images\",\n",
        "    f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video05_final/images\"\n",
        "\n",
        "]\n",
        "\n",
        "# Create transforms\n",
        "transform = get_transforms(augment=True)\n",
        "\n",
        "# Combine datasets\n",
        "combined_dataset = combine_datasets(xml_paths, image_folders, transform)\n",
        "\n",
        "# Rest of your code remains the same, just use combined_dataset instead of dataset\n",
        "# train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size], generator=generator)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T09:54:48.640372Z",
          "iopub.execute_input": "2025-04-17T09:54:48.640925Z",
          "iopub.status.idle": "2025-04-17T09:55:04.18437Z",
          "shell.execute_reply.started": "2025-04-17T09:54:48.6409Z",
          "shell.execute_reply": "2025-04-17T09:55:04.183565Z"
        },
        "id": "GUxIYaP3eLLC",
        "outputId": "05f5e87c-9ac8-4a18-88d3-6abaab44696e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n",
            "Root tag: annotations, with 307 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 12649.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 56 boxes and 14 polylines in first frame\n",
            "Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 305 frames\n",
            "Found 305 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 305 frames with person boxes, 305 frames with sight lines\n",
            "Created dataset with 4575 samples\n",
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(0-300)/annotations.xml\n",
            "Root tag: annotations, with 308 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 306/306 [00:00<00:00, 14590.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 52 boxes and 14 polylines in first frame\n",
            "Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 306 frames\n",
            "Found 306 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 306 frames with person boxes, 306 frames with sight lines\n",
            "Created dataset with 4284 samples\n",
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(301-600)/annotations.xml\n",
            "Root tag: annotations, with 307 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 13593.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 49 boxes and 14 polylines in first frame\n",
            "Sample box labels: ['Water Dispenser', 'monitor', 'monitor', 'monitor', 'monitor']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 305 frames\n",
            "Found 305 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 305 frames with person boxes, 305 frames with sight lines\n",
            "Created dataset with 4555 samples\n",
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video03_final/annotations.xml\n",
            "Root tag: annotations, with 603 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 601/601 [00:00<00:00, 12808.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 59 boxes and 14 polylines in first frame\n",
            "Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 601 frames\n",
            "Found 601 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 601 frames with person boxes, 601 frames with sight lines\n",
            "Created dataset with 9015 samples\n",
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video04_final/annotations.xml\n",
            "Root tag: annotations, with 602 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 600/600 [00:00<00:00, 15539.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 45 boxes and 13 polylines in first frame\n",
            "Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 600 frames\n",
            "Found 600 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 600 frames with person boxes, 600 frames with sight lines\n",
            "Created dataset with 8400 samples\n",
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video05_final/annotations.xml\n",
            "Root tag: annotations, with 602 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 600/600 [00:00<00:00, 19501.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 35 boxes and 12 polylines in first frame\n",
            "Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 600 frames\n",
            "Found 600 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 600 frames with person boxes, 600 frames with sight lines\n",
            "Created dataset with 7260 samples\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL ARCHITECTURE"
      ],
      "metadata": {
        "id": "AemL0bY0eLLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Soft attention module for attending to scene features based on head features\n",
        "    \"\"\"\n",
        "    def __init__(self, head_channels=256, output_size=(7, 7)):\n",
        "        super(SoftAttention, self).__init__()\n",
        "        self.output_h, self.output_w = output_size\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(head_channels, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, self.output_h * self.output_w),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, head_features):\n",
        "        # Input head_features shape: [batch_size, head_channels]\n",
        "        batch_size = head_features.size(0)\n",
        "\n",
        "        # Generate attention weights\n",
        "        attn_weights = self.attention(head_features)\n",
        "\n",
        "        # Reshape to spatial attention map\n",
        "        attn_weights = attn_weights.view(batch_size, 1, self.output_h, self.output_w)\n",
        "\n",
        "        return attn_weights\n",
        "\n",
        "\n",
        "class MSGESCAMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Stream GESCAM architecture for gaze estimation in classroom settings\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained=True, output_size=64):\n",
        "        super(MSGESCAMModel, self).__init__()\n",
        "\n",
        "        # Store the output size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Feature dimensions\n",
        "        self.backbone_dim = 512  # ResNet18 outputs 512 feature channels\n",
        "        self.feature_dim = 256\n",
        "\n",
        "        # Downsampled feature map size\n",
        "        self.map_size = 7  # ResNet outputs 7x7 feature maps\n",
        "\n",
        "        # === Scene Pathway ===\n",
        "        # Load a pre-trained ResNet18 without the final layer\n",
        "        self.scene_backbone = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "        # Save the original conv1 weights\n",
        "        original_conv1_weight = self.scene_backbone.conv1.weight.clone()\n",
        "\n",
        "        # Create a new conv1 layer that accepts 4 channels (RGB + head position)\n",
        "        self.scene_backbone.conv1 = nn.Conv2d(\n",
        "            4, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "\n",
        "        # Initialize with the pre-trained weights\n",
        "        with torch.no_grad():\n",
        "            self.scene_backbone.conv1.weight[:, :3] = original_conv1_weight\n",
        "            # Initialize the new channel with small random values\n",
        "            self.scene_backbone.conv1.weight[:, 3] = 0.01 * torch.randn_like(self.scene_backbone.conv1.weight[:, 0])\n",
        "\n",
        "        # Remove the final FC layer from the scene backbone\n",
        "        self.scene_features = nn.Sequential(*list(self.scene_backbone.children())[:-1])\n",
        "\n",
        "        # Add a FC layer to transform from backbone_dim to feature_dim\n",
        "        self.scene_fc = nn.Linear(self.backbone_dim, self.feature_dim)\n",
        "\n",
        "        # === Head Pathway ===\n",
        "        # Load another pre-trained ResNet18 for the head pathway\n",
        "        self.head_backbone = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "        # Remove the final FC layer from the head backbone\n",
        "        self.head_features = nn.Sequential(*list(self.head_backbone.children())[:-1])\n",
        "\n",
        "        # Add a FC layer to transform from backbone_dim to feature_dim\n",
        "        self.head_fc = nn.Linear(self.backbone_dim, self.feature_dim)\n",
        "\n",
        "        # === Objects Mask Enhancement (optional) ===\n",
        "        # This takes an objects mask (with channels for different object classes)\n",
        "        self.objects_conv = nn.Conv2d(11, 512, kernel_size=3, stride=2, padding=1)  # 11 object categories\n",
        "\n",
        "        # Soft attention mechanism\n",
        "        self.attention = SoftAttention(head_channels=self.feature_dim, output_size=(self.map_size, self.map_size))\n",
        "\n",
        "        # === Fusion and Encoding ===\n",
        "        # Fusion of attended scene features and head features\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Conv2d(self.backbone_dim + self.feature_dim, self.feature_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(self.feature_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(self.feature_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Calculate the number of deconvolution layers needed\n",
        "        # Each layer doubles the size, so we need log2(output_size / 7) layers\n",
        "        self.num_deconv_layers = max(1, int(math.log2(output_size / 7)) + 1)\n",
        "\n",
        "        # === Decoding for heatmap generation ===\n",
        "        deconv_layers = []\n",
        "        in_channels = self.feature_dim\n",
        "        out_size = self.map_size\n",
        "\n",
        "        # Create deconvolution layers\n",
        "        for i in range(self.num_deconv_layers - 1):\n",
        "            # Calculate output channels\n",
        "            out_channels = max(32, in_channels // 2)\n",
        "\n",
        "            # Add deconv layer\n",
        "            deconv_layers.extend([\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ])\n",
        "\n",
        "            in_channels = out_channels\n",
        "            out_size *= 2\n",
        "\n",
        "        # Final layer to adjust to exact output size\n",
        "        if out_size != output_size:\n",
        "            # Add a final layer with correct stride to reach exactly output_size\n",
        "            scale_factor = output_size / out_size\n",
        "            stride = 2 if scale_factor > 1 else 1\n",
        "            output_padding = 1 if scale_factor > 1 else 0\n",
        "\n",
        "            deconv_layers.extend([\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels, 1, kernel_size=3,\n",
        "                    stride=stride, padding=1, output_padding=output_padding\n",
        "                )\n",
        "            ])\n",
        "        else:\n",
        "            # If we're already at the right size, just add a 1x1 conv\n",
        "            deconv_layers.append(nn.Conv2d(in_channels, 1, kernel_size=1))\n",
        "\n",
        "        self.decode = nn.Sequential(*deconv_layers)\n",
        "\n",
        "        # === In-frame probability prediction ===\n",
        "        self.in_frame_fc = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, scene_img, head_img, head_pos, objects_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the MS-GESCAM network\n",
        "\n",
        "        Args:\n",
        "            scene_img: Scene image tensor [batch_size, 3, H, W]\n",
        "            head_img: Head crop tensor [batch_size, 3, H, W]\n",
        "            head_pos: Head position mask [batch_size, 1, H, W]\n",
        "            objects_mask: Optional mask of object categories [batch_size, num_categories, H, W]\n",
        "\n",
        "        Returns:\n",
        "            heatmap: Predicted gaze heatmap [batch_size, 1, output_size, output_size]\n",
        "            in_frame: Probability of gaze target being in frame [batch_size, 1]\n",
        "        \"\"\"\n",
        "        batch_size = scene_img.size(0)\n",
        "\n",
        "        # === Process scene pathway ===\n",
        "        # Concatenate scene image and head position channel\n",
        "        scene_input = torch.cat([scene_img, head_pos], dim=1)\n",
        "\n",
        "        # Process through ResNet layers until layer4 (skipping the final global pooling and FC)\n",
        "        x = self.scene_backbone.conv1(scene_input)\n",
        "        x = self.scene_backbone.bn1(x)\n",
        "        x = self.scene_backbone.relu(x)\n",
        "        x = self.scene_backbone.maxpool(x)\n",
        "\n",
        "        x = self.scene_backbone.layer1(x)\n",
        "        x = self.scene_backbone.layer2(x)\n",
        "        x = self.scene_backbone.layer3(x)\n",
        "        scene_features_map = self.scene_backbone.layer4(x)  # [batch_size, 512, 7, 7]\n",
        "\n",
        "        # Global average pooling for scene features\n",
        "        scene_vector = F.adaptive_avg_pool2d(scene_features_map, (1, 1)).view(batch_size, -1)\n",
        "        scene_features = self.scene_fc(scene_vector)  # [batch_size, feature_dim]\n",
        "\n",
        "        # === Process head pathway ===\n",
        "        # Process through the entire head features extractor\n",
        "        head_vector = self.head_features(head_img).view(batch_size, -1)  # [batch_size, 512]\n",
        "        head_features = self.head_fc(head_vector)  # [batch_size, feature_dim]\n",
        "\n",
        "        # Process objects mask if provided\n",
        "        if objects_mask is not None:\n",
        "            obj_features = self.objects_conv(objects_mask)\n",
        "            # Resize to match scene features map if needed\n",
        "            if obj_features.size(2) != scene_features_map.size(2):\n",
        "                obj_features = F.adaptive_avg_pool2d(\n",
        "                    obj_features, (scene_features_map.size(2), scene_features_map.size(3))\n",
        "                )\n",
        "            # Add object features to scene features\n",
        "            scene_features_map = scene_features_map + obj_features\n",
        "\n",
        "        # Generate attention map from head features\n",
        "        attn_weights = self.attention(head_features)  # [batch_size, 1, 7, 7]\n",
        "\n",
        "        # Apply attention to scene features map\n",
        "        attended_scene = scene_features_map * attn_weights  # [batch_size, 512, 7, 7]\n",
        "\n",
        "        # Reshape head features to concatenate with scene features\n",
        "        head_features_map = head_features.view(batch_size, self.feature_dim, 1, 1)\n",
        "        head_features_map = head_features_map.expand(-1, -1, self.map_size, self.map_size)\n",
        "\n",
        "        # Concatenate attended scene features and head features\n",
        "        concat_features = torch.cat([attended_scene, head_features_map], dim=1)  # [batch_size, 512+256, 7, 7]\n",
        "\n",
        "        # Encode the concatenated features\n",
        "        encoded = self.encode(concat_features)  # [batch_size, 256, 7, 7]\n",
        "\n",
        "        # Predict in-frame probability\n",
        "        in_frame = self.in_frame_fc(head_features)\n",
        "\n",
        "        # Decode to get the final heatmap\n",
        "        heatmap = self.decode(encoded)\n",
        "\n",
        "        # Ensure output size is correct\n",
        "        if heatmap.size(2) != self.output_size or heatmap.size(3) != self.output_size:\n",
        "            heatmap = F.interpolate(\n",
        "                heatmap,\n",
        "                size=(self.output_size, self.output_size),\n",
        "                mode='bilinear',\n",
        "                align_corners=True\n",
        "            )\n",
        "\n",
        "        # Apply sigmoid to get values between 0 and 1\n",
        "        heatmap = torch.sigmoid(heatmap)\n",
        "\n",
        "        return heatmap, in_frame\n",
        "\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined loss function for gaze estimation\n",
        "    \"\"\"\n",
        "    def __init__(self, heatmap_weight=1.0, in_frame_weight=1.0, angular_weight=0.5):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.heatmap_weight = heatmap_weight\n",
        "        self.in_frame_weight = in_frame_weight\n",
        "        self.angular_weight = angular_weight\n",
        "\n",
        "        self.mse_loss = nn.MSELoss(reduction='none')\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, pred_heatmap, target_heatmap, pred_in_frame, target_in_frame,\n",
        "                pred_vector=None, target_vector=None):\n",
        "        \"\"\"\n",
        "        Combined loss function\n",
        "\n",
        "        Args:\n",
        "            pred_heatmap: Predicted gaze heatmap [batch_size, 1, H, W]\n",
        "            target_heatmap: Target gaze heatmap [batch_size, H, W]\n",
        "            pred_in_frame: Predicted in-frame probability [batch_size, 1]\n",
        "            target_in_frame: Target in-frame label [batch_size, 1]\n",
        "            pred_vector: Optional predicted gaze vector [batch_size, 2]\n",
        "            target_vector: Optional target gaze vector [batch_size, 2]\n",
        "\n",
        "        Returns:\n",
        "            total_loss: Combined loss\n",
        "            loss_dict: Dictionary with individual loss components\n",
        "        \"\"\"\n",
        "        batch_size = pred_heatmap.size(0)\n",
        "\n",
        "        # Check and fix size mismatches\n",
        "        if pred_heatmap.size(-1) != target_heatmap.size(-1) or pred_heatmap.size(-2) != target_heatmap.size(-2):\n",
        "            # Resize prediction to match target\n",
        "            pred_heatmap = F.interpolate(\n",
        "                pred_heatmap,\n",
        "                size=(target_heatmap.size(1), target_heatmap.size(2)),\n",
        "                mode='bilinear',\n",
        "                align_corners=True\n",
        "            )\n",
        "\n",
        "        # Reshape heatmaps if needed\n",
        "        if pred_heatmap.size(1) == 1:\n",
        "            pred_heatmap = pred_heatmap.squeeze(1)\n",
        "\n",
        "        # Heatmap loss (MSE) - only for in-frame samples\n",
        "        heatmap_loss = self.mse_loss(pred_heatmap, target_heatmap)\n",
        "        heatmap_loss = heatmap_loss.mean(dim=(1, 2))  # Average over spatial dimensions\n",
        "\n",
        "        # Apply in-frame masking\n",
        "        masked_heatmap_loss = heatmap_loss * target_in_frame.squeeze()\n",
        "\n",
        "        # Average over valid samples\n",
        "        num_valid = max(1, target_in_frame.sum().item())  # Avoid division by zero\n",
        "        heatmap_loss = masked_heatmap_loss.sum() / num_valid\n",
        "\n",
        "        # In-frame prediction loss (BCE)\n",
        "        in_frame_loss = self.bce_loss(pred_in_frame, target_in_frame)\n",
        "\n",
        "        # Angular loss (if vectors are provided)\n",
        "        angular_loss = torch.tensor(0.0, device=pred_heatmap.device)\n",
        "        if pred_vector is not None and target_vector is not None:\n",
        "            # Compute cosine similarity between predicted and target vectors\n",
        "            target_in_frame_bool = target_in_frame.squeeze().bool()\n",
        "\n",
        "            if target_in_frame_bool.sum() > 0:\n",
        "                # Only compute for in-frame samples\n",
        "                pred_vec_valid = pred_vector[target_in_frame_bool]\n",
        "                target_vec_valid = target_vector[target_in_frame_bool]\n",
        "\n",
        "                # Normalize vectors\n",
        "                pred_norm = torch.norm(pred_vec_valid, dim=1, keepdim=True)\n",
        "                target_norm = torch.norm(target_vec_valid, dim=1, keepdim=True)\n",
        "\n",
        "                # Avoid division by zero\n",
        "                pred_norm = torch.clamp(pred_norm, min=1e-7)\n",
        "                target_norm = torch.clamp(target_norm, min=1e-7)\n",
        "\n",
        "                pred_vec_norm = pred_vec_valid / pred_norm\n",
        "                target_vec_norm = target_vec_valid / target_norm\n",
        "\n",
        "                # Cosine similarity (dot product of normalized vectors)\n",
        "                cos_sim = torch.sum(pred_vec_norm * target_vec_norm, dim=1)\n",
        "\n",
        "                # Angular loss = 1 - cos_sim\n",
        "                angular_loss = 1.0 - cos_sim.mean()\n",
        "\n",
        "        # Combine losses\n",
        "        total_loss = (\n",
        "            self.heatmap_weight * heatmap_loss +\n",
        "            self.in_frame_weight * in_frame_loss +\n",
        "            self.angular_weight * angular_loss\n",
        "        )\n",
        "\n",
        "        # Create loss dictionary for logging\n",
        "        loss_dict = {\n",
        "            'total_loss': total_loss.item(),\n",
        "            'heatmap_loss': heatmap_loss.item(),\n",
        "            'in_frame_loss': in_frame_loss.item(),\n",
        "            'angular_loss': angular_loss.item() if isinstance(angular_loss, torch.Tensor) else angular_loss\n",
        "        }\n",
        "\n",
        "        return total_loss, loss_dict\n",
        "\n",
        "\n",
        "# Example usage to test the model\n",
        "def test_model():\n",
        "    # Create model\n",
        "    model = MSGESCAMModel(pretrained=True, output_size=64)\n",
        "\n",
        "    # Create sample batch\n",
        "    batch_size = 2\n",
        "    scene_img = torch.randn(batch_size, 3, 224, 224)\n",
        "    head_img = torch.randn(batch_size, 3, 224, 224)\n",
        "    head_pos = torch.randn(batch_size, 1, 224, 224)\n",
        "\n",
        "    # Forward pass\n",
        "    pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos)\n",
        "\n",
        "    print(f\"Model test successful\")\n",
        "    print(f\"Predicted heatmap shape: {pred_heatmap.shape}\")\n",
        "    print(f\"Predicted in-frame shape: {pred_in_frame.shape}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = test_model()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T09:58:21.467611Z",
          "iopub.execute_input": "2025-04-17T09:58:21.468285Z",
          "iopub.status.idle": "2025-04-17T09:58:22.77459Z",
          "shell.execute_reply.started": "2025-04-17T09:58:21.468257Z",
          "shell.execute_reply": "2025-04-17T09:58:22.773826Z"
        },
        "id": "a-2fibCTeLLC",
        "outputId": "b4464de8-16eb-4a40-ab07-5e5b9db934d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 239MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model test successful\n",
            "Predicted heatmap shape: torch.Size([2, 1, 64, 64])\n",
            "Predicted in-frame shape: torch.Size([2, 1])\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Early stopping class\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
        "    def __init__(self, patience=5, min_delta=0, verbose=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    train_pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    for batch in train_pbar:\n",
        "        # Unpack the batch\n",
        "        scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, object_masks, metadata = batch\n",
        "\n",
        "        # Move to device\n",
        "        scene_img = scene_img.to(device)\n",
        "        head_img = head_img.to(device)\n",
        "        head_pos = head_pos.to(device)\n",
        "        target_heatmap = target_heatmap.to(device)\n",
        "        target_in_frame = target_in_frame.to(device)\n",
        "        target_vector = target_vector.to(device)\n",
        "        object_masks = object_masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos, object_masks)\n",
        "\n",
        "        # Compute loss\n",
        "        loss, loss_dict = criterion(\n",
        "            pred_heatmap, target_heatmap,\n",
        "            pred_in_frame, target_in_frame,\n",
        "            None, target_vector\n",
        "        )\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        train_losses.append(loss_dict)\n",
        "        train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = sum(d['total_loss'] for d in train_losses) / len(train_losses)\n",
        "    avg_heatmap_loss = sum(d['heatmap_loss'] for d in train_losses) / len(train_losses)\n",
        "    avg_in_frame_loss = sum(d['in_frame_loss'] for d in train_losses) / len(train_losses)\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'heatmap_loss': avg_heatmap_loss,\n",
        "        'in_frame_loss': avg_in_frame_loss\n",
        "    }\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "\n",
        "    val_pbar = tqdm(val_loader, desc=\"Validation\")\n",
        "    with torch.no_grad():\n",
        "        for batch in val_pbar:\n",
        "            # Unpack the batch\n",
        "            scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, object_masks, metadata = batch\n",
        "\n",
        "            # Move to device\n",
        "            scene_img = scene_img.to(device)\n",
        "            head_img = head_img.to(device)\n",
        "            head_pos = head_pos.to(device)\n",
        "            target_heatmap = target_heatmap.to(device)\n",
        "            target_in_frame = target_in_frame.to(device)\n",
        "            target_vector = target_vector.to(device)\n",
        "            object_masks = object_masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos, object_masks)\n",
        "\n",
        "            # Compute loss\n",
        "            loss, loss_dict = criterion(\n",
        "                pred_heatmap, target_heatmap,\n",
        "                pred_in_frame, target_in_frame,\n",
        "                None, target_vector\n",
        "            )\n",
        "\n",
        "            # Track metrics\n",
        "            val_losses.append(loss_dict)\n",
        "            val_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = sum(d['total_loss'] for d in val_losses) / len(val_losses)\n",
        "    avg_heatmap_loss = sum(d['heatmap_loss'] for d in val_losses) / len(val_losses)\n",
        "    avg_in_frame_loss = sum(d['in_frame_loss'] for d in val_losses) / len(val_losses)\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'heatmap_loss': avg_heatmap_loss,\n",
        "        'in_frame_loss': avg_in_frame_loss\n",
        "    }\n",
        "\n",
        "def visualize_predictions(model, dataset, device, indices=None, num_samples=5, save_dir=None):\n",
        "    \"\"\"Visualize model predictions\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if indices is None:\n",
        "        # Choose random samples\n",
        "        indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            # Get a sample\n",
        "            sample = dataset[idx]\n",
        "            scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, object_masks, metadata = sample\n",
        "\n",
        "            # Add batch dimension\n",
        "            scene_img = scene_img.unsqueeze(0).to(device)\n",
        "            head_img = head_img.unsqueeze(0).to(device)\n",
        "            head_pos = head_pos.unsqueeze(0).to(device)\n",
        "            object_masks = object_masks.unsqueeze(0).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos, object_masks)\n",
        "\n",
        "            # Convert predictions to numpy\n",
        "            pred_heatmap = pred_heatmap.squeeze().cpu().numpy()\n",
        "            pred_in_frame_prob = torch.sigmoid(pred_in_frame).squeeze().cpu().numpy()\n",
        "\n",
        "            # Create figure\n",
        "            plt.figure(figsize=(15, 10))\n",
        "\n",
        "            # Denormalize image\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "            img_vis = scene_img.squeeze().cpu()\n",
        "            img_vis = img_vis * std + mean\n",
        "            img_vis = img_vis.permute(1, 2, 0).numpy()\n",
        "            img_vis = np.clip(img_vis, 0, 1)\n",
        "\n",
        "            # Original image\n",
        "            plt.subplot(2, 3, 1)\n",
        "            plt.imshow(img_vis)\n",
        "            plt.title(f\"Frame {metadata['frame_id']}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Head crop\n",
        "            head_img_vis = head_img.squeeze().cpu()\n",
        "            head_img_vis = head_img_vis * std + mean\n",
        "            head_img_vis = head_img_vis.permute(1, 2, 0).numpy()\n",
        "            head_img_vis = np.clip(head_img_vis, 0, 1)\n",
        "\n",
        "            plt.subplot(2, 3, 2)\n",
        "            plt.imshow(head_img_vis)\n",
        "            plt.title(\"Head Crop\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Ground truth heatmap\n",
        "            plt.subplot(2, 3, 3)\n",
        "            plt.imshow(target_heatmap.numpy(), cmap='jet')\n",
        "            plt.title(f\"GT Heatmap (In-frame: {bool(target_in_frame.item())})\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Predicted heatmap\n",
        "            plt.subplot(2, 3, 4)\n",
        "            plt.imshow(pred_heatmap, cmap='jet')\n",
        "            plt.title(f\"Pred Heatmap (In-frame: {pred_in_frame_prob:.2f})\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Overlay on original image\n",
        "            plt.subplot(2, 3, 5)\n",
        "            plt.imshow(img_vis)\n",
        "            plt.imshow(pred_heatmap, cmap='jet', alpha=0.5)\n",
        "            plt.title(\"Prediction Overlay\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Error visualization\n",
        "            plt.subplot(2, 3, 6)\n",
        "            error_map = np.abs(pred_heatmap - target_heatmap.numpy())\n",
        "            plt.imshow(error_map, cmap='hot')\n",
        "            plt.title(\"Prediction Error\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save or display\n",
        "            if save_dir:\n",
        "                os.makedirs(save_dir, exist_ok=True)\n",
        "                plt.savefig(os.path.join(save_dir, f\"pred_{i}_sample_{idx}.png\"))\n",
        "                plt.close()\n",
        "            else:\n",
        "                plt.show()\n",
        "\n",
        "def plot_training_history(history, save_path=None):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Total Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot component losses\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_heatmap_loss'], label='Train Heatmap Loss')\n",
        "    plt.plot(history['val_heatmap_loss'], label='Val Heatmap Loss')\n",
        "    plt.plot(history['train_in_frame_loss'], label='Train In-Frame Loss')\n",
        "    plt.plot(history['val_in_frame_loss'], label='Val In-Frame Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Component Losses')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def test_model():\n",
        "    \"\"\"Simple test to verify the model architecture\"\"\"\n",
        "    # Create model\n",
        "    model = MSGESCAMModel(pretrained=True, output_size=64)\n",
        "\n",
        "    # Create sample batch\n",
        "    batch_size = 2\n",
        "    scene_img = torch.randn(batch_size, 3, 224, 224)\n",
        "    head_img = torch.randn(batch_size, 3, 224, 224)\n",
        "    head_pos = torch.randn(batch_size, 1, 224, 224)\n",
        "    object_masks = torch.randn(batch_size, 11, 224, 224)  # 11 object categories\n",
        "\n",
        "    # Forward pass\n",
        "    pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos, object_masks)\n",
        "    ## without headmasks\n",
        "    # pred_heatmap2, pred_in_frame2 = model(scene_img, head_img, head_pos)\n",
        "\n",
        "    print(f\"Model test successful\")\n",
        "    print(f\"Predicted heatmap shape: {pred_heatmap.shape}\")\n",
        "    print(f\"Predicted in-frame shape: {pred_in_frame.shape}\")\n",
        "\n",
        "# Training function\n",
        "def train_gescam_model(xml_path, image_folder, output_dir, batch_size=8, epochs=20,\n",
        "                      lr=1e-4, val_split=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Train the MS-GESCAM model\n",
        "\n",
        "    Args:\n",
        "        xml_path: Path to XML annotations\n",
        "        image_folder: Path to image folder\n",
        "        output_dir: Output directory\n",
        "        batch_size: Batch size\n",
        "        epochs: Number of epochs\n",
        "        lr: Learning rate\n",
        "        val_split: Validation split ratio\n",
        "        seed: Random seed\n",
        "    \"\"\"\n",
        "    # Set random seed\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create transforms\n",
        "    transform = get_transforms(augment=True)\n",
        "    val_transform = get_transforms(augment=False)\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    full_dataset = GESCAMCustomDataset(\n",
        "        xml_path=xml_path,\n",
        "        image_folder=image_folder,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Split dataset\n",
        "    # val_size = int(val_split * len(full_dataset))\n",
        "    # train_size = len(full_dataset) - val_size\n",
        "\n",
        "    val_size = int(val_split * len(combined_dataset))\n",
        "    train_size = len(combined_dataset) - val_size\n",
        "\n",
        "    # Use different random seeds for train and validation\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    #train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
        "    train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size], generator=generator)\n",
        "\n",
        "    # Create separate validation dataset with different transforms\n",
        "    val_dataset_with_transform = GESCAMCustomDataset(\n",
        "        xml_path=xml_path,\n",
        "        image_folder=image_folder,\n",
        "        transform=val_transform\n",
        "    )\n",
        "\n",
        "    # Use the same indices for validation\n",
        "    val_indices = [i for i in range(len(full_dataset)) if i not in train_dataset.indices]\n",
        "\n",
        "    # Create a subset for validation with the correct indices\n",
        "    class IndexSubset:\n",
        "        def __init__(self, dataset, indices):\n",
        "            self.dataset = dataset\n",
        "            self.indices = indices\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.dataset[self.indices[idx]]\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.indices)\n",
        "\n",
        "    val_dataset = IndexSubset(val_dataset_with_transform, val_indices)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset split: {train_size} train, {val_size} validation\")\n",
        "\n",
        "    # Create model\n",
        "    print(\"Creating model...\")\n",
        "    model = MSGESCAMModel(pretrained=True, output_size=64)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Create loss function\n",
        "    criterion = CombinedLoss(\n",
        "        heatmap_weight=1.0,\n",
        "        in_frame_weight=1.0,\n",
        "        angular_weight=0.5\n",
        "    )\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Create learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Create early stopping\n",
        "    early_stopping = EarlyStopping(patience=7, verbose=True)\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_heatmap_loss': [],\n",
        "        'val_heatmap_loss': [],\n",
        "        'train_in_frame_loss': [],\n",
        "        'val_in_frame_loss': []\n",
        "    }\n",
        "\n",
        "    # Best model state\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Train model\n",
        "    print(f\"Training for {epochs} epochs...\")\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        print(f\"Train loss: {train_metrics['loss']:.4f}, \"\n",
        "              f\"Heatmap loss: {train_metrics['heatmap_loss']:.4f}, \"\n",
        "              f\"In-frame loss: {train_metrics['in_frame_loss']:.4f}\")\n",
        "\n",
        "        # Validate\n",
        "        val_metrics = validate(model, val_loader, criterion, device)\n",
        "        print(f\"Val loss: {val_metrics['loss']:.4f}, \"\n",
        "              f\"Heatmap loss: {val_metrics['heatmap_loss']:.4f}, \"\n",
        "              f\"In-frame loss: {val_metrics['in_frame_loss']:.4f}\")\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_metrics['loss'])\n",
        "        history['val_loss'].append(val_metrics['loss'])\n",
        "        history['train_heatmap_loss'].append(train_metrics['heatmap_loss'])\n",
        "        history['val_heatmap_loss'].append(val_metrics['heatmap_loss'])\n",
        "        history['train_in_frame_loss'].append(train_metrics['in_frame_loss'])\n",
        "        history['val_in_frame_loss'].append(val_metrics['in_frame_loss'])\n",
        "\n",
        "        # Step learning rate scheduler\n",
        "        scheduler.step(val_metrics['loss'])\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss': train_metrics['loss'],\n",
        "            'val_loss': val_metrics['loss'],\n",
        "            'history': history\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        # Save best model\n",
        "        if val_metrics['loss'] < best_val_loss:\n",
        "            best_val_loss = val_metrics['loss']\n",
        "            best_model_path = os.path.join(output_dir, 'best_model.pt')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_metrics['loss']\n",
        "            }, best_model_path)\n",
        "            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "        # Check early stopping\n",
        "        early_stopping(val_metrics['loss'])\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    # Plot training history\n",
        "    history_path = os.path.join(output_dir, 'training_history.png')\n",
        "    plot_training_history(history, history_path)\n",
        "    print(f\"Training history plot saved to {history_path}\")\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    checkpoint = torch.load(os.path.join(output_dir, 'best_model.pt'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Visualize predictions on validation set\n",
        "    vis_dir = os.path.join(output_dir, 'visualizations')\n",
        "    os.makedirs(vis_dir, exist_ok=True)\n",
        "    print(f\"Generating visualizations in {vis_dir}...\")\n",
        "\n",
        "    # Choose a few random samples from validation set\n",
        "    val_samples = np.random.choice(len(val_dataset), min(10, len(val_dataset)), replace=False)\n",
        "    visualize_predictions(model, val_dataset, device, indices=val_samples, save_dir=vis_dir)\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # For Kaggle, set your paths here\n",
        "    xml_path = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\"\n",
        "    image_folder = f\"{base_dir}/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\"\n",
        "\n",
        "\n",
        "    # Optional: Just test the model architecture\n",
        "    # test_model()\n",
        "\n",
        "    # Train the model\n",
        "    model, history = train_gescam_model(\n",
        "        xml_path=xml_path,\n",
        "        image_folder=image_folder,\n",
        "        output_dir=output_dir,\n",
        "        batch_size=8,  # Adjust based on your GPU memory\n",
        "        epochs=2,     # Adjust as needed\n",
        "        lr=1e-4\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T10:09:39.526269Z",
          "iopub.execute_input": "2025-04-17T10:09:39.527092Z",
          "iopub.status.idle": "2025-04-17T10:09:46.881828Z",
          "shell.execute_reply.started": "2025-04-17T10:09:39.527063Z",
          "shell.execute_reply": "2025-04-17T10:09:46.880763Z"
        },
        "id": "xQnwfVfkeLLD",
        "outputId": "7b911815-fad3-4655-cfa0-6e4c9cfd5304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading dataset...\n",
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n",
            "Root tag: annotations, with 307 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 11259.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 56 boxes and 14 polylines in first frame\n",
            "Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 305 frames\n",
            "Found 305 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 305 frames with person boxes, 305 frames with sight lines\n",
            "Created dataset with 4575 samples\n",
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\n",
            "Root tag: annotations, with 307 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 12252.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 56 boxes and 14 polylines in first frame\n",
            "Sample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 305 frames\n",
            "Found 305 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 305 frames with person boxes, 305 frames with sight lines\n",
            "Created dataset with 4575 samples\n",
            "Dataset split: 30472 train, 7617 validation\n",
            "Creating model...\n",
            "Training for 2 epochs...\n",
            "\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 3809/3809 [15:56<00:00,  3.98it/s, loss=0.0153]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.0214, Heatmap loss: 0.0121, In-frame loss: 0.0093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 117/117 [00:28<00:00,  4.08it/s, loss=0.0016]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.0018, Heatmap loss: 0.0018, In-frame loss: 0.0000\n",
            "Saved new best model with validation loss: 0.0018\n",
            "\n",
            "Epoch 2/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 3809/3809 [16:01<00:00,  3.96it/s, loss=0.0015]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.0083, Heatmap loss: 0.0013, In-frame loss: 0.0070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 117/117 [00:28<00:00,  4.07it/s, loss=0.0008]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.0013, Heatmap loss: 0.0013, In-frame loss: 0.0000\n",
            "Saved new best model with validation loss: 0.0013\n",
            "Training history plot saved to ./training_history.png\n",
            "Generating visualizations in ./visualizations...\n",
            "Training complete!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_auc(pred_heatmap, target_heatmap):\n",
        "    \"\"\"\n",
        "    Calculate Area Under the ROC Curve for heatmap prediction\n",
        "\n",
        "    Args:\n",
        "        pred_heatmap: Predicted heatmap (numpy array)\n",
        "        target_heatmap: Ground truth heatmap (numpy array)\n",
        "\n",
        "    Returns:\n",
        "        auc_score: AUC score\n",
        "    \"\"\"\n",
        "    # Flatten heatmaps\n",
        "    pred_flat = pred_heatmap.flatten()\n",
        "    target_flat = (target_heatmap > 0.1).flatten().astype(int)  # Binarize target\n",
        "\n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, _ = roc_curve(target_flat, pred_flat)\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc_score = auc(fpr, tpr)\n",
        "\n",
        "    return auc_score\n",
        "\n",
        "def calculate_distance_error(pred_heatmap, target_heatmap, normalize=True):\n",
        "    \"\"\"\n",
        "    Calculate distance error between predicted and target gaze points\n",
        "\n",
        "    Args:\n",
        "        pred_heatmap: Predicted heatmap (numpy array)\n",
        "        target_heatmap: Ground truth heatmap (numpy array)\n",
        "        normalize: Whether to normalize by heatmap dimensions\n",
        "\n",
        "    Returns:\n",
        "        distance: L2 distance between peaks\n",
        "    \"\"\"\n",
        "    # Find peak positions\n",
        "    pred_idx = np.unravel_index(np.argmax(pred_heatmap), pred_heatmap.shape)\n",
        "    target_idx = np.unravel_index(np.argmax(target_heatmap), target_heatmap.shape)\n",
        "\n",
        "    # Calculate L2 distance\n",
        "    y1, x1 = pred_idx\n",
        "    y2, x2 = target_idx\n",
        "\n",
        "    if normalize:\n",
        "        # Normalize by heatmap dimensions\n",
        "        h, w = target_heatmap.shape\n",
        "        x1, y1 = x1/w, y1/h\n",
        "        x2, y2 = x2/w, y2/h\n",
        "\n",
        "    distance = np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
        "\n",
        "    return distance\n",
        "\n",
        "def calculate_angular_error(pred_vector, target_vector):\n",
        "    \"\"\"\n",
        "    Calculate angular error between predicted and target gaze vectors\n",
        "\n",
        "    Args:\n",
        "        pred_vector: Predicted gaze vector [x, y]\n",
        "        target_vector: Ground truth gaze vector [x, y]\n",
        "\n",
        "    Returns:\n",
        "        angle: Angular error in degrees\n",
        "    \"\"\"\n",
        "    # Normalize vectors\n",
        "    pred_norm = np.linalg.norm(pred_vector)\n",
        "    target_norm = np.linalg.norm(target_vector)\n",
        "\n",
        "    if pred_norm < 1e-7 or target_norm < 1e-7:\n",
        "        return 180.0  # Maximum error\n",
        "\n",
        "    pred_normalized = pred_vector / pred_norm\n",
        "    target_normalized = target_vector / target_norm\n",
        "\n",
        "    # Calculate dot product\n",
        "    dot_product = np.clip(np.dot(pred_normalized, target_normalized), -1.0, 1.0)\n",
        "\n",
        "    # Calculate angle in degrees\n",
        "    angle = np.arccos(dot_product) * 180 / np.pi\n",
        "\n",
        "    return angle\n",
        "\n",
        "def calculate_in_frame_accuracy(pred_in_frame, target_in_frame, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculate accuracy of in-frame prediction\n",
        "\n",
        "    Args:\n",
        "        pred_in_frame: Predicted in-frame probability\n",
        "        target_in_frame: Ground truth in-frame label\n",
        "        threshold: Classification threshold\n",
        "\n",
        "    Returns:\n",
        "        accuracy: Accuracy score\n",
        "    \"\"\"\n",
        "    pred_binary = (pred_in_frame > threshold).astype(int)\n",
        "    target_binary = target_in_frame.astype(int)\n",
        "\n",
        "    accuracy = (pred_binary == target_binary).mean()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def extract_gaze_vector_from_heatmap(heatmap, head_center, heatmap_size, normalize=True):\n",
        "    \"\"\"\n",
        "    Extract gaze vector from heatmap peak\n",
        "\n",
        "    Args:\n",
        "        heatmap: Gaze heatmap\n",
        "        head_center: Head center coordinates (x, y) normalized\n",
        "        heatmap_size: Original heatmap dimensions\n",
        "        normalize: Whether to normalize the vector\n",
        "\n",
        "    Returns:\n",
        "        gaze_vector: Vector from head center to gaze target\n",
        "    \"\"\"\n",
        "    # Find peak position\n",
        "    peak_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n",
        "    peak_y, peak_x = peak_idx\n",
        "\n",
        "    # Convert to normalized coordinates\n",
        "    h, w = heatmap.shape\n",
        "    peak_x_norm = peak_x / w\n",
        "    peak_y_norm = peak_y / h\n",
        "\n",
        "    # Calculate vector\n",
        "    gaze_vector = np.array([peak_x_norm - head_center[0], peak_y_norm - head_center[1]])\n",
        "\n",
        "    # Normalize if requested\n",
        "    if normalize and np.linalg.norm(gaze_vector) > 0:\n",
        "        gaze_vector = gaze_vector / np.linalg.norm(gaze_vector)\n",
        "\n",
        "    return gaze_vector\n",
        "\n",
        "def visualize_prediction(img, head_bbox, pred_heatmap, target_heatmap,\n",
        "                        pred_in_frame, target_in_frame, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize model prediction versus ground truth\n",
        "\n",
        "    Args:\n",
        "        img: Original image (RGB)\n",
        "        head_bbox: Head bounding box [x1, y1, x2, y2]\n",
        "        pred_heatmap: Predicted heatmap\n",
        "        target_heatmap: Ground truth heatmap\n",
        "        pred_in_frame: Predicted in-frame probability\n",
        "        target_in_frame: Ground truth in-frame label\n",
        "        save_path: Path to save visualization\n",
        "    \"\"\"\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Original image with head box\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.imshow(img)\n",
        "    x1, y1, x2, y2 = head_bbox\n",
        "    plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                     fill=False, edgecolor='green', linewidth=2))\n",
        "    plt.title(f\"Head (Target In-frame: {bool(target_in_frame)})\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Predicted heatmap\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.imshow(pred_heatmap, cmap='jet')\n",
        "    plt.title(f\"Predicted Heatmap (P={pred_in_frame:.2f})\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Ground truth heatmap\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.imshow(target_heatmap, cmap='jet')\n",
        "    plt.title(\"Ground Truth Heatmap\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Image with prediction overlay\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.imshow(img)\n",
        "    plt.imshow(pred_heatmap, cmap='jet', alpha=0.5)\n",
        "    plt.title(\"Predicted Overlay\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Image with ground truth overlay\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.imshow(img)\n",
        "    plt.imshow(target_heatmap, cmap='jet', alpha=0.5)\n",
        "    plt.title(\"Ground Truth Overlay\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Error heatmap\n",
        "    plt.subplot(2, 3, 6)\n",
        "    error_map = np.abs(pred_heatmap - target_heatmap)\n",
        "    plt.imshow(error_map, cmap='hot')\n",
        "    plt.title(\"Prediction Error\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def validate_model(model, dataset, device, batch_size=8, num_vis=10, vis_dir=None):\n",
        "    \"\"\"\n",
        "    Validate model performance on dataset\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        dataset: Validation dataset\n",
        "        device: Device to run model on\n",
        "        batch_size: Batch size for evaluation\n",
        "        num_vis: Number of visualizations to generate\n",
        "        vis_dir: Directory to save visualizations\n",
        "\n",
        "    Returns:\n",
        "        metrics: Dictionary of evaluation metrics\n",
        "    \"\"\"\n",
        "    # Create data loader\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Create directory for visualizations\n",
        "    if vis_dir:\n",
        "        os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize metrics\n",
        "    all_auc = []\n",
        "    all_distance = []\n",
        "    all_angular = []\n",
        "    all_in_frame_acc = []\n",
        "\n",
        "    # Initialize lists for confusion matrix\n",
        "    all_pred_in_frame = []\n",
        "    all_target_in_frame = []\n",
        "\n",
        "    # Generate random indices for visualization\n",
        "    if len(dataset) > 0 and num_vis > 0:\n",
        "        vis_indices = np.random.choice(len(dataset), min(num_vis, len(dataset)), replace=False)\n",
        "    else:\n",
        "        vis_indices = []\n",
        "\n",
        "    # Process all samples\n",
        "    with torch.no_grad():\n",
        "        # Process batched samples\n",
        "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Validating\")):\n",
        "            # Unpack batch\n",
        "            scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, object_masks, metadata = batch\n",
        "\n",
        "            # Move tensors to device\n",
        "            scene_img = scene_img.to(device)\n",
        "            head_img = head_img.to(device)\n",
        "            head_pos = head_pos.to(device)\n",
        "            object_masks = object_masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos, object_masks)\n",
        "\n",
        "            # Move predictions to CPU for evaluation\n",
        "            pred_heatmap = pred_heatmap.squeeze(1).cpu().numpy()\n",
        "            pred_in_frame_prob = torch.sigmoid(pred_in_frame).squeeze().cpu().numpy()\n",
        "\n",
        "            # Convert targets to numpy\n",
        "            target_heatmap_np = target_heatmap.cpu().numpy()\n",
        "            target_in_frame_np = target_in_frame.squeeze().cpu().numpy()\n",
        "\n",
        "            # Evaluate each sample in batch\n",
        "            for i in range(len(scene_img)):\n",
        "                # Only evaluate in-frame samples for gaze metrics\n",
        "                if target_in_frame_np[i] > 0.5:\n",
        "                    # Calculate AUC\n",
        "                    auc_score = calculate_auc(pred_heatmap[i], target_heatmap_np[i])\n",
        "                    all_auc.append(auc_score)\n",
        "\n",
        "                    # Calculate distance error\n",
        "                    dist_error = calculate_distance_error(pred_heatmap[i], target_heatmap_np[i])\n",
        "                    all_distance.append(dist_error)\n",
        "\n",
        "                    # Calculate angular error using vectors\n",
        "                    pred_vector = target_vector[i].cpu().numpy()  # Use target vector for now\n",
        "                    target_vec = target_vector[i].cpu().numpy()\n",
        "                    angular_error = calculate_angular_error(pred_vector, target_vec)\n",
        "                    all_angular.append(angular_error)\n",
        "\n",
        "                # Record in-frame prediction for all samples\n",
        "                all_pred_in_frame.append(pred_in_frame_prob[i])\n",
        "                all_target_in_frame.append(target_in_frame_np[i])\n",
        "\n",
        "        # Create individual visualizations for selected samples\n",
        "        if vis_dir:\n",
        "            # Reset normalization parameters for visualization\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "            for vis_idx, idx in enumerate(tqdm(vis_indices, desc=\"Generating visualizations\")):\n",
        "                # Get sample\n",
        "                sample = dataset[idx]\n",
        "                scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, _, object_masks, metadata = sample\n",
        "\n",
        "\n",
        "                # Prepare inputs for model\n",
        "                scene_img_batch = scene_img.unsqueeze(0).to(device)\n",
        "                head_img_batch = head_img.unsqueeze(0).to(device)\n",
        "                head_pos_batch = head_pos.unsqueeze(0).to(device)\n",
        "                object_masks_batch = object_masks.unsqueeze(0).to(device)  # Add this line\n",
        "\n",
        "                # Forward pass\n",
        "                pred_heatmap, pred_in_frame = model(scene_img_batch, head_img_batch, head_pos_batch, object_masks_batch)\n",
        "\n",
        "                # Move predictions to CPU for visualization\n",
        "                pred_heatmap_np = pred_heatmap.squeeze().cpu().numpy()\n",
        "                pred_in_frame_prob = torch.sigmoid(pred_in_frame).item()\n",
        "\n",
        "                # Denormalize image for visualization\n",
        "                img_vis = scene_img.clone()\n",
        "                img_vis = img_vis * std + mean\n",
        "                img_vis = img_vis.permute(1, 2, 0).numpy()\n",
        "                img_vis = np.clip(img_vis, 0, 1)\n",
        "\n",
        "                # Get head bbox for visualization\n",
        "                x1, y1, x2, y2 = metadata['head_bbox']\n",
        "\n",
        "                # Scale for visualization\n",
        "                h, w = img_vis.shape[:2]\n",
        "                orig_w, orig_h = metadata['original_size']\n",
        "                scale_x, scale_y = w/orig_w, h/orig_h\n",
        "\n",
        "                # Scale bbox\n",
        "                x1 = x1 * scale_x\n",
        "                y1 = y1 * scale_y\n",
        "                x2 = x2 * scale_x\n",
        "                y2 = y2 * scale_y\n",
        "\n",
        "                # Create visualization\n",
        "                vis_path = os.path.join(vis_dir, f\"validation_{vis_idx}.png\")\n",
        "                visualize_prediction(\n",
        "                    img_vis, [x1, y1, x2, y2],\n",
        "                    pred_heatmap_np, target_heatmap.numpy(),\n",
        "                    pred_in_frame_prob, target_in_frame.item(),\n",
        "                    vis_path\n",
        "                )\n",
        "\n",
        "    # Calculate in-frame accuracy\n",
        "    in_frame_accuracy = calculate_in_frame_accuracy(\n",
        "        np.array(all_pred_in_frame),\n",
        "        np.array(all_target_in_frame)\n",
        "    )\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'auc_mean': np.mean(all_auc) if all_auc else np.nan,\n",
        "        'auc_std': np.std(all_auc) if all_auc else np.nan,\n",
        "        'distance_mean': np.mean(all_distance) if all_distance else np.nan,\n",
        "        'distance_std': np.std(all_distance) if all_distance else np.nan,\n",
        "        'angular_mean': np.mean(all_angular) if all_angular else np.nan,\n",
        "        'angular_std': np.std(all_angular) if all_angular else np.nan,\n",
        "        'in_frame_accuracy': in_frame_accuracy,\n",
        "        'num_evaluated': len(all_auc)\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def create_attention_heatmap(model, dataset, device, output_path, frame_indices=None, num_frames=10):\n",
        "    \"\"\"\n",
        "    Create an attention heatmap visualization for entire frames\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        dataset: Dataset to visualize\n",
        "        device: Device to run model on\n",
        "        output_path: Path to save visualization video\n",
        "        frame_indices: Specific frame indices to visualize (optional)\n",
        "        num_frames: Number of frames to visualize (if frame_indices not provided)\n",
        "    \"\"\"\n",
        "    # Get unique frame IDs\n",
        "    all_frame_ids = []\n",
        "    for idx in range(len(dataset)):\n",
        "        sample = dataset[idx]\n",
        "        metadata = sample[8]  # Metadata is the 9th element\n",
        "        frame_id = metadata['frame_id']\n",
        "        if frame_id not in all_frame_ids:\n",
        "            all_frame_ids.append(frame_id)\n",
        "\n",
        "    # Select frames to visualize\n",
        "    if frame_indices is None:\n",
        "        if len(all_frame_ids) <= num_frames:\n",
        "            frame_indices = all_frame_ids\n",
        "        else:\n",
        "            frame_indices = sorted(np.random.choice(all_frame_ids, num_frames, replace=False))\n",
        "\n",
        "    # Create temporary directory for frames\n",
        "    temp_dir = \"temp_attention_frames\"\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Process each selected frame\n",
        "    for i, frame_id in enumerate(tqdm(frame_indices, desc=\"Creating attention heatmaps\")):\n",
        "        # Find all samples with this frame ID\n",
        "        frame_samples = []\n",
        "        for idx in range(len(dataset)):\n",
        "            sample = dataset[idx]\n",
        "            metadata = sample[8]\n",
        "            if metadata['frame_id'] == frame_id:\n",
        "                frame_samples.append(idx)\n",
        "\n",
        "        if not frame_samples:\n",
        "            continue\n",
        "\n",
        "        # Load the first sample for frame information\n",
        "        first_sample = dataset[frame_samples[0]]\n",
        "        scene_img, _, _, _, _, _, _, object_masks, metadata = first_sample  # Unpack correctly\n",
        "        # Get image size\n",
        "        img_size = scene_img.shape[1:3]\n",
        "\n",
        "        # Denormalize image for visualization\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        img_vis = scene_img.clone()\n",
        "        img_vis = img_vis * std + mean\n",
        "        img_vis = img_vis.permute(1, 2, 0).numpy()\n",
        "        img_vis = np.clip(img_vis, 0, 1)\n",
        "\n",
        "        # Initialize combined heatmap\n",
        "        combined_heatmap = np.zeros(img_size)\n",
        "\n",
        "        # Process each person in the frame\n",
        "        with torch.no_grad():  # Add no_grad context to prevent tracking gradients\n",
        "            for sample_idx in frame_samples:\n",
        "                sample = dataset[sample_idx]\n",
        "                scene_img, head_img, head_pos, _, target_in_frame, _, _, object_masks, metadata = sample  # Unpack correctly\n",
        "                # Only process in-frame samples\n",
        "                if target_in_frame.item() < 0.5:\n",
        "                    continue\n",
        "\n",
        "                # Prepare inputs for model\n",
        "                scene_img_batch = scene_img.unsqueeze(0).to(device)\n",
        "                head_img_batch = head_img.unsqueeze(0).to(device)\n",
        "                head_pos_batch = head_pos.unsqueeze(0).to(device)\n",
        "                object_masks_batch = object_masks.unsqueeze(0).to(device)  # Add this line\n",
        "\n",
        "                # Forward pass\n",
        "                pred_heatmap, _ = model(scene_img_batch, head_img_batch, head_pos_batch, object_masks_batch)\n",
        "\n",
        "                # Add to combined heatmap\n",
        "                pred_heatmap_np = pred_heatmap.squeeze().cpu().numpy()  # This is safe now with no_grad\n",
        "\n",
        "                # Resize to match image size\n",
        "                pred_heatmap_resized = cv2.resize(pred_heatmap_np, (img_size[1], img_size[0]))\n",
        "\n",
        "                # Add to combined heatmap\n",
        "                combined_heatmap += pred_heatmap_resized\n",
        "\n",
        "        # Normalize combined heatmap\n",
        "        if np.max(combined_heatmap) > 0:\n",
        "            combined_heatmap = combined_heatmap / np.max(combined_heatmap)\n",
        "\n",
        "        # Create visualization\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(img_vis)\n",
        "        plt.imshow(combined_heatmap, cmap='jet', alpha=0.5)\n",
        "        plt.title(f\"Frame {frame_id} - Combined Attention\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Save frame\n",
        "        frame_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n",
        "        plt.savefig(frame_path)\n",
        "        plt.close()\n",
        "\n",
        "    # Create video\n",
        "    frame_paths = sorted([os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('.png')])\n",
        "\n",
        "    if not frame_paths:\n",
        "        print(\"No frames generated!\")\n",
        "        return\n",
        "\n",
        "    # Get first frame to determine dimensions\n",
        "    first_frame = cv2.imread(frame_paths[0])\n",
        "    height, width, _ = first_frame.shape\n",
        "\n",
        "    # Create video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_writer = cv2.VideoWriter(output_path, fourcc, 2, (width, height))\n",
        "\n",
        "    # Add frames to video\n",
        "    for frame_path in frame_paths:\n",
        "        frame = cv2.imread(frame_path)\n",
        "        video_writer.write(frame)\n",
        "\n",
        "    # Release video writer\n",
        "    video_writer.release()\n",
        "\n",
        "    # Clean up temp files\n",
        "    for frame_path in frame_paths:\n",
        "        os.remove(frame_path)\n",
        "    os.rmdir(temp_dir)\n",
        "\n",
        "    print(f\"Attention heatmap video saved to {output_path}\")\n",
        "def main():\n",
        "    # Paths\n",
        "    model_path = f\"{output_dir}/gescam_output/best_model.pt\"\n",
        "    xml_path = f\"{base_dir}/test_subset/task_classroom_11_video-01_final/annotations.xml\"\n",
        "    image_folder = f\"{base_dir}/test_subset/task_classroom_11_video-01_final/images\"\n",
        "    output = f\"{output_dir}/validation_results\"\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output, exist_ok=True)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load model\n",
        "    print(\"Loading model...\")\n",
        "    model = MSGESCAMModel(pretrained=False, output_size=64)\n",
        "\n",
        "    # Load checkpoint\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Initializing with random weights\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    transform = get_transforms(augment=False)\n",
        "\n",
        "    dataset = GESCAMCustomDataset(\n",
        "        xml_path=xml_path,\n",
        "        image_folder=image_folder,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Split dataset\n",
        "    val_size = min(int(0.2 * len(dataset)), 500)  # Cap at 500 for validation\n",
        "    generator = torch.Generator().manual_seed(42)\n",
        "    _, val_dataset = random_split(dataset, [len(dataset) - val_size, val_size],\n",
        "                                 generator=generator)\n",
        "\n",
        "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "    # Validate model\n",
        "    print(\"Validating model...\")\n",
        "    metrics = validate_model(\n",
        "        model=model,\n",
        "        dataset=val_dataset,\n",
        "        device=device,\n",
        "        batch_size=8,\n",
        "        num_vis=20,\n",
        "        vis_dir=os.path.join(output, \"visualizations\")\n",
        "    )\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nModel Validation Metrics:\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"AUC: {metrics['auc_mean']:.4f} ± {metrics['auc_std']:.4f}\")\n",
        "    print(f\"Distance Error: {metrics['distance_mean']:.4f} ± {metrics['distance_std']:.4f}\")\n",
        "    print(f\"Angular Error: {metrics['angular_mean']:.2f}° ± {metrics['angular_std']:.2f}°\")\n",
        "    print(f\"In-frame Accuracy: {metrics['in_frame_accuracy']:.4f}\")\n",
        "    print(f\"Number of evaluated samples: {metrics['num_evaluated']}\")\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_path = os.path.join(output, \"metrics.txt\")\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        for key, value in metrics.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    # Create attention heatmap video\n",
        "    print(\"Creating attention heatmap video...\")\n",
        "    heatmap_video_path = os.path.join(output, \"attention_heatmap.mp4\")\n",
        "    create_attention_heatmap(\n",
        "        model=model,\n",
        "        dataset=val_dataset,\n",
        "        device=device,\n",
        "        output_path=heatmap_video_path,\n",
        "        num_frames=20\n",
        "    )\n",
        "\n",
        "    print(f\"Validation complete. Results saved to {output}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T09:09:24.439041Z",
          "iopub.execute_input": "2025-03-26T09:09:24.439356Z"
        },
        "id": "yAf4yZDXeLLE",
        "outputId": "902d9484-3a8d-4633-df6e-da8a273ec0db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading model: [Errno 2] No such file or directory: './gescam_output/best_model.pt'\n",
            "Initializing with random weights\n",
            "Loading dataset...\n",
            "Parsing XML annotations from /root/.cache/kagglehub/datasets/thebrokenvessel/gescam-partial/versions/1/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset/task_classroom_11_video-01_final/annotations.xml\n",
            "Root tag: annotations, with 601 child elements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing frames: 100%|██████████| 599/599 [00:00<00:00, 9870.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample frame: ID=0, Name=frame_000000, Size=1920x1080\n",
            "Found 69 boxes and 13 polylines in first frame\n",
            "Sample box labels: ['Mug', 'book', 'book', 'table lamp', 'table lamp']\n",
            "Sample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\n",
            "Successfully parsed 599 frames\n",
            "Found 599 images with extractable frame IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics: 599 frames with person boxes, 599 frames with sight lines\n",
            "Created dataset with 7787 samples\n",
            "Validation dataset size: 500\n",
            "Validating model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 63/63 [00:33<00:00,  1.87it/s]\n",
            "Generating visualizations: 100%|██████████| 20/20 [00:14<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Validation Metrics:\n",
            "------------------------------\n",
            "AUC: 0.4983 ± 0.0270\n",
            "Distance Error: 0.2789 ± 0.1742\n",
            "Angular Error: 0.01° ± 0.01°\n",
            "In-frame Accuracy: 0.3240\n",
            "Number of evaluated samples: 500\n",
            "Creating attention heatmap video...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating attention heatmaps: 100%|██████████| 20/20 [09:42<00:00, 29.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention heatmap video saved to ./validation_results/attention_heatmap.mp4\n",
            "Validation complete. Results saved to ./validation_results\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OBJECT OF INTEREST\n"
      ],
      "metadata": {
        "id": "MEkV9WsN5bNT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "Asv4KfOqeLLE"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}